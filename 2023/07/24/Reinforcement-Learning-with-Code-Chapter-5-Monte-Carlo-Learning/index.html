<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Reinforcement Learning with Code [Chapter 5. Monte Carlo Learning] | Shen Mu Xin's Blog</title><meta name="author" content="Shen Mu Xin"><meta name="copyright" content="Shen Mu Xin"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="This note record how the author begin to learn RL. Both theoretical understanding and code practice are presented. Many material are referenced.">
<meta property="og:type" content="article">
<meta property="og:title" content="Reinforcement Learning with Code [Chapter 5. Monte Carlo Learning]">
<meta property="og:url" content="http://shenmuxin.github.io/2023/07/24/Reinforcement-Learning-with-Code-Chapter-5-Monte-Carlo-Learning/index.html">
<meta property="og:site_name" content="Shen Mu Xin&#39;s Blog">
<meta property="og:description" content="This note record how the author begin to learn RL. Both theoretical understanding and code practice are presented. Many material are referenced.">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic.imgdb.cn/item/658bde8dc458853aef14c63a.jpg">
<meta property="article:published_time" content="2023-07-24T13:00:31.000Z">
<meta property="article:modified_time" content="2023-12-27T10:25:56.754Z">
<meta property="article:author" content="Shen Mu Xin">
<meta property="article:tag" content="RL">
<meta property="article:tag" content="Reinforcement Learning">
<meta property="article:tag" content="Deep Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic.imgdb.cn/item/658bde8dc458853aef14c63a.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://shenmuxin.github.io/2023/07/24/Reinforcement-Learning-with-Code-Chapter-5-Monte-Carlo-Learning/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":-1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Reinforcement Learning with Code [Chapter 5. Monte Carlo Learning]',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-12-27 18:25:56'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><script>window.paceOptions = {
  restartOnPushState: false
}

document.addEventListener('pjax:send', () => {
  Pace.restart()
})
</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js/themes/blue/pace-theme-minimal.min.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js/pace.min.js"></script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">20</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">26</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://pic.imgdb.cn/item/658bde8dc458853aef14c63a.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Shen Mu Xin's Blog"><span class="site-name">Shen Mu Xin's Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Reinforcement Learning with Code [Chapter 5. Monte Carlo Learning]</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-07-24T13:00:31.000Z" title="发表于 2023-07-24 21:00:31">2023-07-24</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-27T10:25:56.754Z" title="更新于 2023-12-27 18:25:56">2023-12-27</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Reinforcement-Learning/">Reinforcement Learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">1.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>10分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Reinforcement Learning with Code [Chapter 5. Monte Carlo Learning]"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Reinforcement-Learning-with-Code"><a href="#Reinforcement-Learning-with-Code" class="headerlink" title="Reinforcement Learning with Code"></a>Reinforcement Learning with Code</h1><p>This note records how the author begin to learn RL. Both theoretical understanding and code practice are presented. Many material are referenced such as ZhaoShiyu’s <em>Mathematical Foundation of Reinforcement Learning</em>, .</p>
<h2 id="Chapter-5-Monte-Carlo-Learning"><a href="#Chapter-5-Monte-Carlo-Learning" class="headerlink" title="Chapter 5. Monte Carlo Learning"></a>Chapter 5. Monte Carlo Learning</h2><p>​	What is Monte Carlo estimation? Monte Carlo estimation refers to a broad class of techniques that use stochastic samples to solve approximation problems using the <font color=blue>Law of Large Numbers</font>. </p>
<p>​	ChatGpt tells us that “Monte Carlo estimation is a statistical method used to estimate unknown quantities or solve problems by generating random samples and using the law of large numbers to approximate the true value.”</p>
<h3 id="5-1-Law-of-large-numbers"><a href="#5-1-Law-of-large-numbers" class="headerlink" title="5.1 Law of large numbers"></a>5.1 Law of large numbers</h3><p>​	(Law of Large Numbers) <em>For a random variable</em> $X$. <em>Suppose</em> ${x_i}<em>{i&#x3D;1}^n$ <em>are some independent and indentically distribution (iid) samples</em>. <em>Let</em> $\bar{x}&#x3D;\frac{1}{n}\sum</em>{i&#x3D;1}^n x_i$ <em>be the average of the samples. Then</em>,<br>$$<br>\mathbb{E}[\bar{x}] &#x3D; \mathbb{E}[X]	\<br>\mathrm{var}[\bar{x}] &#x3D; \frac{1}{n} \mathrm{var}[X]<br>$$<br>The above two equations indicate that $\bar{x}$ is an <em>unbiased estimate</em> of $\mathbb{E}[X]$ and its variance decreases to zero as $n$ increases to infinity.</p>
<p>​	<strong>Proof</strong>, First, $\mathbb{E}[\bar{x}] &#x3D; \mathbb{E}[\frac{1}{n}\sum_{i&#x3D;1}^n x_i]&#x3D;\frac{1}{n}\sum_{i&#x3D;1}^n\mathbb{E}[x_i]&#x3D;\frac{1}{n}n\mathbb{E}[X]&#x3D;\mathbb{E}[X]$, where the last equability is because the samples are <em>independent and indentically distribution (iid)</em> (that is, $\mathbb{E}[x_i]&#x3D;\mathbb{E}[X]$).</p>
<p>​	Second, $\mathrm{var}[\bar{x}]&#x3D;\mathrm{var}[\frac{1}{n}\sum_{i&#x3D;1}^n x_i]&#x3D;\frac{1}{n^2}\sum_{i&#x3D;1}^n\mathrm{var}[x_i]&#x3D;\frac{1}{n^2}n*\mathrm{var}[X]&#x3D;\frac{1}{n}\mathrm{var}[X]$, where the second equality is because the samples are <em>independent and indentically distribution (iid)</em> (that is, $\mathrm{var}[x_i]&#x3D;\mathrm{var}[X]$)</p>
<h3 id="5-2-Simple-example"><a href="#5-2-Simple-example" class="headerlink" title="5.2 Simple example"></a>5.2 Simple example</h3><p>​	Consider a problem that we need to calculate the expectation $\mathbb{E}[X]$ of random variable $X$ which takes value in the finite set $\mathcal{X}$. There are two ways. </p>
<p>​	First, <em>model-based</em> approach, we can use the definition of expectation that<br>$$<br>\mathbb{E}[X] &#x3D; \sum_{x\in\mathcal{X}} p(x)x<br>$$<br>​	Second, <em>model-free</em> approach, if the probability of distribution is unknown we can use the <em>Law of Large Numbers</em> to estimate the expectation.<br>$$<br>\mathbb{E}[X]\approx \bar{x} &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^n x_i<br>$$<br>The example gives us an intutition that: When the system model is available, the expectation can be calculated based on the model.</p>
<p>When the model is unavailable, the expectation can be estimated approximately using stochastic samples.</p>
<h3 id="5-3-Monte-Carlo-Basic"><a href="#5-3-Monte-Carlo-Basic" class="headerlink" title="5.3 Monte Carlo Basic"></a>5.3 Monte Carlo Basic</h3><p><strong>Review policy iteration</strong>:</p>
<p>​	The simplest Monte Carlo learning also called Monte Carlo Basic (MC Basic) just replaces the policy iteration model-based part by MC estimation. Recall the <font color=blue>matrix-vector form of policy iteration</font></p>
<p>$$<br>\begin{aligned}<br>    v_{\pi_k}^{(j+1)} &amp; &#x3D; r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}^{(j)}	\quad \text{(policy evaluation)} \<br>    \pi_{k+1} &amp; &#x3D; \arg \max_{\pi_k}(r_{\pi_k} + \gamma P_{\pi_k}v_{\pi_k})	\quad \text{(policy improvement)}<br>\end{aligned}<br>$$</p>
<p>The <font color=blue>elementwise form of policy iteration</font> is</p>
<p>$$<br>\begin{aligned}<br>    v_{\pi_k}^{(j+1)}(s) &amp; &#x3D; \sum_a \pi_k(a|s) \Big(\sum_r p(r|s,a)r + \sum_{s^\prime} p(s^\prime|s,a) v_{\pi_k}^{(j)} (s^\prime)	\Big)	\quad \text{(policy evaluation)}		\<br>    \pi_{k+1}(s) &amp; &#x3D; \arg \max_{\pi_k}  \sum_a \pi(a|s)<br>    \underbrace{<br>    \Big(\sum_r p(r|s,a)r + \sum_{s^\prime} p(s^\prime|s,a) v_{\pi_k} (s^\prime)	\Big)}<em>{q</em>{\pi_k}(s,a)}<br>    \quad \text{(policy improvement)}	\<br>    \to \pi_{k+1}(s) &amp; &#x3D;  \sum_a \pi(a|s) \arg \max_a q_{\pi_k}(s,a)<br>\end{aligned}<br>$$</p>
<p>Its obvious that the core is to calculate the action values by usting<br>$$<br>q_{\pi_k}(s,a) &#x3D; \sum_r p(r|s,a)r + \sum_{s^\prime}p(s^\prime|s,a)v_{\pi_k}(s^\prime)<br>$$<br>​	However, the above equation needs the model of the environment (that  $p(r|s,a)$ and $p(s^\prime|s,a)$ are required). We can replace this part by Monte Carlo estimation. Recall the definition of action value refer to section 2.5.<br>$$<br>q_\pi(s,a) \triangleq \mathbb{E}[G_t|S_t&#x3D;s, A_t&#x3D;a]<br>$$<br>Action value can be interpreted as <font color=blue>average return</font> along trajectory generated by <font color=blue>policy $\pi$ </font> after taking a <font color=blue>specific action $a$</font>.</p>
<p><strong>Convert to model free</strong>:</p>
<p>​	Suppose there are $n$ episodes sampling starting at state $s$ and takeing action $a$,  and then denote the $i$ th episode return as $\textcolor{blue}{g^{(i)}(s,a)}$. Then using the idea of Monte Carlo estimation and Law of Large Numbers, the action values can be approxiamte as<br>$$<br>q_\pi(s,a) \triangleq \mathbb{E}[G_t|S_t&#x3D;s, A_t&#x3D;a] \textcolor{blue}{\approx \frac{1}{n} \sum_{j&#x3D;1}^n g^{(j)}(s,a)}<br>$$<br>Suppose the number of episodes $n$ is sufficiently large. So, the above equation is the unbiased estimation of action value $q_\pi(s,a)$.</p>
<p><strong>Pesudocode</strong>:</p>
<img src="https://pic.imgdb.cn/item/658bfa94c458853aef6a9405.png" class="" width="800" height="400" title="state-transform">


<p>​	Here are some questions.  <font color=red>Why does the MC Basic algorithm estimate action values instead of state values?</font> That is because state value cannot be used to improve policies directly. Even if we are given state values, we still need to calculate action values from these state value using $q_{\pi_k}(s,a) &#x3D; \sum_r p(r|s,a)r + \sum_{s^\prime}p(s^\prime|s,a)v_{\pi_k}(s^\prime)$. But this calculation requires the system model. Therefore, when models are not available, we should directly estimate action values.</p>
<h3 id="5-4-Monte-Carlo-Exploring-Starts"><a href="#5-4-Monte-Carlo-Exploring-Starts" class="headerlink" title="5.4 Monte Carlo Exploring Starts"></a>5.4 Monte Carlo Exploring Starts</h3><p><strong>Some concepts</strong>:</p>
<ul>
<li><p><font color=blue>Vist</font>: a state-action pair appears in the episode, it is called a <em>vist</em> of the state-action pair. Such as<br>$$<br>\textcolor{blue}{s_1 \xrightarrow{a_2}} s_2 \xrightarrow{a_4} \textcolor{blue}{s_1 \xrightarrow{a_2}} s_2 \xrightarrow{a_3} s_5 \xrightarrow{a_13}\cdots<br>$$<br>$(s_1,a_2)$ is called a visit.</p>
</li>
<li><p><font color=blue>First visit</font>: In the above trajectory $(s_1,a_2)$ is visited twice. If we only count the first-time visit, such kind of strategy is called <em>first-visit</em>.</p>
</li>
<li><p><font color=blue>Every visit</font>: If every time state-action pair is visited and the rest of the episode is used to estimate its action value, such a strategy is called <em>every-visit</em>.</p>
</li>
</ul>
<p>​	For example,</p>
<p>$$<br>\begin{aligned}<br>    s_1 \xrightarrow{a_2} s_2 \xrightarrow{a_4} s_1 \xrightarrow{a_2} s_2 \xrightarrow{a_3} s_5 \xrightarrow{a_13}\cdots &amp; \quad [\textcolor{blue}{\text{origianl episode}}]	\<br>    s_2 \xrightarrow{a_4} s_1 \xrightarrow{a_2} s_2 \xrightarrow{a_3} s_5 \xrightarrow{a_13}\cdots &amp; \quad [\text{episode starting from }(s_2,a_4)]	\<br>    s_1 \xrightarrow{a_2} s_2 \xrightarrow{a_3} s_5 \xrightarrow{a_13}\cdots &amp; \quad [\textcolor{red}{\text{episode starting from }(s_1,a_2)}]	\<br>    s_2 \xrightarrow{a_3} s_5 \xrightarrow{a_13}\cdots &amp; \quad [\text{episode starting from }(s_2,a_3)]	\<br>    s_5 \xrightarrow{a_13}\cdots &amp; \quad [\text{episode starting from }(s_2,a_4)]	<br>\end{aligned}<br>$$</p>
<p>Suppose we need to estimate the action value $q_\pi(s_1,a_2)$. The <em>first-visit</em> only counts the first visit of $(s_1,a_2)$. So we need a huge numbers of episodes starting from $(s_1,a_2)$. As the blue one episode in the above equations. The <em>every-visi</em>t counts every time the visit of $(s_1,a_2)$. Hence, we can use the blue one and the red one to estimate the action value $q_\pi(s_1,a_2)$. In this way, the samples in the episode are utilized more sufficiently.</p>
<p><strong>Using sample more efficiently</strong>:</p>
<p>​	From the example, we are informed that <font color=blue>if an episode is sufficiently long so that it can visit all the state-action pairs many times, then this single episode is sufficient to estimate all the action values by using the every-visit strategy</font>. However, one single episode is pretty ideal result. Because the samples obtained by the <em>every-visit</em> is relevant due to the trajectory starting from the second visit is merely a subset of the trajectory starting from the first. Nevertheless, if the two visits are far away from each other, which means there is a siginificant non-overlap portion, the relevance would not be strong. Moreover, the relevance can be further suppressed due to the discount rate. <font color=blue>Therefore, when there are few episodes and each episode is very long, the every-visit strategy is a good option</font>.</p>
<p><strong>Using estimation more efficiently</strong>:</p>
<p>​	The first startegy in MC Basic, in the policy evaluation step, to collect all the episodes starting from the same state-action pair and then approximate the action value using the average return of these episodes. The drawback of the strategy is that the agent must wait until all episodes have been collected.</p>
<p>​	The second strategy, which can overcome the drawback, is to <font color=blue>use the return of a single episode to approximate the corresponding action value</font> (the idea of stochastic estimation). In this way, we can improve the policy in an <font color=blue>episode-by-episode</font> fasion.</p>
<p><strong>Pesudocode</strong>:</p>
<img src="https://pic.imgdb.cn/item/658bfac5c458853aef6b25fb.png" class="" width="800" height="500" title="state-transform">


<p><font color=blue>MC Exploring Starts algorithm compared to MC Basic, the sample usage and estimation update are more efficient</font>.</p>
<h3 id="5-5-Monte-Carlo-epsilon-Greedy"><a href="#5-5-Monte-Carlo-epsilon-Greedy" class="headerlink" title="5.5 Monte Carlo $\epsilon$-Greedy"></a>5.5 Monte Carlo $\epsilon$-Greedy</h3><p>​	Why is exploring starts important? In theory, exploring starts is necessary to find optimal policies. Only if every state-action pair is well explored, can we select optimal policies. However, in practice, exploring starts is difficult to achieve. Because its difficult to collect episodes starting from every state-action pair.</p>
<p>​	We can use <em>soft policy</em> to remove the requirement of exploring starts. There are many soft policies. The most common one is $\textcolor{blue}{\epsilon}$<font color=blue>-greedy</font>. The $\epsilon$-greedy has the form of<br>$$<br>\pi(a|s) &#x3D;<br>\left {<br>    \begin{aligned}<br>    1 - \frac{\epsilon}{|\mathcal{A}(s)|}(|\mathcal{A(s)}|-1), &amp; \quad \text{for the geedy action}	\<br>    \frac{\epsilon}{|\mathcal{A}(s)|}, &amp; \quad \text{for the other } |\mathcal{A}(s)|-1 \text{ actions}<br>    \end{aligned}<br>\right.<br>$$<br>where $|\mathcal{A}(s) |$ denotes the number of actions associated with $s$. It is worth noting taht the probability to take the greedy action is always greater than other action, because<br>$$<br>1 - \frac{\epsilon}{|\mathcal{A}(s)|}(|\mathcal{A(s)}|-1) &#x3D;<br>1 - \epsilon + \frac{\epsilon}{|\mathcal{A}(s)|} \ge<br>\frac{\epsilon}{|\mathcal{A}(s)|}<br>$$<br>for any $\epsilon\in[0,1]$, when $\epsilon&#x3D;0$ the $\epsilon$-greedy becomes greedy policy.</p>
<p><strong>Pesudocode</strong>:</p>
<img src="https://pic.imgdb.cn/item/658bfae1c458853aef6b7740.png" class="" width="800" height="500" title="state-transform">

<hr>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>  <a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1sd4y167NS/?spm_id_from=333.999.0.0">赵世钰老师的课程</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://shenmuxin.github.io">Shen Mu Xin</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://shenmuxin.github.io/2023/07/24/Reinforcement-Learning-with-Code-Chapter-5-Monte-Carlo-Learning/">http://shenmuxin.github.io/2023/07/24/Reinforcement-Learning-with-Code-Chapter-5-Monte-Carlo-Learning/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://shenmuxin.github.io" target="_blank">Shen Mu Xin's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/RL/">RL</a><a class="post-meta__tags" href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><a class="post-meta__tags" href="/tags/Deep-Learning/">Deep Learning</a></div><div class="post_share"><div class="social-share" data-image="https://pic.imgdb.cn/item/658bde8dc458853aef14c63a.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/12/28/hexo-github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2-%E6%95%99%E7%A8%8B-%E4%B8%80-%E6%90%AD%E5%BB%BA%E5%9F%BA%E6%9C%AC%E7%8E%AF%E5%A2%83%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/" title="hexo+github搭建个人博客 教程(一)搭建基本环境与基本操作"><img class="cover" src="https://pic.imgdb.cn/item/658cd678c458853aef7408f6.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">hexo+github搭建个人博客 教程(一)搭建基本环境与基本操作</div></div></a></div><div class="next-post pull-right"><a href="/2023/07/23/Reinforcement-Learning-with-Code-Chapter-4-Value-Iteration-and-Policy-Iteration/" title="Reinforcement Learning with Code [Chapter 4. Value Iteration and Policy Iteration]"><img class="cover" src="https://pic.imgdb.cn/item/658bde8dc458853aef14c63a.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Reinforcement Learning with Code [Chapter 4. Value Iteration and Policy Iteration]</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/07/02/Reinforcement-Learning-with-Code-Chapter-1-Basic-Concepts/" title="Reinforcement Learning with Code [Chapter 1. Basic Concepts]"><img class="cover" src="https://pic.imgdb.cn/item/658bde8dc458853aef14c63a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-07-02</div><div class="title">Reinforcement Learning with Code [Chapter 1. Basic Concepts]</div></div></a></div><div><a href="/2023/07/21/Reinforcement-Learning-with-Code-Chapter-2-State-Value-and-Bellman-Equation/" title="Reinforcement Learning with Code [Chapter 2. State Value and Bellman Equation]"><img class="cover" src="https://pic.imgdb.cn/item/658bde8dc458853aef14c63a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-07-21</div><div class="title">Reinforcement Learning with Code [Chapter 2. State Value and Bellman Equation]</div></div></a></div><div><a href="/2023/07/23/Reinforcement-Learning-with-Code-Chapter-3-Optimal-State-Value-and-Bellman-Optimal-Equation/" title="Reinforcement Learning with Code [Chapter 3. Optimal State Value and Bellman Optimal Equation]"><img class="cover" src="https://pic.imgdb.cn/item/658bde8dc458853aef14c63a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-07-23</div><div class="title">Reinforcement Learning with Code [Chapter 3. Optimal State Value and Bellman Optimal Equation]</div></div></a></div><div><a href="/2023/07/23/Reinforcement-Learning-with-Code-Chapter-4-Value-Iteration-and-Policy-Iteration/" title="Reinforcement Learning with Code [Chapter 4. Value Iteration and Policy Iteration]"><img class="cover" src="https://pic.imgdb.cn/item/658bde8dc458853aef14c63a.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-07-23</div><div class="title">Reinforcement Learning with Code [Chapter 4. Value Iteration and Policy Iteration]</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Shen Mu Xin</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">20</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">26</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/shenmuxin" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="https://blog.csdn.net/qq_44940689?type=blog" target="_blank" title="Csdn"><i class="fa-solid fa-c" style="color: #24292e;"></i></a><a class="social-icon" href="https://github.com/shenmuxin" target="_blank" title="Resume"><i class="fa-solid fa-file" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">你的手表走一秒钟是多久</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Reinforcement-Learning-with-Code"><span class="toc-number">1.</span> <span class="toc-text">Reinforcement Learning with Code</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Chapter-5-Monte-Carlo-Learning"><span class="toc-number">1.1.</span> <span class="toc-text">Chapter 5. Monte Carlo Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-Law-of-large-numbers"><span class="toc-number">1.1.1.</span> <span class="toc-text">5.1 Law of large numbers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-Simple-example"><span class="toc-number">1.1.2.</span> <span class="toc-text">5.2 Simple example</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-Monte-Carlo-Basic"><span class="toc-number">1.1.3.</span> <span class="toc-text">5.3 Monte Carlo Basic</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-Monte-Carlo-Exploring-Starts"><span class="toc-number">1.1.4.</span> <span class="toc-text">5.4 Monte Carlo Exploring Starts</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-5-Monte-Carlo-epsilon-Greedy"><span class="toc-number">1.1.5.</span> <span class="toc-text">5.5 Monte Carlo $\epsilon$-Greedy</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reference"><span class="toc-number">1.2.</span> <span class="toc-text">Reference</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/05/24/step%E6%88%96stl%E6%96%87%E4%BB%B6%E8%BD%ACurdf%E5%B9%B6%E6%B7%BB%E5%8A%A0%E5%88%B0%E6%9C%BA%E6%A2%B0%E8%87%82%E4%B8%8A/" title="step或stl文件转urdf并添加到机械臂上"><img src="https://pic.imgdb.cn/item/66d2f659d9c307b7e940b43f.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="step或stl文件转urdf并添加到机械臂上"/></a><div class="content"><a class="title" href="/2024/05/24/step%E6%88%96stl%E6%96%87%E4%BB%B6%E8%BD%ACurdf%E5%B9%B6%E6%B7%BB%E5%8A%A0%E5%88%B0%E6%9C%BA%E6%A2%B0%E8%87%82%E4%B8%8A/" title="step或stl文件转urdf并添加到机械臂上">step或stl文件转urdf并添加到机械臂上</a><time datetime="2024-05-24T06:44:15.000Z" title="发表于 2024-05-24 14:44:15">2024-05-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/05/21/Graspnet-Astra2%E7%9B%B8%E6%9C%BA%E5%AE%9E%E7%8E%B0%E9%83%A8%E7%BD%B2/" title="Graspnet+Astra2相机实现部署"><img src="https://pic.imgdb.cn/item/66d2f4e6d9c307b7e93f943b.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Graspnet+Astra2相机实现部署"/></a><div class="content"><a class="title" href="/2024/05/21/Graspnet-Astra2%E7%9B%B8%E6%9C%BA%E5%AE%9E%E7%8E%B0%E9%83%A8%E7%BD%B2/" title="Graspnet+Astra2相机实现部署">Graspnet+Astra2相机实现部署</a><time datetime="2024-05-21T01:44:15.000Z" title="发表于 2024-05-21 09:44:15">2024-05-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/05/10/Ubuntu%E4%B8%AD%E5%A4%8D%E7%8E%B0Graspnet/" title="Ubuntu中复现Graspnet"><img src="https://pic.imgdb.cn/item/66d2f85fd9c307b7e9439b4e.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Ubuntu中复现Graspnet"/></a><div class="content"><a class="title" href="/2024/05/10/Ubuntu%E4%B8%AD%E5%A4%8D%E7%8E%B0Graspnet/" title="Ubuntu中复现Graspnet">Ubuntu中复现Graspnet</a><time datetime="2024-05-10T05:22:15.000Z" title="发表于 2024-05-10 13:22:15">2024-05-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/03/31/C-STL%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/" title="C++ STL简明教程"><img src="https://pic.imgdb.cn/item/66d2b0fbd9c307b7e9e55370.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="C++ STL简明教程"/></a><div class="content"><a class="title" href="/2024/03/31/C-STL%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/" title="C++ STL简明教程">C++ STL简明教程</a><time datetime="2024-03-31T05:55:29.000Z" title="发表于 2024-03-31 13:55:29">2024-03-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/02/22/%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E6%8E%A7%E5%88%B6MPC%E8%AF%A6%E8%A7%A3%EF%BC%88%E9%99%84%E5%B8%A6%E6%A1%88%E4%BE%8B%E5%AE%9E%E7%8E%B0%EF%BC%89/" title="模型预测控制MPC详解（附带案例实现）"><img src="https://pic.imgdb.cn/item/66d2eed6d9c307b7e93b5a37.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="模型预测控制MPC详解（附带案例实现）"/></a><div class="content"><a class="title" href="/2024/02/22/%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E6%8E%A7%E5%88%B6MPC%E8%AF%A6%E8%A7%A3%EF%BC%88%E9%99%84%E5%B8%A6%E6%A1%88%E4%BE%8B%E5%AE%9E%E7%8E%B0%EF%BC%89/" title="模型预测控制MPC详解（附带案例实现）">模型预测控制MPC详解（附带案例实现）</a><time datetime="2024-02-22T05:55:29.000Z" title="发表于 2024-02-22 13:55:29">2024-02-22</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Shen Mu Xin</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const initGitalk = () => {
    const gitalk = new Gitalk(Object.assign({
      clientID: 'bb96298695bae7b9b1b6',
      clientSecret: 'e5bb4ac1d3493a60b1e9886eca7f020fe3ed6a46',
      repo: 'shenmuxin.github.io',
      owner: 'shenmuxin',
      admin: ['shenmuxin'],
      id: '08a316efa5716804afbcc318285a6c93',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  const loadGitalk = async() => {
    if (typeof Gitalk === 'function') initGitalk()
    else {
      await getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
      await getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js')
      initGitalk()
    }
  }
  
  const commentCount = n => {
    const isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
    if (isCommentCount) {
      isCommentCount.textContent= n
    }
  }

  if ('Gitalk' === 'Gitalk' || !false) {
    if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
    else loadGitalk()
  } else {
    window.loadOtherComment = loadGitalk
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>