<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/12/24/hello-world/"/>
      <url>/2023/12/24/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Reinforcement Learning with Code [Chapter 4. Value Iteration and Policy Iteration]</title>
      <link href="/2023/07/23/Reinforcement-Learning-with-Code-Chapter-4-Value-Iteration-and-Policy-Iteration/"/>
      <url>/2023/07/23/Reinforcement-Learning-with-Code-Chapter-4-Value-Iteration-and-Policy-Iteration/</url>
      
        <content type="html"><![CDATA[<h1 id="Reinforcement-Learning-with-Code"><a href="#Reinforcement-Learning-with-Code" class="headerlink" title="Reinforcement Learning with Code"></a>Reinforcement Learning with Code</h1><p>This note records how the author begin to learn RL. Both theoretical understanding and code practice are presented. Many material are referenced such as ZhaoShiyu’s <em>Mathematical Foundation of Reinforcement Learning</em>, .</p><h2 id="Chapter-4-Value-Iteration-and-Policy-Iteration"><a href="#Chapter-4-Value-Iteration-and-Policy-Iteration" class="headerlink" title="Chapter 4. Value Iteration and Policy Iteration"></a>Chapter 4. Value Iteration and Policy Iteration</h2><p>​Value iteration and policy iteration have a common name called <font color=blue>dynamic programming</font>. Dynamic programming is model-based algorithm, which is the simplest RL algorithm. Its helpful to us to understand the model-free algorithm.</p><h3 id="4-1-Value-iteration"><a href="#4-1-Value-iteration" class="headerlink" title="4.1 Value iteration"></a>4.1 Value iteration</h3><p>​Value iteration is solving the Bellman optimal equation directly.</p><p><strong>Matrix-vector form</strong>:</p><p>​The <font color=red>value iteration</font> is exactly the algorithm suggested by the contraction mapping in chapter 3. Value iteration concludes two parts. First, in every iteration is called <em>policy update</em>. </p><p>$$<br>\pi_{k+1} &#x3D; \arg \textcolor{red}{\max_{\pi_k}(r_{\pi_k} + \gamma P_\pi v_k)}<br>$$<br>Second, the step is <em>value update</em>. Mathematically, it is to substitute $\pi_{k+1}$ and do the following operation:</p><p>$$<br>v_{k+1} &#x3D; r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}}v_k<br>$$</p><p>The above algorithm is matrix-vector form which is useful to understand the core idea. <font color=red>The above equation is iterative which means we calculate</font> $v_{k+1}$ <font color=red>is only one step calculation</font>.</p><p>There is a <font color=red>misunderstanding</font> that we doesn’t use Bellman equation to calculate state value $v_{k+1}$ directly. Instead, when we use greedy policy update strategy the state value $v_{k+1}$ is actually the maximum action value $\max_a q_k(s,a)$.</p><p>The value iteration includes solving the Bellman optimal equation part as show in red.</p><p><strong>Elementwise form</strong>:</p><p>​First, the elementwise form <em>policy update</em> is </p><p>$$<br>\begin{aligned}<br>    \pi_{k+1} &amp; &#x3D; \arg \max_{\pi_k}(r_{\pi_k} + \gamma P_\pi v_k)\quad \text{(matrx-vector form)}\newline<br>    \to \pi_{k+1}(s) &amp; &#x3D; \arg \max_{\pi_k} \sum_a \pi_k(a|s)<br>    \underbrace{<br>    \Big(\sum_r p(r|s,a)r + \sum_{s^\prime} p(s^\prime|s,a) v_k(s^\prime)\Big)}<em>{q</em>{\pi_k}(s,a)} \quad \text{(elementwise  form)}\newline<br>    \to \pi_{k+1}(s) &amp; &#x3D; \arg \max_{\pi_k} \sum_a \pi_k(a|s) q_{\pi_k}(s,a)\newline<br>    \to \pi_{k+1}(s) &amp; &#x3D;  \sum_a \pi_k(a|s) \arg \max_{a_k}q_{\pi_k}(s,a)<br>\end{aligned}<br>$$</p><p>Then use the greedy policy update algorithm</p><p>$$<br>\pi_{k+1}(a|s) &#x3D;<br>\left {<br>    \begin{aligned}<br>        &amp; 1 &amp; a&#x3D;a_k^*(s)\newline<br>        &amp; 0 &amp; a\ne a_k^*(s)<br>    \end{aligned}<br>\right.<br>$$</p><p>where $a_k^*(s) &#x3D; \arg\max_a q_k(a,s)$.</p><p>​Second, the elementwise form <em>value update</em> is </p><p>$$<br>\begin{aligned}<br>    v_{k+1} &amp; &#x3D; r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}}v_k\quad \text{(matrx-vector form)}\newline<br>    \to v_{k+1}(s) &amp; &#x3D; \sum_a \pi_{k+1}(a|s)<br>    \underbrace{<br>    \Big(\sum_r p(r|s,a)r+ \gamma \sum_{s^\prime}p(s^\prime|s,a)v_k(s^\prime) \Big)}_{q_k(s,a)}\quad \text{(elementwise  form)}<br>\end{aligned}<br>$$</p><p><font color=blue>Since $\pi_{k+1}$ is a greedy policy, the above equation is simply</font></p><p>$$<br>v_{k+1}(s) &#x3D; \max_a q_k(s,a)<br>$$</p><p><strong>Pesudocode</strong>:</p><img src="https://pic.imgdb.cn/item/658be0b6c458853aef1b95ef.png" class="" width="800" height="400" title="state-transform"><h3 id="4-2-Policy-iteration"><a href="#4-2-Policy-iteration" class="headerlink" title="4.2 Policy iteration"></a>4.2 Policy iteration</h3><p>​The <font color=red>policy iteration</font> is not an algorithm directly solving the Bellman optimality equation. However, it has an intimate relationship to value iteration. The policy iteration includes two parts <font color=blue>policy evaluation</font> and <font color=blue>policy improvement</font>.</p><p><strong>Policy evaluation</strong>:</p><p>​The first step is policy evaluation. Mathematically, it is to sovle Bellman equation of $\pi_k$:</p><p>$$<br>v_{\pi_k} &#x3D; r_{\pi_k} + \gamma P_{\pi_k}v_{\pi_k}<br>$$</p><p>This is the matrix-vector form of the Bellman equation, where $r_{\pi_k}$ and $P_{\pi_k}$ are known. Here, $v_{\pi_k}$ is the state value to be solved.</p><p><font color=blue>How to calculate state value</font> $v_{\pi_k}$ is important. In section 2.4 we have already introduced how to solve Bellman equation to get state value $v_{\pi_k}$ as following</p><p>$$<br>\textcolor{blue}{v_{\pi_k}^{(j+1)} &#x3D; r_{\pi_k} + \gamma P_{\pi_k}v_{\pi_k}^{(j)}}, \quad j&#x3D;0,1,2,\dots<br>$$</p><p><font color=red>Its clear that policy iteration is directly calculate the state value using the above equation, which means the calculation is infity step calculation</font>.</p><p><strong>Policy improvement</strong>:</p><p>​The second step is to imporve the policy. How to do that? Once $v_{\pi_k}$ is calculated in the first step, a new and improved policy could be obtained as </p><p>$$<br>\pi_{k+1} &#x3D; \arg \max_\pi (r_\pi + \gamma P_\pi v_{\pi_k})<br>$$</p><p> <strong>Elementwise form</strong>:</p><p>​The policy evaluation step is to solve $v_{\pi_k}$ from the Bellman equation dirctly.</p><p>$$<br>\begin{aligned}<br>    v_{\pi_k}^{(j+1)} &amp; &#x3D; r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}^{(j)}\quad \text{(matrix-vector form)} \newline<br>    \to v_{\pi_k}^{(j+1)}(s) &amp; &#x3D; \sum_a \pi_k(a|s) \Big(\sum_r p(r|s,a)r + \sum_{s^\prime} p(s^\prime|s,a) v_{\pi_k}^{(j)} (s^\prime)\Big) \quad \text{(elementwise form)}<br>\end{aligned}<br>$$<br>where $j&#x3D;0,1,2,\dots$</p><p>The policy improvement step is to solve $\pi_{k+1}&#x3D;\arg \max_\pi (r_\pi + \gamma P_{\pi_k}v_{\pi_k})$.</p><p>$$<br>\begin{aligned}<br>    \pi_{k+1} &amp; &#x3D;\arg \max_{\pi_k} (r_{\pi_k} + \gamma P_{\pi_k}v_{\pi_k})\quad \text{(matrix-vector form)}\newline<br>    \to \pi_{k+1}(s) &amp; &#x3D; \arg \max_{\pi_k}  \sum_a \pi(a|s)<br>    \underbrace{<br>    \Big(\sum_r p(r|s,a)r + \sum_{s^\prime} p(s^\prime|s,a) v_{\pi_k} (s^\prime)\Big)}<em>{q</em>{\pi_k}(s,a)}<br>    \quad \text{(elementwise form)}\newline<br>    \to \pi_{k+1}(s) &amp; &#x3D;  \sum_a \pi(a|s) \arg \max_a q_{\pi_k}(s,a)<br>\end{aligned}<br>$$</p><p>Let $a^*<em>k(s) &#x3D; \arg \max_a q</em>{\pi_k}(s,a)$. The greedy optimal policy is</p><p>$$<br>\pi_{k+1}(a|s) &#x3D;<br>\left {<br>    \begin{aligned}<br>        &amp; 1 &amp; a&#x3D;a_k^*(s)\newline<br>        &amp; 0 &amp; a\ne a_k^*(s)<br>    \end{aligned}<br>\right.<br>$$</p><p><strong>Pesudocode:</strong></p><img src="https://pic.imgdb.cn/item/658be0dfc458853aef1c2798.png" class="" width="800" height="400" title="state-transform"><hr><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>  <a href="https://www.bilibili.com/video/BV1sd4y167NS/?spm_id_from=333.999.0.0">赵世钰老师的课程</a></p>]]></content>
      
      
      <categories>
          
          <category> Reinforcement Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reinforcement Learning with Code [Chapter 3. Optimal State Value and Bellman Optimal Equation]</title>
      <link href="/2023/07/23/Reinforcement-Learning-with-Code-Chapter-3-Optimal-State-Value-and-Bellman-Optimal-Equation/"/>
      <url>/2023/07/23/Reinforcement-Learning-with-Code-Chapter-3-Optimal-State-Value-and-Bellman-Optimal-Equation/</url>
      
        <content type="html"><![CDATA[<h1 id="Reinforcement-Learning-with-Code"><a href="#Reinforcement-Learning-with-Code" class="headerlink" title="Reinforcement Learning with Code."></a>Reinforcement Learning with Code.</h1><p>This note records how the author begin to learn RL. Both theoretical understanding and code practice are presented. Many material are referenced such as ZhaoShiyu’s <em>Mathematical Foundation of Reinforcement Learning</em>, .</p><h2 id="Chapter-3-Optimal-State-Value-and-Bellman-Optimality-Equation"><a href="#Chapter-3-Optimal-State-Value-and-Bellman-Optimality-Equation" class="headerlink" title="Chapter 3. Optimal State Value and Bellman Optimality Equation"></a>Chapter 3. Optimal State Value and Bellman Optimality Equation</h2><h3 id="3-1-How-to-define-optimal"><a href="#3-1-How-to-define-optimal" class="headerlink" title="3.1 How to define optimal"></a>3.1 How to define optimal</h3><p>​One core idea is that we use the action value to judge the optimality of the action. If we update the policy to select the action with the <em>greatest action value</em>, we could find a better policy.</p><p>​(<font color=blue>Optimal policy and optimal state value</font>). <em>A policy</em> $\pi^*$ <em>is optimal if</em> $v_{\pi^*}(s) \ge v_\pi(s)$ <em>for all</em> $s\in\mathcal{S}$ <em>and for any other policy</em> $\pi$. <em>The state values of</em>  $\pi^*$ <em>are the optimal state values</em>.</p><p><font color=blue>Why does the definition works? One intuitive explanation is that state value $v_\pi(s)$ denote the mean of return along the trajectory following policy $\pi$. If the policy $\pi^*$ has greatest expectation of return, hence we can believe that the policy $\pi^*$ is optimal.</font></p><h3 id="3-2-Bellman-optimal-equation-BOE"><a href="#3-2-Bellman-optimal-equation-BOE" class="headerlink" title="3.2 Bellman optimal equation (BOE)"></a>3.2 Bellman optimal equation (BOE)</h3><p>​The Bellamn optimal eqaution (BOE) is </p><p>$$<br>\begin{aligned}<br>v(s) &amp; &#x3D; \max_\pi \sum_a \pi(a|s) \Big[ \sum_r p(r|s,a)r + \gamma \sum_{s^\prime} p(s^{\prime}|s,a) v_\pi(s^{\prime}) \Big] \newline<br>\textcolor{red}{v(s)} &amp; \textcolor{red}{&#x3D; \max_\pi \sum_a \pi(a|s) q_\pi(s,a)}<br>\end{aligned}<br>$$</p><p>There is a problem that the BOE has two unknown variables $q_\pi(s,a)$ and $\pi(a|s)$. How can we solve the BOE?</p><p>​The idea is that we can <font color=blue>fix one variable</font> and solve the maximization problem. For Bellman optimal equation, we can <font color=blue>fix variable $\pi(a|s)$ for all $a\in\mathcal{A}(s)$</font> and then by maximize the state value to find the optimal policy $\pi$.</p><p>​Before analysis, we consider one example first. Suppose $x_1,x_2,x_3\in\mathbb{R}$ are given. Find $c_1^*,c_2^*,c_3^*$ solving<br>$$<br>\max_{c_1,c_2,c_3} c_1x_1+c_2x_2+c_3x_3\newline<br>\text{subject to } c_1+c_2+c_3&#x3D;1<br>$$<br>Without generality, suppose $x_3 \ge x_1, x_2$. Then, the optimal solution is $c_3^*&#x3D;1$ and $c_1^*&#x3D;c_2^*&#x3D;0$. This is because for any $c_1,c_2,c_3$<br>$$<br>x_3 &#x3D; (c_1+c_2+c_3)x_3 &#x3D; c_1x_3 + c_2x_3 + c_3x_3 \ge c_1x_1 +c_2x_2+c_3x_3<br>$$<br>​Hence, inspired by the above example, considering that $\sum_a \pi(a|s)&#x3D;1$, we have</p><p>$$<br>\begin{aligned}<br>v(s) &amp; &#x3D; \max_\pi \sum_a \pi(a|s) q_\pi(s,a) \newline<br>&amp; &#x3D; \sum_a \pi(a|s) \max_\pi q_\pi(s,a) \quad \text{by fix } \pi(a|s) \newline<br>&amp; &#x3D; \sum_a \pi(a|s) \max_{a\in\mathcal{A}} q_\pi(s,a) \newline<br>&amp; \le \max_{a\in\mathcal{A}} q_\pi(s,a)<br>\end{aligned}<br>$$</p><p>where the equality is achieved when $a&#x3D;a^*$, $\pi(a|s)&#x3D;1$ when $a\ne a^*$, $\pi(a|s)&#x3D;0$.</p><p>$$<br>a^* &#x3D; \text{arg} \max_a q(s,a)<br>$$</p><p>This policy is often called <font color=blue>greedy policy</font>.</p><h3 id="3-3-Matrix-vector-form-of-Bellman-optimal-equation"><a href="#3-3-Matrix-vector-form-of-Bellman-optimal-equation" class="headerlink" title="3.3 Matrix-vector form of Bellman optimal equation"></a>3.3 Matrix-vector form of Bellman optimal equation</h3><p>Refering to matrix-vector form of Bellman equation, it’s obvious to get matrix-vector form of Bellman optimal equation as follows<br>$$<br>\textcolor{blue} {v &#x3D; \max_\pi (r_\pi +\gamma P_\pi v)}<br>$$<br>where $v\in\mathbb{R^{|\mathcal{S}|}}$. The structure of $r_\pi$ and $P_\pi$ are the same as those in the matrix-vector form of Bellman equation:</p><p>$$<br>[r_\pi]<em>s \triangleq \sum_a \pi(a|s) \sum_r p(r|s,a)r,<br>[P_\pi]</em>{s,s^\prime} &#x3D; \sum_a \pi(a|s) \sum_{s^\prime} p(s^\prime|s,a)<br>$$</p><p>Furthermore, denote the right hand side as </p><p>$$<br>f(v) \triangleq \max_\pi(r_\pi + \gamma P_\pi v)<br>$$</p><p>Hence, we have the Bellman optimal equation as</p><p>$$<br>v&#x3D;f(v) \newline<br>f(v) - v &#x3D; 0<br>$$</p><p><font color=blue>It turns solving Bellman optimal equation to solving find the root of function</font> $\textcolor{blue}{f(v) - v &#x3D;0}$.</p><h3 id="3-4-Contraction-mapping-theorem"><a href="#3-4-Contraction-mapping-theorem" class="headerlink" title="3.4 Contraction mapping theorem"></a>3.4 Contraction mapping theorem</h3><p>​<font color=red>Fixed point</font>: consider a function $f(x)$ where $x\in\mathbb{R}^b$ and $f:\mathbb{R}^b\to\mathbb{R}^b$. A point $x^*$ is called a fixed point if<br>$$<br>f(x^*) &#x3D; x^*<br>$$<br>The function $f$ is called a <font color=red>contracting mapping</font> if there <font color=blue>exists</font> $\gamma\in(0,1)$ such that<br>$$<br>||f(x_1)-f(x_2)|| \le \gamma ||x_1 - x_2||<br>$$<br>for any $x_1, x_2\in \mathbb{R}$.</p><p>​(<font color=red>Contraction mapping theorem</font>) For any equation that has the form of $x&#x3D;f(x)$ where $x$ and $f(x)$ are real vectors, if $f$ is a contraction mapping, then</p><ul><li><p>Existence: There exists a fixed point $x^*$ satisfying $f(x^*)&#x3D;x^*$.</p></li><li><p>Uniqueness: The fixed point $x^*$ is unique.</p></li><li><p>Algorithm: Consider the iterative process:<br>$$<br>x_{k+1} &#x3D; f(x_k)<br>$$<br>where $k&#x3D;0,1,2,\dots$. Then , $x_k\to x^*$ as $k\to \infty$ for any initial guess $x_0$. Moreover, the convergence rate is exponentially fast.</p></li></ul><h3 id="3-5-Solution-of-BOE"><a href="#3-5-Solution-of-BOE" class="headerlink" title="3.5 Solution of BOE"></a>3.5 Solution of BOE</h3><p>​(Contraction property of BOE). The function $f(v) \triangleq \max_\pi(r_\pi + \gamma P_\pi v)$ in the BOE is a contraction mapping statisfying<br>$$<br>||f(v_1)-f(v_2)|| \le \gamma ||v_1 - v_2||<br>$$<br>where $v_1,v_2\in\mathbb{R}^{|\mathcal{S}|}$ are any two vectors and $\gamma\in(0,1)$ is the discounted rate.</p><p>The proof is omitted.</p><p>​Using contraction mapping theorem to solve BOE, we have the following theorem:</p><p>​(<font color=red>Existence, Uniqueness, Algorithm</font>). <em>For the BOE</em> $v&#x3D;f(v) \triangleq \max_\pi(r_\pi + \gamma P_\pi v)$, <em>there always exists a unique solution</em> $v^*$, <em>which can be solved iteratively by</em><br>$$<br>v_{k+1} &#x3D; f(v_k) &#x3D; \max_\pi(r_\pi + \gamma P_\pi v_k), \quad k&#x3D;0,1,\dots<br>$$<br><em>The sequence</em> ${v(k)}$ <em>converges to optimal solution of BOE</em> $v^*$ <em>exponentially fast given any inital guess</em> $v_0$.</p><p>This algorithm is actually called <font  color=blue>value iteration</font>. </p><p>​<font color=blue>First</font>, following the algorithm above will give us the optimal state value $v^*$. <font color=blue>Second</font>, we solve the unknown policy $\pi$ in the BOE. Suppose $v^*$ has been obtained and<br>$$<br>\pi^* &#x3D; \text{arg}\max_\pi(r_\pi+\gamma P_\pi v^*)<br>$$<br>Then, $v^*$ and $\pi^*$ statisfy<br>$$<br>v^* &#x3D; r_{\pi^*} + \gamma P_{\pi^*} v^*<br>$$<br>​(<font color=blue>Greedy optimal policy</font>). <em>For any</em> $s\in\mathcal{S}$, <em>the deterministic greedy policy</em> </p><p>where the equality is achieved when $a&#x3D;a^*$, $\pi(a|s)&#x3D;1$ when $a\ne a^*$, $\pi(a|s)&#x3D;0$.</p><p><em>is an optimal policy solving the BOE. Here</em><br>$$<br>a^*(s) &#x3D; \text{arg} \max_a q^*(a,s)<br>$$<br><em>where</em><br>$$<br>q^*(s,a) \triangleq \sum_r p(r|s,a)r + \gamma \sum_{s^\prime} p(s^{\prime}|s,a) v^*(s)<br>$$<br>​The matrix-vecotr form of the optimal policy is $\pi^*&#x3D;\text{arg}\max_\pi(r_\pi+\gamma P_\pi v^*(s))$. Its <font color=blue>elementwise form</font> is</p><p>$$<br>\begin{aligned}<br> \pi^* &amp; &#x3D; \text{arg}\max_\pi(r_\pi+\gamma P_\pi v^*)\newline<br> \to \pi^*(s) &amp; &#x3D; \text{arg}\max_\pi \sum_a \pi(a|s) \Big( \sum_r p(r|s,a)r + \gamma \sum_{s^\prime} p(s^{\prime}|s,a) v^*(s)\Big)\newline<br> \to \pi^*(s) &amp; &#x3D; \text{arg}\max_\pi \sum_a \pi(a|s) q^*(s,a),\quad \text{for s}\in\mathcal{S}<br>\end{aligned}<br>$$</p><p>It is clear that $\sum_a \pi(a|s)q^*(s,a)$ is maximized if $\pi(s)$ selects the action with the greatest value of $q^*(s,a)$.</p><hr><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>  <a href="https://www.bilibili.com/video/BV1sd4y167NS/?spm_id_from=333.999.0.0">赵世钰老师的课程</a></p>]]></content>
      
      
      <categories>
          
          <category> Reinforcement Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reinforcement Learning with Code [Chapter 2. State Value and Bellman Equation]</title>
      <link href="/2023/07/21/Reinforcement-Learning-with-Code-Chapter-2-State-Value-and-Bellman-Equation/"/>
      <url>/2023/07/21/Reinforcement-Learning-with-Code-Chapter-2-State-Value-and-Bellman-Equation/</url>
      
        <content type="html"><![CDATA[<h1 id="Reinforcement-Learning-with-Code"><a href="#Reinforcement-Learning-with-Code" class="headerlink" title="Reinforcement Learning with Code."></a>Reinforcement Learning with Code.</h1><p>This note records how the author begin to learn RL. Both theoretical understanding and code practice are presented. Many material are referenced such as ZhaoShiyu’s <em>Mathematical Foundation of Reinforcement Learning</em>, .</p><h2 id="Chapter-2-State-Value-and-Bellman-Equation"><a href="#Chapter-2-State-Value-and-Bellman-Equation" class="headerlink" title="Chapter 2. State Value and Bellman Equation"></a>Chapter 2. State Value and Bellman Equation</h2><h3 id="2-1-State-value"><a href="#2-1-State-value" class="headerlink" title="2.1 State value"></a>2.1 State value</h3><ul><li><p><font color=red>State value</font> is defined as the mean of <font color=blue>all possible returns</font> starting from a state, which is actually the <font color=blue>expectation of return</font> from a specific state.</p></li><li><p>The mathematical definition is as follows:</p><p>Note that the capital letters denote <em>random variables</em>, such as $S_t, S_{t+1}, A_t, R_{t+1}$. In particular, $S_t,S_{t+1}\in\mathcal{S},A_t\in\mathcal{A}(S_t)$ and $R_{t+1}\in\mathcal{R}(S_t,A_t)$. $G_t$ denote the random variable of return.</p><p>Starting from $t$, we can obtain a state-action-reward trajectory:</p><p>$$<br>S_t \xrightarrow{A_t} S_{t+1},R_{t+1} \xrightarrow{A_{t+1}} S_{t+2},R_{t+2} \xrightarrow{A_{t+2}} S_{t+3},R_{t+3} \cdots<br>$$</p><p>The discounted return along the trajectory is </p><p>$$<br>G_t &#x3D; R_{t+1} + \gamma R_{t+2} + \gamma R_{t+3} + \cdots<br>$$</p><p>The state value is defined as:</p><p>$$<br>\textcolor{blue}{v_\pi(s)\triangleq \mathbb{E}[G_t|S_t&#x3D;s]}<br>$$</p><p>which means <font color=blue>start from state</font> $s$ can get the <font color=blue>expectation return</font> along the trajecotry generated by <font color=blue>policy $\pi$ </font>. </p><p>$v_\pi(s)$ is also called <font color=blue>state-value funtion</font>.</p></li></ul><h3 id="2-2-Bellman-Equation"><a href="#2-2-Bellman-Equation" class="headerlink" title="2.2 Bellman Equation"></a>2.2 Bellman Equation</h3><ul><li><p>Bellman equation is a set of linear equations <font color=blue>describing the relationship among the values of all the states</font>.</p><p>For example,</p></li></ul><img src="https://pic.imgdb.cn/item/658bd598c458853aeff51217.png" class="" width="400" height="400" title="state-transform"><p>  Let $v_i$ denote the return obtained starting from $s_i$. The return starting from the four states in figure can be respectively calculated as </p><p>$$<br>v_1 &#x3D; r_1 + \gamma r_2 + \gamma^2 r_3 + \cdots \newline<br>v_2 &#x3D; r_2 + \gamma r_3 + \gamma^2 r_4 + \cdots \newline<br>v_3 &#x3D; r_3 + \gamma r_4 + \gamma^2 r_1 + \cdots \newline<br>v_4 &#x3D; r_4 + \gamma r_1 + \gamma^2 r_2 + \cdots<br>$$</p><p> Using the idea of <em>bootstrapping</em>, we can rewrite it to</p><p>$$<br>v_1 &#x3D; r_1 + \gamma (r_2 + \gamma r_3) + \cdots &#x3D; r_1 + \gamma v_2 \newline<br>v_2 &#x3D; r_2 + \gamma (r_3 + \gamma r_4) + \cdots &#x3D; r_2 + \gamma v_3 \newline<br>v_3 &#x3D; r_3 + \gamma (r_4 + \gamma r_1) + \cdots &#x3D; r_3 + \gamma v_4 \newline<br>v_4 &#x3D; r_4 + \gamma (r_1 + \gamma r_2) + \cdots &#x3D; r_4 + \gamma v_1<br>$$</p><p>  Then rewrite it into matrix form</p><p>$$<br>\textbf{v} &#x3D;  \begin{bmatrix} v_1 \newline v_2 \newline v_3 \newline v_4 \end{bmatrix} , \textbf{r} &#x3D; \begin{bmatrix} r_1 \newline r_2 \newline r_3 \newline r_4 \end{bmatrix}, \textbf{v} &#x3D;  \begin{bmatrix} v_1 \newline v_2 \newline v_3 \newline v_4 \end{bmatrix}<br>$$</p><p>$$<br>\textbf{P} &#x3D;  \begin{bmatrix} 0 &amp; 1 &amp; 0 &amp; 0 \newline 0 &amp; 0 &amp; 1 &amp; 0 \newline 0 &amp; 0 &amp; 0 &amp; 1 \newline 1 &amp; 0 &amp; 0 &amp; 0 \end{bmatrix}<br>$$</p><p> The equation $\textbf{v}&#x3D;\textbf{r}+\gamma\textbf{P}\textbf{v}$ is called Bellman equation.</p><ul><li>Then we can derive Bellman equation from scratch as follows:</li></ul><p>Note that the discounted return random variable $G_t$ can be rewritten as </p><p>$$<br>\begin{aligned}<br>G_t &amp; &#x3D;  R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots\newline<br>&amp; &#x3D;  R_{t+1} + \gamma(R_{t+2}+\gamma R_{t+3} + \cdots) \newline<br>&amp; &#x3D;  R_{t+1} + \gamma G_{t+1}<br>\end{aligned}<br>$$</p><p> where $G_{t+1} &#x3D; R_{t+2} + \gamma R_{t+3} + \cdots$ . This equation establishes the relationship between $G_t$ and $G_{t+1}$. Then the state value can be rewritten as </p><p>$$<br>\begin{aligned}<br>v_\pi(s) &amp; &#x3D;  \mathbb{E}[G_t|S_t&#x3D;s] \newline<br>&amp; &#x3D;  \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t&#x3D;s] \newline<br>&amp; &#x3D;  \mathbb{E}[R_{t+1}|S_t&#x3D;s] + \gamma \mathbb{E}[G_{t+1}|S_t&#x3D;s]<br>\quad \text{(linear property of expectation)}<br>\end{aligned}<br>$$</p><p> The <font color = blue> mean of immediate reward</font> can be written as (use 2.3 conditional expectation)</p><p>$$<br>\begin{aligned}<br>\mathbb{E}[R_{t+1}|S_t&#x3D;s] &amp; &#x3D;  \sum_a \pi(a|s) \mathbb{E}[R_{t+1} | S_t&#x3D;s,A_t&#x3D;a] \quad \text{(conditional expectation)}\newline<br>&amp; &#x3D;  \sum_a \pi(a|s) \sum_r p(r|s,a)r<br>\end{aligned}<br>$$</p><p>  The <font color = blue>mean of future reward</font> can be written as </p><p>$$<br>\begin{aligned}<br>\gamma \mathbb{E}[G_{t+1}|S_t&#x3D;s] &amp; &#x3D;  \gamma \sum_a \pi(a|s) \mathbb{E}[G_{t+1}|S_t&#x3D;s, A_t&#x3D;a]\quad \text{(conditional expectation)}\newline<br>&amp; &#x3D;  \gamma \sum_a \pi(a|s) \sum_{s^\prime} p(s^\prime|s,a) v_\pi(s^\prime)\newline<br>\end{aligned}<br>$$</p><p>Then the state value can be rewritten as <font color=red>Bellman equation (BE)</font> form </p><p>$$<br>\begin{aligned}<br>v_\pi(s) &amp; &#x3D;  \mathbb{E}[G_t|S_t&#x3D;s] \newline<br>&amp; &#x3D;  \mathbb{E}[R_{t+1}|S_t&#x3D;s] + \gamma \mathbb{E}[G_{t+1}|S_t&#x3D;s] \newline<br>&amp; &#x3D;<br>\sum_a \pi(a|s) \sum_r p(r|s,a)r </p><p>+<br>\gamma \sum_a \pi(a|s) \sum_{s^\prime} p(s^\prime|s,a) v_\pi(s^\prime)\newline<br>&amp; &#x3D;  \sum_a \pi(a|s) \Big[\sum_r p(r|s,a)r+ \gamma\sum_{s^\prime} p(s^\prime|s,a) v_\pi(s^\prime)\Big], \quad \text{for all } s\in\mathcal{S}<br>\end{aligned}<br>$$</p><p>Next, we will introduce the matrix vector form of Bellman equation in terms of state value. Let $v_\pi&#x3D;[v_\pi(s_1),v_\pi(s_2),\cdots]^T\in\mathbb{R}^n$, $r_\pi&#x3D;[r_\pi(s_1),r_\pi(s_2),\cdots]^T\in\mathbb{R}^n$, and $P_\pi\in\mathbb{R}^{n\times n}$. Then we have the matrix-vector form of Bellman equation in terms of state value.</p><p>$$<br>\textcolor{red}{v_\pi &#x3D; r + \gamma P_\pi v_\pi} \quad \text{(matrix-vector form)}<br>$$</p><p>where</p><p>$$<br>[r_\pi]<em>s \triangleq \sum_a \pi(a|s) \sum_r p(r|s,a)r,<br>[P_\pi]</em>{s,s^\prime} &#x3D; \sum_a \pi(a|s) \sum_{s^\prime} p(s^\prime|s,a)<br>$$</p><h3 id="2-3-Review-conditional-expectaion"><a href="#2-3-Review-conditional-expectaion" class="headerlink" title="2.3 Review conditional expectaion"></a>2.3 Review conditional expectaion</h3><ul><li>The definition of conditional expectation is</li></ul><p>$$<br>\mathbb{E} &#x3D; [X|A&#x3D;a] &#x3D; \sum_{x}xp(x|a)<br>$$</p><p>  Similar to the law of total probability, we have the law of total expectation:</p><p>$$<br>\mathbb{E}[X] &#x3D; \sum_a p(a) \mathbb{E}[X|A&#x3D;a]<br>$$</p><p>  <em>Proof</em></p><p>  By definition of expectation the right hand side can be written as </p><p>$$<br>\begin{aligned}<br>\mathbb{E}[X] &amp; &#x3D;  \sum_a p(a) \sum_x x p(x|a)\newline<br>&amp; &#x3D;  \sum_x \sum_a p(x|a) p(a)  \quad \text{(law of total probability)}\newline<br>&amp; &#x3D;  \sum_x p(x)\newline<br>&amp; &#x3D;  \mathbb{E}[X]<br>\end{aligned}<br>$$</p><h3 id="2-4-Solve-Bellman-Equation"><a href="#2-4-Solve-Bellman-Equation" class="headerlink" title="2.4 Solve Bellman Equation"></a>2.4 Solve Bellman Equation</h3><p>​From the analysis above we are informed that Bellman equation is</p><p>$$<br>\textcolor{blue}{v_\pi(s)&#x3D;\sum_a \pi(a|s) \Big[\sum_r p(r|s,a)r+ \gamma\sum_{s^\prime} p(s^\prime|s,a) v_\pi(s^\prime)\Big]}<br>$$</p><p>We can rewrite it into the follow form</p><p>$$<br>v_\pi(s) &#x3D; r_\pi(s) + \gamma \sum_{s^\prime} p_\pi(s^\prime|s) v_\pi(s^\prime)<br>$$</p><p>where</p><p>$$<br>\begin{aligned}<br>r_\pi(s) \triangleq \sum_a \pi(a|s) \sum_r p(r|s,a)r\newline<br>\newline<br>\newline<br>\newline<br>\end{aligned} \qquad<br>\begin{aligned}<br>p_\pi(s^\prime|s) &amp; \triangleq \sum_a \pi(a|s) \sum_{s^\prime} p(s^\prime|s,a)\newline<br>&amp; &#x3D; \sum_{s^\prime} \sum_a \pi(a|s) p(s^\prime|s,a)\newline<br>&amp; &#x3D; p_\pi(s^\prime|s)<br>\end{aligned}<br>$$</p><p>​Suppose the states are indexed as $s_i$. For state $s_i$, the Bellman equation is </p><p>$$<br>v_\pi(s_i) &#x3D; r_\pi(s_i) + \gamma \sum_{s_j} p_\pi(s_j|s_i) v_\pi(s_j)<br>$$</p><p>Let $v_\pi&#x3D;[v_\pi(s_1),\dots,v_\pi(s_n)]^T \in \mathbb{R}^n$, $r_\pi&#x3D;[r_\pi(s_1),\dots,r_\pi(s_n)]^T \in \mathbb{R}^n$, and $P_\pi \in \mathbb{R}^{n\times n}$, where $[P_\pi]_{ij} &#x3D; p_\pi(s_j|s_i)$. Hence we have the matrix form as </p><p>$$<br>v_\pi &#x3D; r_\pi + \gamma P_\pi v_\pi<br>$$</p><p>For example, consider four states $s_i$ and four actions $a_i$ the matrix form can be</p><p>$$<br>\underbrace{<br>\begin{bmatrix}<br>v_\pi(s_1) \newline<br>v_\pi(s_2) \newline<br>v_\pi(s_3) \newline<br>v_\pi(s_4)<br>\end{bmatrix}<br>}<em>{v_\pi} &#x3D;<br>\underbrace{<br>\begin{bmatrix}<br>r_\pi(s_1) \newline<br>r_\pi(s_1) \newline<br>r_\pi(s_1) \newline<br>r_\pi(s_1)<br>\end{bmatrix}<br>}</em>{r_\pi} + \gamma<br>\underbrace{<br>\begin{bmatrix}<br>p_\pi(s_1|s_1) &amp; p_\pi(s_2|s_1) &amp; p_\pi(s_3|s_1) &amp; p_\pi(s_4|s_1) \newline<br>p_\pi(s_1|s_2) &amp; p_\pi(s_2|s_2) &amp; p_\pi(s_3|s_2) &amp; p_\pi(s_4|s_2) \newline<br>p_\pi(s_1|s_3) &amp; p_\pi(s_2|s_3) &amp; p_\pi(s_3|s_3) &amp; p_\pi(s_4|s_3) \newline<br>p_\pi(s_1|s_4) &amp; p_\pi(s_2|s_4) &amp; p_\pi(s_3|s_4) &amp; p_\pi(s_4|s_4) \newline<br>\end{bmatrix}<br>}<em>{P_\pi}<br>\underbrace{<br>\begin{bmatrix}<br>v_\pi(s_1) \newline<br>v_\pi(s_2) \newline<br>v_\pi(s_3) \newline<br>v_\pi(s_4)<br>\end{bmatrix}<br>}</em>{v_\pi}<br>$$</p><p><strong>Conclusions</strong>:</p><ul><li>So we have the <font color=blue>closed form</font> of the solution as $v_\pi &#x3D; (I-\gamma P_\pi)^{-1}r_\pi$. However the inverse operation is hard to implement.</li><li>We seek for other iterative solving methods.</li></ul><p><strong>Iterative solution</strong></p><p>​We can directly sovle the Bellman equation using the following iterative algorithm:</p><p>$$<br>\textcolor{blue}{v_{k+1} &#x3D; r_\pi + \gamma P_\pi v_k}<br>$$</p><p>The proof is omitted.</p><h3 id="2-5-Action-value"><a href="#2-5-Action-value" class="headerlink" title="2.5 Action value"></a>2.5 Action value</h3><p>​<font color=red>Action value</font> is denoted by $q_\pi(s,a)$ which is defined as</p><p>$$<br>q_\pi(s,a) \triangleq \mathbb{E}[G_t|S_t&#x3D;s, A_t&#x3D;a]<br>$$</p><p>for all $s \in \mathcal{S}, a\in \mathcal{A}(s)$. </p><p>Action value can be interpreted as <font color=blue>average return</font> along trajectory generated by <font color=blue>policy $\pi$ </font> after taking a <font color=blue>specific action $a$</font>.</p><p>​From the conditional expectation property that</p><p>$$<br>\underbrace{\mathbb{E}[G_t|S_t&#x3D;s]}<em>{v_\pi(s)} &#x3D; \sum_a \pi(a|s) \underbrace{\mathbb{E}[G_t|S_t&#x3D;s,A_t&#x3D;a]}</em>{q_\pi(s)}<br>$$</p><p>Hence, </p><p>$$<br>v_\pi(s) &#x3D; \sum_a \pi(a|s) q_\pi(s)<br>$$</p><p>So we can obtain the mathematical definition of action value as</p><p>$$<br>\textcolor{blue}{q_\pi(s)&#x3D;\sum_r p(r|s,a)r+ \gamma\sum_{s^\prime} p(s^\prime|s,a) v_\pi(s^\prime)}<br>$$</p><p>Substituting $(1)$ into $(2)$ we have</p><p>$$<br>q_\pi(s,a)&#x3D;\sum_r p(r|s,a)r+ \gamma\sum_{s^\prime} p(s^\prime|s,a) \sum_{a^\prime \in \mathcal{A}(s^\prime)}\pi(a^\prime|s^\prime) q_\pi(s^\prime,a^\prime)<br>$$</p><p>​Suppose each state has the same number of actions. The matrix-vecotr form of Bellman eqaution in terms of action value is</p><p>$$<br>\textcolor{red}{q_\pi &#x3D; \tilde{r} + \gamma P \Pi q_\pi}\quad \text{(matrix-vector form)}<br>$$</p><p>where $q_\pi \in \mathbb{R}^{|\mathcal{S}||\mathcal{A}|}$ is the action value vector indexed by state-action pairs. In particular, the $(s,a)$th element is </p><p>$$<br>[q_\pi]_{(s,a)} &#x3D; q_\pi(s,a)<br>$$</p><p>Here, $\tilde{r}\in\mathbb{R}^{|\mathcal{S} ||\mathcal{A} |}$ is the immediate reward vector indexed by state-action pairs. In paricular, the $(s,a)$th reward is</p><p>$$<br>[\tilde{r}]_{(s,a)} &#x3D; \sum_r p(r|s,a)r<br>$$</p><p>Here, $P\in \mathbb{R}^{|\mathcal{S}||\mathcal{A}|\times|\mathcal{S}| }$ is the probability transition matrix, whose row is indexed by state-action pairs and column indexed by states. In particular</p><p>$$<br>[P]_{(s,a),s^\prime} &#x3D; p(s^\prime|s,a)<br>$$</p><p>And $\Pi \in\mathbb{R}^{|\mathcal{S}|\times|\mathcal{S}||\mathcal{A}|}$ describes the policy $\pi$. In particular</p><p>$$<br>\Pi_{s^\prime, (s^\prime,a^\prime)} &#x3D; \pi(a^\prime|s^\prime)<br>$$</p><p>and the other entries of $\Pi$ is zero. $\Pi$ is block diagonal matrix with each block as a $1\times|\mathcal{A}|$ vector.</p><hr><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>  <a href="https://www.bilibili.com/video/BV1sd4y167NS/?spm_id_from=333.999.0.0">赵世钰老师的课程</a></p>]]></content>
      
      
      <categories>
          
          <category> Reinforcement Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reinforcement Learning with Code [Chapter 1. Basic Concepts]</title>
      <link href="/2023/07/02/Reinforcement-Learning-with-Code-Chapter-1-Basic-Concepts/"/>
      <url>/2023/07/02/Reinforcement-Learning-with-Code-Chapter-1-Basic-Concepts/</url>
      
        <content type="html"><![CDATA[<h1 id="Reinforcement-Learning-with-Code"><a href="#Reinforcement-Learning-with-Code" class="headerlink" title="Reinforcement Learning with Code."></a>Reinforcement Learning with Code.</h1><p>This note record how the author begin to learn RL. Both theoretical understanding and code practice are presented. Many material are referenced such as ZhaoShiyu’s <em>Mathematical Foundation of Reinforcement Learning</em> .</p><h2 id="Chapter-1-Basic-Concepts"><a href="#Chapter-1-Basic-Concepts" class="headerlink" title="Chapter 1. Basic Concepts"></a>Chapter 1. Basic Concepts</h2><h3 id="1-1-State-and-action"><a href="#1-1-State-and-action" class="headerlink" title="1.1 State and action"></a>1.1 State and action</h3><ul><li><font color=red><em>State</em> </font>describe the status of the agent with respect to the environment, denoted by $s$.</li><li><font color=red><em>State space</em> </font> is the set of all states, denoted by $\mathcal{S}&#x3D;{s_1, s_2,\dots,s_n}$.</li><li><font color = red><em>Action</em></font> describe the action that the agent may take with respect to the environment, denoted by $a$.</li><li><font color = red><em>Action space</em></font> is the set of all actions, denoted by $\mathcal{A}&#x3D;{a_1, a_2,\dots,a_n}.$</li></ul><h3 id="1-2-State-transition"><a href="#1-2-State-transition" class="headerlink" title="1.2 State transition"></a>1.2 State transition</h3><p>When taking an action, the agent may move from one state to another. Such a process is called <font color=red><em>state transition</em></font>. State trasition can be denoted by<br>$$<br>s_1 \stackrel{a_2}\longrightarrow s_2<br>$$<br>described by $p(s^\prime|s,a)$.</p><p>State trainsition can be both <em>deterministic</em> and <em>stochastic</em>. For example the deterministic state transition is<br>$$<br>p(s_1|s_1,a_2) &#x3D; 0 \<br>p(s_2|s_1,a_2) &#x3D; 1 \<br>p(s_3|s_1,a_2) &#x3D; 0<br>$$<br>For example the stochastic state transition is<br>$$<br>p(s_1|s_1,a_2) &#x3D; 0.5 \<br>p(s_2|s_1,a_2) &#x3D; 0.3 \<br>p(s_3|s_1,a_2) &#x3D; 0.2<br>$$</p><h3 id="1-3-Policy"><a href="#1-3-Policy" class="headerlink" title="1.3 Policy"></a>1.3 Policy</h3><ul><li><font color = red><em>Policy</em></font> tells the agents which actions to take <font color=blue>at each state</font>, denoted by $\pi$.</li><li>Policy is described by conditional probability.</li><li>Policy can be <em>deterministic</em> or <em>stochastic</em>, which means one state has a deterministic action or one state has probability to select other actions.</li></ul><p>Suppose the actions space is $\mathcal{A}&#x3D;{a_1, a_2,a_3}$, such  deterministic policy can be dentoed by<br>$$<br>\pi(a_1|s_1) &#x3D; 0 \<br>\pi(a_2|s_1) &#x3D; 1 \<br>\pi(a_3|s_1) &#x3D; 0<br>$$<br>which indicated the probability of taking action $a_2$ is $1$ and others are zero. </p><p>Such stochastic policy can be denoted by<br>$$<br>\pi(a_1|s_1) &#x3D; 0.5 \<br>\pi(a_2|s_1) &#x3D; 0.3 \<br>\pi(a_3|s_1) &#x3D; 0.2<br>$$</p><h3 id="1-4-Reward"><a href="#1-4-Reward" class="headerlink" title="1.4 Reward"></a>1.4 Reward</h3><ul><li><font color = red><em>Reward</em></font> is one of the most unique concept in RL.</li><li><font color = red><em>Immediate reward</em></font> can be obtained after taking an action.</li><li><font color = red><em>Reward transition</em></font> is the process of getting a reward after taking an action, reward transition can be <em>deterministic</em> or <em>stochastic</em>. Reward transition is described by $p(r|s,a)$</li></ul><p>For example deterministic reward transition can be denoted by<br>$$<br>p(r&#x3D;-1|s_1,a_2) &#x3D; 1, p(r\ne -1|s_1,a_2)&#x3D;0<br>$$<br>which means at state $s_1$ take action $a_2$ the probability to get immediate reward $-1$ is $1$.</p><p>Stochastic reward transition can be denoted by<br>$$<br>p(r&#x3D;1|s_1,a_2) &#x3D; 0.5, p(r&#x3D; 0|s_1,a_2)&#x3D;0.5<br>$$<br>which means at state $s_1$ take action $a_2$ the probability to get immediate reward $1$ is $0.5$, the probability to get immediate reward $0$ is $0.5$.</p><h3 id="1-5-Trajectory-return-episode"><a href="#1-5-Trajectory-return-episode" class="headerlink" title="1.5 Trajectory, return, episode"></a>1.5 Trajectory, return, episode</h3><ul><li><p><font color = red><em>Trajectory</em></font> is a state-action-reward chain, such as $s_1 \underset{r&#x3D;0}{\xrightarrow{a_2}} s_2 \underset{r&#x3D;0}{\xrightarrow{a_3}} s_3 \underset{r&#x3D;0}{\xrightarrow{a_4}} \cdots\underset{r&#x3D;1}{\xrightarrow{a_n}} s_{n}$.</p></li><li><p><font color = red><em>Return</em></font> of this trajecotry is the sum of all the rewards collected along the trajectory, such as $\text{return} &#x3D; 0+0+0+\cdots+1&#x3D;1$. Return is also called <font color=blue><em>total rewards</em></font> or <font color=blue><em>cumulative rewards</em></font>.</p></li><li><p><font color = red><em>Discounted return</em></font> is defined by the <font color=blue><em>discounted rate</em></font>, denoted by $\gamma\in(0,1)$. Such discounted retrun is<br>$$<br>\text{discounted return} &#x3D; 0+\gamma0+\gamma^2 0 + \gamma^3 0 + \cdots + \gamma^n 1<br>$$</p></li><li><p><font color = red><em>Episode</em></font> refers the trajectory that interacting with the enviornment following a policy <font color = blue>until the agent reach the terminal state</font>. An episode is usually assumed to be a finite trajectory,  that task with episodes are called <em>episodic tasks</em>. Some task may have no terminal state, such task is called <em>continuing tasks</em>.</p></li></ul><h3 id="1-6-Markov-decision-process-MDP"><a href="#1-6-Markov-decision-process-MDP" class="headerlink" title="1.6 Markov decision process (MDP)"></a>1.6 Markov decision process (MDP)</h3><p>Markov decision process is a general framework to <font color=blue>describe stochastic dynamical systems</font>. The key ingredients of an MDP are listed:</p><ul><li><p>Sets:</p><ul><li>State set: the set of all states, denoted as $\mathcal{S}$.</li><li>Actions set: a set of actions, denoted as $\mathcal{A}(s)$, is associated for each state $s\in\mathcal{S}$.</li><li>Reward set: a set of rewards, denoted as $\mathcal{R}(s,a)$, is associated for each state action pari $(s,a)$.</li></ul></li><li><p>Model:</p><ul><li>State transition probability: at state $s$, taking actions $a$, the probability to transit to state $s^\prime$ is $p(s^\prime|s,a)$.</li><li>Reward transition probability: at state $s$, taking action $a$, the probability to get reward $r$ is $p(r|s,a)$.</li></ul></li><li><p>Policy: as state $s$, the probability to choose action $a$ is $\pi(a|s)$.</p></li><li><p>Markov property: one key property of MDPs is the <em>Markov property</em>, which refers to the <font color=blue>memoryless property</font> of a stochastic process, which means<br>$$<br>p(s_{t+1}|s_t,a_t,s_{t-1},a_{t-1},\dots,s_0,a_0)&#x3D;p(s_{t+1}|s_t,a_t)\<br>p(r_{t+1}|s_t,a_t,s_{t-1},a_{t-1},\dots,s_0,a_0)&#x3D;p(r_{t+1}|s_t,a_t)\<br>$$</p><hr><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://www.bilibili.com/video/BV1sd4y167NS/?spm_id_from=333.999.0.0">赵世钰老师的课程</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Reinforcement Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
