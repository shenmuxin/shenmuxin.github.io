<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/12/24/hello-world/"/>
      <url>/2023/12/24/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Reinforcement Learning with Code [Chapter 2. State Value and Bellman Equation]</title>
      <link href="/2023/07/27/Reinforcement-Learning-with-Code-Chapter-2-State-Value-and-Bellman-Equation/"/>
      <url>/2023/07/27/Reinforcement-Learning-with-Code-Chapter-2-State-Value-and-Bellman-Equation/</url>
      
        <content type="html"><![CDATA[<h1 id="Reinforcement-Learning-with-Code"><a href="#Reinforcement-Learning-with-Code" class="headerlink" title="Reinforcement Learning with Code"></a>Reinforcement Learning with Code</h1><p>This note records how the author begin to learn RL. Both theoretical understanding and code practice are presented. Many material are referenced such as ZhaoShiyu’s <em>Mathematical Foundation of Reinforcement Learning</em>, .</p><h2 id="Chapter-2-State-Value-and-Bellman-Equation"><a href="#Chapter-2-State-Value-and-Bellman-Equation" class="headerlink" title="Chapter 2. State Value and Bellman Equation"></a>Chapter 2. State Value and Bellman Equation</h2><h3 id="2-1-State-value"><a href="#2-1-State-value" class="headerlink" title="2.1 State value"></a>2.1 State value</h3><ul><li><p><font color="red">State value</font> is defined as the mean of <font color="blue">all possible returns</font> starting from a state, which is actually the <font color="blue">expectation of return</font> from a specific state.</p></li><li><p>The mathematical definition is as follows:</p><p>Note that the capital letters denote <em>random variables</em>, such as $S_t, S_{t+1}, A_t, R_{t+1}$. In particular, $S_t,S_{t+1}\in\mathcal{S},A_t\in\mathcal{A}(S_t)$ and $R_{t+1}\in\mathcal{R}(S_t,A_t)$. $G_t$ denote the random variable of return.</p><p>Starting from $t$, we can obtain a state-action-reward trajectory:</p><p>$$<br>S_t \xrightarrow{A_t} S_{t+1},R_{t+1} \xrightarrow{A_{t+1}} S_{t+2},R_{t+2} \xrightarrow{A_{t+2}} S_{t+3},R_{t+3} \cdots<br>$$</p><p>The discounted return along the trajectory is </p><p>$$<br>G_t &#x3D; R_{t+1} + \gamma R_{t+2} + \gamma R_{t+3} + \cdots<br>$$</p><p>The state value is defined as:</p><p>$$<br>\textcolor{blue}{v_\pi(s)\triangleq \mathbb{E}[G_t|S_t&#x3D;s]}<br>$$</p><p>which means <font color="blue">start from state</font> $s$ can get the <font color="blue">expectation return</font> along the trajecotry generated by <font color="blue">policy $\pi$ </font>. </p><p>$v_\pi(s)$ is also called <font color="blue">state-value funtion</font>.</p></li></ul><h3 id="2-2-Bellman-Equation"><a href="#2-2-Bellman-Equation" class="headerlink" title="2.2 Bellman Equation"></a>2.2 Bellman Equation</h3><ul><li><p>Bellman equation is a set of linear equations <font color="blue">describing the relationship among the values of all the states</font>.</p><p>For example,</p></li></ul><img src="https://pic.imgdb.cn/item/658bd598c458853aeff51217.png" class width="400" height="400" title="state-transform"><p>  Let $v_i$ denote the return obtained starting from $s_i$. The return starting from the four states in figure can be respectively calculated as </p><p>$$<br>v_1 &#x3D; r_1 + \gamma r_2 + \gamma^2 r_3 + \cdots \newline<br>v_2 &#x3D; r_2 + \gamma r_3 + \gamma^2 r_4 + \cdots \newline<br>v_3 &#x3D; r_3 + \gamma r_4 + \gamma^2 r_1 + \cdots \newline<br>v_4 &#x3D; r_4 + \gamma r_1 + \gamma^2 r_2 + \cdots<br>$$</p><p> Using the idea of <em>bootstrapping</em>, we can rewrite it to</p><p>$$<br>v_1 &#x3D; r_1 + \gamma (r_2 + \gamma r_3) + \cdots &#x3D; r_1 + \gamma v_2 \newline<br>v_2 &#x3D; r_2 + \gamma (r_3 + \gamma r_4) + \cdots &#x3D; r_2 + \gamma v_3 \newline<br>v_3 &#x3D; r_3 + \gamma (r_4 + \gamma r_1) + \cdots &#x3D; r_3 + \gamma v_4 \newline<br>v_4 &#x3D; r_4 + \gamma (r_1 + \gamma r_2) + \cdots &#x3D; r_4 + \gamma v_1<br>$$</p><p>  Then rewrite it into matrix form</p><p>$$<br>\underbrace{ \begin{bmatrix} v_1 \newline v_2 \newline v_3 \newline v_4 \end{bmatrix}}<em>{\text{v}} &#x3D; \underbrace{ \begin{bmatrix} r_1 \newline r_2 \newline r_3 \newline r_4 \end{bmatrix}}</em>{\text{r}} + \gamma\underbrace{ \begin{bmatrix} 0 &amp; 1 &amp; 0 &amp; 0 \newline 0 &amp; 0 &amp; 1 &amp; 0 \newline 0 &amp; 0 &amp; 0 &amp; 1 \newline 1 &amp; 0 &amp; 0 &amp; 0 \end{bmatrix}}<em>{\text{P}}\underbrace{ \begin{bmatrix} v_1 \newline v_2 \newline v_3 \newline v_4 \end{bmatrix}}</em>{\text{v}}<br>$$</p><p> The equation $\textbf{v}&#x3D;\textbf{r}+\gamma\textbf{P}\textbf{v}$ is called Bellman equation.</p><ul><li>Then we can derive Bellman equation from scratch as follows:</li></ul><p>Note that the discounted return random variable $G_t$ can be rewritten as </p><p>$$<br>\begin{aligned}<br>G_t &amp; &#x3D;  R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots\newline<br>&amp; &#x3D;  R_{t+1} + \gamma(R_{t+2}+\gamma R_{t+3} + \cdots) \newline<br>&amp; &#x3D;  R_{t+1} + \gamma G_{t+1}<br>\end{aligned}<br>$$</p><p> where $G_{t+1} &#x3D; R_{t+2} + \gamma R_{t+3} + \cdots$ . This equation establishes the relationship between $G_t$ and $G_{t+1}$. Then the state value can be rewritten as </p><p>$$<br>\begin{aligned}<br>v_\pi(s) &amp; &#x3D;  \mathbb{E}[G_t|S_t&#x3D;s] \newline<br>&amp; &#x3D;  \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t&#x3D;s] \newline<br>&amp; &#x3D;  \underbrace{\mathbb{E}[R_{t+1}|S_t&#x3D;s]}<em>{\text{mean of immediate reward}} +<br>\underbrace{\gamma \mathbb{E}[G</em>{t+1}|S_t&#x3D;s]}_{\text{mean of future reward}}<br>\quad \text{(linear property of expectation)}<br>\end{aligned}<br>$$</p><p> The <font color="blue"> mean of immediate reward</font> can be written as (use 2.3 conditional expectation)</p><p>$$<br>\begin{aligned}<br>\mathbb{E}[R_{t+1}|S_t&#x3D;s] &amp; &#x3D;  \sum_a \pi(a|s) \mathbb{E}[R_{t+1} | S_t&#x3D;s,A_t&#x3D;a] \quad \text{(conditional expectation)}\newline<br>&amp; &#x3D;  \sum_a \pi(a|s) \sum_r p(r|s,a)r<br>\end{aligned}<br>$$</p><p>  The <font color="blue">mean of future reward</font> can be written as </p><p>$$<br>\begin{aligned}<br>\gamma \mathbb{E}[G_{t+1}|S_t&#x3D;s] &amp; &#x3D;  \gamma \sum_a \pi(a|s) \mathbb{E}[G_{t+1}|S_t&#x3D;s, A_t&#x3D;a]\quad \text{(conditional expectation)}\newline<br>&amp; &#x3D;  \gamma \sum_a \pi(a|s) \sum_{s^\prime} p(s^\prime|s,a) v_\pi(s^\prime)\newline<br>\end{aligned}<br>$$</p><p>Then the state value can be rewritten as <font color="red">Bellman equation (BE)</font> form </p><p>$$<br>\begin{aligned}<br>v_\pi(s) &amp; &#x3D;  \mathbb{E}[G_t|S_t&#x3D;s] \newline<br>&amp; &#x3D;  \mathbb{E}[R_{t+1}|S_t&#x3D;s] + \gamma \mathbb{E}[G_{t+1}|S_t&#x3D;s] \newline<br>&amp; &#x3D;<br>\underbrace{\sum_a \pi(a|s) \sum_r p(r|s,a)r}_{\text{mean of immediate reward}} </p><p>+<br>\underbrace{\gamma \sum_a \pi(a|s) \sum_{s^\prime} p(s^\prime|s,a) v_\pi(s^\prime)}<em>{\text{mean of future reward}}\newline<br>&amp; &#x3D;  \sum_a \pi(a|s) \Big[\sum_r p(r|s,a)r+ \gamma\sum</em>{s^\prime} p(s^\prime|s,a) v_\pi(s^\prime)\Big], \quad \text{for all } s\in\mathcal{S}<br>\end{aligned}<br>$$</p><p>Next, we will introduce the matrix vector form of Bellman equation in terms of state value. Let $v_\pi&#x3D;[v_\pi(s_1),v_\pi(s_2),\cdots]^T\in\mathbb{R}^n$, $r_\pi&#x3D;[r_\pi(s_1),r_\pi(s_2),\cdots]^T\in\mathbb{R}^n$, and $P_\pi\in\mathbb{R}^{n\times n}$. Then we have the matrix-vector form of Bellman equation in terms of state value.</p><p>$$<br>\textcolor{red}{v_\pi &#x3D; r + \gamma P_\pi v_\pi} \quad \text{(matrix-vector form)}<br>$$</p><p>where</p><p>$$<br>[r_\pi]<em>s \triangleq \sum_a \pi(a|s) \sum_r p(r|s,a)r \qquad<br>[P_\pi]</em>{s,s^\prime} &#x3D; \sum_a \pi(a|s) \sum_{s^\prime} p(s^\prime|s,a)<br>$$</p><h3 id="2-3-Review-conditional-expectaion"><a href="#2-3-Review-conditional-expectaion" class="headerlink" title="2.3 Review conditional expectaion"></a>2.3 Review conditional expectaion</h3><ul><li>The definition of conditional expectation is</li></ul><p>$$<br>\mathbb{E} &#x3D; [X|A&#x3D;a] &#x3D; \sum_{x}xp(x|a)<br>$$</p><p>  Similar to the law of total probability, we have the law of total expectation:</p><p>$$<br>\mathbb{E}[X] &#x3D; \sum_a p(a) \mathbb{E}[X|A&#x3D;a]<br>$$</p><p>  <em>Proof</em></p><p>  By definition of expectation the right hand side can be written as </p><p>$$<br>\begin{aligned}<br>\mathbb{E}[X] &amp; &#x3D;  \sum_a p(a) \sum_x x p(x|a)\newline<br>&amp; &#x3D;  \sum_x \sum_a p(x|a) p(a)  \quad \text{(law of total probability)}\newline<br>&amp; &#x3D;  \sum_x p(x)\newline<br>&amp; &#x3D;  \mathbb{E}[X]<br>\end{aligned}<br>$$</p><h3 id="2-4-Solve-Bellman-Equation"><a href="#2-4-Solve-Bellman-Equation" class="headerlink" title="2.4 Solve Bellman Equation"></a>2.4 Solve Bellman Equation</h3><p>​From the analysis above we are informed that Bellman equation is</p><p>$$<br>\textcolor{blue}{v_\pi(s)&#x3D;\sum_a \pi(a|s) \Big[\sum_r p(r|s,a)r+ \gamma\sum_{s^\prime} p(s^\prime|s,a) v_\pi(s^\prime)\Big]}<br>$$</p><p>We can rewrite it into the follow form</p><p>$$<br>v_\pi(s) &#x3D; r_\pi(s) + \gamma \sum_{s^\prime} p_\pi(s^\prime|s) v_\pi(s^\prime)<br>$$</p><p>where</p><p>$$<br>\begin{aligned}<br>r_\pi(s) \triangleq \sum_a \pi(a|s) \sum_r p(r|s,a)r\newline<br>\newline<br>\newline<br>\newline<br>\end{aligned} \qquad<br>\begin{aligned}<br>p_\pi(s^\prime|s) &amp; \triangleq \sum_a \pi(a|s) \sum_{s^\prime} p(s^\prime|s,a)\newline<br>&amp; &#x3D; \sum_{s^\prime} \sum_a \pi(a|s) p(s^\prime|s,a)\newline<br>&amp; &#x3D; p_\pi(s^\prime|s)<br>\end{aligned}<br>$$</p><p>​Suppose the states are indexed as $s_i$. For state $s_i$, the Bellman equation is </p><p>$$<br>v_\pi(s_i) &#x3D; r_\pi(s_i) + \gamma \sum_{s_j} p_\pi(s_j|s_i) v_\pi(s_j)<br>$$</p><p>Let $v_\pi&#x3D;[v_\pi(s_1),\dots,v_\pi(s_n)]^T \in \mathbb{R}^n$, $r_\pi&#x3D;[r_\pi(s_1),\dots,r_\pi(s_n)]^T \in \mathbb{R}^n$, and $P_\pi \in \mathbb{R}^{n\times n}$, where $[P_\pi]_{ij} &#x3D; p_\pi(s_j|s_i)$. Hence we have the matrix form as </p><p>$$<br>v_\pi &#x3D; r_\pi + \gamma P_\pi v_\pi<br>$$</p><p>For example, consider four states $s_i$ and four actions $a_i$ the matrix form can be</p><p>$$<br>\underbrace{<br>\begin{bmatrix}<br>v_\pi(s_1) \newline<br>v_\pi(s_2) \newline<br>v_\pi(s_3) \newline<br>v_\pi(s_4)<br>\end{bmatrix}<br>}<em>{v_\pi} &#x3D;<br>\underbrace{<br>\begin{bmatrix}<br>r_\pi(s_1) \newline<br>r_\pi(s_1) \newline<br>r_\pi(s_1) \newline<br>r_\pi(s_1)<br>\end{bmatrix}<br>}</em>{r_\pi} + \gamma<br>\underbrace{<br>\begin{bmatrix}<br>p_\pi(s_1|s_1) &amp; p_\pi(s_2|s_1) &amp; p_\pi(s_3|s_1) &amp; p_\pi(s_4|s_1) \newline<br>p_\pi(s_1|s_2) &amp; p_\pi(s_2|s_2) &amp; p_\pi(s_3|s_2) &amp; p_\pi(s_4|s_2) \newline<br>p_\pi(s_1|s_3) &amp; p_\pi(s_2|s_3) &amp; p_\pi(s_3|s_3) &amp; p_\pi(s_4|s_3) \newline<br>p_\pi(s_1|s_4) &amp; p_\pi(s_2|s_4) &amp; p_\pi(s_3|s_4) &amp; p_\pi(s_4|s_4) \newline<br>\end{bmatrix}<br>}<em>{P_\pi}<br>\underbrace{<br>\begin{bmatrix}<br>v_\pi(s_1) \newline<br>v_\pi(s_2) \newline<br>v_\pi(s_3) \newline<br>v_\pi(s_4)<br>\end{bmatrix}<br>}</em>{v_\pi}<br>$$</p><p><strong>Conclusions</strong>:</p><ul><li>So we have the <font color="blue">closed form</font> of the solution as $v_\pi &#x3D; (I-\gamma P_\pi)^{-1}r_\pi$. However the inverse operation is hard to implement.</li><li>We seek for other iterative solving methods.</li></ul><p><strong>Iterative solution</strong></p><p>​We can directly sovle the Bellman equation using the following iterative algorithm:</p><p>$$<br>\textcolor{blue}{v_{k+1} &#x3D; r_\pi + \gamma P_\pi v_k}<br>$$</p><p>The proof is omitted.</p><h3 id="2-5-Action-value"><a href="#2-5-Action-value" class="headerlink" title="2.5 Action value"></a>2.5 Action value</h3><p>​<font color="red">Action value</font> is denoted by $q_\pi(s,a)$ which is defined as</p><p>$$<br>q_\pi(s,a) \triangleq \mathbb{E}[G_t|S_t&#x3D;s, A_t&#x3D;a]<br>$$</p><p>for all $s \in \mathcal{S}, a\in \mathcal{A}(s)$. </p><p>Action value can be interpreted as <font color="blue">average return</font> along trajectory generated by <font color="blue">policy $\pi$ </font> after taking a <font color="blue">specific action $a$</font>.</p><p>​From the conditional expectation property that</p><p>$$<br>\underbrace{\mathbb{E}[G_t|S_t&#x3D;s]}<em>{v_\pi(s)} &#x3D; \sum_a \pi(a|s) \underbrace{\mathbb{E}[G_t|S_t&#x3D;s,A_t&#x3D;a]}</em>{q_\pi(s)}<br>$$</p><p>Hence, </p><p>$$<br>v_\pi(s) &#x3D; \sum_a \pi(a|s) q_\pi(s)<br>$$</p><p>So we can obtain the mathematical definition of action value as</p><p>$$<br>\textcolor{blue}{q_\pi(s)&#x3D;\sum_r p(r|s,a)r+ \gamma\sum_{s^\prime} p(s^\prime|s,a) v_\pi(s^\prime)}<br>$$</p><p>Substituting $(1)$ into $(2)$ we have</p><p>$$<br>q_\pi(s,a)&#x3D;\sum_r p(r|s,a)r+ \gamma\sum_{s^\prime} p(s^\prime|s,a) \sum_{a^\prime \in \mathcal{A}(s^\prime)}\pi(a^\prime|s^\prime) q_\pi(s^\prime,a^\prime)<br>$$</p><p>​Suppose each state has the same number of actions. The matrix-vecotr form of Bellman eqaution in terms of action value is</p><p>$$<br>\textcolor{red}{q_\pi &#x3D; \tilde{r} + \gamma P \Pi q_\pi}\quad \text{(matrix-vector form)}<br>$$</p><p>where $q_\pi \in \mathbb{R}^{|\mathcal{S}||\mathcal{A}|}$ is the action value vector indexed by state-action pairs. In particular, the $(s,a)$th element is </p><p>$$<br>[q_\pi]_{(s,a)} &#x3D; q_\pi(s,a)<br>$$</p><p>Here, $\tilde{r}\in\mathbb{R}^{|\mathcal{S} ||\mathcal{A} |}$ is the immediate reward vector indexed by state-action pairs. In paricular, the $(s,a)$th reward is</p><p>$$<br>[\tilde{r}]_{(s,a)} &#x3D; \sum_r p(r|s,a)r<br>$$</p><p>Here, $P\in \mathbb{R}^{|\mathcal{S}||\mathcal{A}|\times|\mathcal{S}| }$ is the probability transition matrix, whose row is indexed by state-action pairs and column indexed by states. In particular</p><p>$$<br>[P]_{(s,a),s^\prime} &#x3D; p(s^\prime|s,a)<br>$$</p><p>And $\Pi \in\mathbb{R}^{|\mathcal{S}|\times|\mathcal{S}||\mathcal{A}|}$ describes the policy $\pi$. In particular</p><p>$$<br>\Pi_{s^\prime, (s^\prime,a^\prime)} &#x3D; \pi(a^\prime|s^\prime)<br>$$</p><p>and the other entries of $\Pi$ is zero. $\Pi$ is block diagonal matrix with each block as a $1\times|\mathcal{A}|$ vector.</p><hr><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>  <a href="https://www.bilibili.com/video/BV1sd4y167NS/?spm_id_from=333.999.0.0">赵世钰老师的课程</a></p>]]></content>
      
      
      <categories>
          
          <category> Reinforcement Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reinforcement Learning with Code [Chapter 1. Basic Concepts]</title>
      <link href="/2023/07/02/Reinforcement-Learning-with-Code-Chapter-1-Basic-Concepts/"/>
      <url>/2023/07/02/Reinforcement-Learning-with-Code-Chapter-1-Basic-Concepts/</url>
      
        <content type="html"><![CDATA[<h1 id="Reinforcement-Learning-with-Code"><a href="#Reinforcement-Learning-with-Code" class="headerlink" title="Reinforcement Learning with Code"></a>Reinforcement Learning with Code</h1><p>This note record how the author begin to learn RL. Both theoretical understanding and code practice are presented. Many material are referenced such as ZhaoShiyu’s <em>Mathematical Foundation of Reinforcement Learning</em> .</p><h2 id="Chapter-1-Basic-Concepts"><a href="#Chapter-1-Basic-Concepts" class="headerlink" title="Chapter 1. Basic Concepts"></a>Chapter 1. Basic Concepts</h2><h3 id="1-1-State-and-action"><a href="#1-1-State-and-action" class="headerlink" title="1.1 State and action"></a>1.1 State and action</h3><ul><li><font color="red"><em>State</em> </font>describe the status of the agent with respect to the environment, denoted by $s$.</li><li><font color="red"><em>State space</em> </font> is the set of all states, denoted by $\mathcal{S}&#x3D;{s_1, s_2,\dots,s_n}$.</li><li><font color="red"><em>Action</em></font> describe the action that the agent may take with respect to the environment, denoted by $a$.</li><li><font color="red"><em>Action space</em></font> is the set of all actions, denoted by $\mathcal{A}&#x3D;{a_1, a_2,\dots,a_n}.$</li></ul><h3 id="1-2-State-transition"><a href="#1-2-State-transition" class="headerlink" title="1.2 State transition"></a>1.2 State transition</h3><p>When taking an action, the agent may move from one state to another. Such a process is called <font color="red"><em>state transition</em></font>. State trasition can be denoted by<br>$$<br>s_1 \stackrel{a_2}\longrightarrow s_2<br>$$<br>described by $p(s^\prime|s,a)$.</p><p>State trainsition can be both <em>deterministic</em> and <em>stochastic</em>. For example the deterministic state transition is<br>$$<br>p(s_1|s_1,a_2) &#x3D; 0 \<br>p(s_2|s_1,a_2) &#x3D; 1 \<br>p(s_3|s_1,a_2) &#x3D; 0<br>$$<br>For example the stochastic state transition is<br>$$<br>p(s_1|s_1,a_2) &#x3D; 0.5 \<br>p(s_2|s_1,a_2) &#x3D; 0.3 \<br>p(s_3|s_1,a_2) &#x3D; 0.2<br>$$</p><h3 id="1-3-Policy"><a href="#1-3-Policy" class="headerlink" title="1.3 Policy"></a>1.3 Policy</h3><ul><li><font color="red"><em>Policy</em></font> tells the agents which actions to take <font color="blue">at each state</font>, denoted by $\pi$.</li><li>Policy is described by conditional probability.</li><li>Policy can be <em>deterministic</em> or <em>stochastic</em>, which means one state has a deterministic action or one state has probability to select other actions.</li></ul><p>Suppose the actions space is $\mathcal{A}&#x3D;{a_1, a_2,a_3}$, such  deterministic policy can be dentoed by<br>$$<br>\pi(a_1|s_1) &#x3D; 0 \<br>\pi(a_2|s_1) &#x3D; 1 \<br>\pi(a_3|s_1) &#x3D; 0<br>$$<br>which indicated the probability of taking action $a_2$ is $1$ and others are zero. </p><p>Such stochastic policy can be denoted by<br>$$<br>\pi(a_1|s_1) &#x3D; 0.5 \<br>\pi(a_2|s_1) &#x3D; 0.3 \<br>\pi(a_3|s_1) &#x3D; 0.2<br>$$</p><h3 id="1-4-Reward"><a href="#1-4-Reward" class="headerlink" title="1.4 Reward"></a>1.4 Reward</h3><ul><li><font color="red"><em>Reward</em></font> is one of the most unique concept in RL.</li><li><font color="red"><em>Immediate reward</em></font> can be obtained after taking an action.</li><li><font color="red"><em>Reward transition</em></font> is the process of getting a reward after taking an action, reward transition can be <em>deterministic</em> or <em>stochastic</em>. Reward transition is described by $p(r|s,a)$</li></ul><p>For example deterministic reward transition can be denoted by<br>$$<br>p(r&#x3D;-1|s_1,a_2) &#x3D; 1, p(r\ne -1|s_1,a_2)&#x3D;0<br>$$<br>which means at state $s_1$ take action $a_2$ the probability to get immediate reward $-1$ is $1$.</p><p>Stochastic reward transition can be denoted by<br>$$<br>p(r&#x3D;1|s_1,a_2) &#x3D; 0.5, p(r&#x3D; 0|s_1,a_2)&#x3D;0.5<br>$$<br>which means at state $s_1$ take action $a_2$ the probability to get immediate reward $1$ is $0.5$, the probability to get immediate reward $0$ is $0.5$.</p><h3 id="1-5-Trajectory-return-episode"><a href="#1-5-Trajectory-return-episode" class="headerlink" title="1.5 Trajectory, return, episode"></a>1.5 Trajectory, return, episode</h3><ul><li><p><font color="red"><em>Trajectory</em></font> is a state-action-reward chain, such as $s_1 \underset{r&#x3D;0}{\xrightarrow{a_2}} s_2 \underset{r&#x3D;0}{\xrightarrow{a_3}} s_3 \underset{r&#x3D;0}{\xrightarrow{a_4}} \cdots\underset{r&#x3D;1}{\xrightarrow{a_n}} s_{n}$.</p></li><li><p><font color="red"><em>Return</em></font> of this trajecotry is the sum of all the rewards collected along the trajectory, such as $\text{return} &#x3D; 0+0+0+\cdots+1&#x3D;1$. Return is also called <font color="blue"><em>total rewards</em></font> or <font color="blue"><em>cumulative rewards</em></font>.</p></li><li><p><font color="red"><em>Discounted return</em></font> is defined by the <font color="blue"><em>discounted rate</em></font>, denoted by $\gamma\in(0,1)$. Such discounted retrun is<br>$$<br>\text{discounted return} &#x3D; 0+\gamma0+\gamma^2 0 + \gamma^3 0 + \cdots + \gamma^n 1<br>$$</p></li><li><p><font color="red"><em>Episode</em></font> refers the trajectory that interacting with the enviornment following a policy <font color="blue">until the agent reach the terminal state</font>. An episode is usually assumed to be a finite trajectory,  that task with episodes are called <em>episodic tasks</em>. Some task may have no terminal state, such task is called <em>continuing tasks</em>.</p></li></ul><h3 id="1-6-Markov-decision-process-MDP"><a href="#1-6-Markov-decision-process-MDP" class="headerlink" title="1.6 Markov decision process (MDP)"></a>1.6 Markov decision process (MDP)</h3><p>Markov decision process is a general framework to <font color="blue">describe stochastic dynamical systems</font>. The key ingredients of an MDP are listed:</p><ul><li><p>Sets:</p><ul><li>State set: the set of all states, denoted as $\mathcal{S}$.</li><li>Actions set: a set of actions, denoted as $\mathcal{A}(s)$, is associated for each state $s\in\mathcal{S}$.</li><li>Reward set: a set of rewards, denoted as $\mathcal{R}(s,a)$, is associated for each state action pari $(s,a)$.</li></ul></li><li><p>Model:</p><ul><li>State transition probability: at state $s$, taking actions $a$, the probability to transit to state $s^\prime$ is $p(s^\prime|s,a)$.</li><li>Reward transition probability: at state $s$, taking action $a$, the probability to get reward $r$ is $p(r|s,a)$.</li></ul></li><li><p>Policy: as state $s$, the probability to choose action $a$ is $\pi(a|s)$.</p></li><li><p>Markov property: one key property of MDPs is the <em>Markov property</em>, which refers to the <font color="blue">memoryless property</font> of a stochastic process, which means<br>$$<br>p(s_{t+1}|s_t,a_t,s_{t-1},a_{t-1},\dots,s_0,a_0)&#x3D;p(s_{t+1}|s_t,a_t)\<br>p(r_{t+1}|s_t,a_t,s_{t-1},a_{t-1},\dots,s_0,a_0)&#x3D;p(r_{t+1}|s_t,a_t)\<br>$$</p><hr><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://www.bilibili.com/video/BV1sd4y167NS/?spm_id_from=333.999.0.0">赵世钰老师的课程</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Reinforcement Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
