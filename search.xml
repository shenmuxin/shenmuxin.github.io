<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/12/24/hello-world/"/>
      <url>/2023/12/24/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Reinforcement Learning with Code [Chapter 5. Monte Carlo Learning]</title>
      <link href="/2023/07/24/Reinforcement-Learning-with-Code-Chapter-5-Monte-Carlo-Learning/"/>
      <url>/2023/07/24/Reinforcement-Learning-with-Code-Chapter-5-Monte-Carlo-Learning/</url>
      
        <content type="html"><![CDATA[<h1 id="Reinforcement-Learning-with-Code"><a href="#Reinforcement-Learning-with-Code" class="headerlink" title="Reinforcement Learning with Code"></a>Reinforcement Learning with Code</h1><p>This note records how the author begin to learn RL. Both theoretical understanding and code practice are presented. Many material are referenced such as ZhaoShiyu’s <em>Mathematical Foundation of Reinforcement Learning</em>, .</p><h2 id="Chapter-5-Monte-Carlo-Learning"><a href="#Chapter-5-Monte-Carlo-Learning" class="headerlink" title="Chapter 5. Monte Carlo Learning"></a>Chapter 5. Monte Carlo Learning</h2><p>​What is Monte Carlo estimation? Monte Carlo estimation refers to a broad class of techniques that use stochastic samples to solve approximation problems using the <font color=blue>Law of Large Numbers</font>. </p><p>​ChatGpt tells us that “Monte Carlo estimation is a statistical method used to estimate unknown quantities or solve problems by generating random samples and using the law of large numbers to approximate the true value.”</p><h3 id="5-1-Law-of-large-numbers"><a href="#5-1-Law-of-large-numbers" class="headerlink" title="5.1 Law of large numbers"></a>5.1 Law of large numbers</h3><p>​(Law of Large Numbers) <em>For a random variable</em> $X$. <em>Suppose</em> ${x_i}<em>{i&#x3D;1}^n$ <em>are some independent and indentically distribution (iid) samples</em>. <em>Let</em> $\bar{x}&#x3D;\frac{1}{n}\sum</em>{i&#x3D;1}^n x_i$ <em>be the average of the samples. Then</em>,<br>$$<br>\mathbb{E}[\bar{x}] &#x3D; \mathbb{E}[X]\<br>\mathrm{var}[\bar{x}] &#x3D; \frac{1}{n} \mathrm{var}[X]<br>$$<br>The above two equations indicate that $\bar{x}$ is an <em>unbiased estimate</em> of $\mathbb{E}[X]$ and its variance decreases to zero as $n$ increases to infinity.</p><p>​<strong>Proof</strong>, First, $\mathbb{E}[\bar{x}] &#x3D; \mathbb{E}[\frac{1}{n}\sum_{i&#x3D;1}^n x_i]&#x3D;\frac{1}{n}\sum_{i&#x3D;1}^n\mathbb{E}[x_i]&#x3D;\frac{1}{n}n\mathbb{E}[X]&#x3D;\mathbb{E}[X]$, where the last equability is because the samples are <em>independent and indentically distribution (iid)</em> (that is, $\mathbb{E}[x_i]&#x3D;\mathbb{E}[X]$).</p><p>​Second, $\mathrm{var}[\bar{x}]&#x3D;\mathrm{var}[\frac{1}{n}\sum_{i&#x3D;1}^n x_i]&#x3D;\frac{1}{n^2}\sum_{i&#x3D;1}^n\mathrm{var}[x_i]&#x3D;\frac{1}{n^2}n*\mathrm{var}[X]&#x3D;\frac{1}{n}\mathrm{var}[X]$, where the second equality is because the samples are <em>independent and indentically distribution (iid)</em> (that is, $\mathrm{var}[x_i]&#x3D;\mathrm{var}[X]$)</p><h3 id="5-2-Simple-example"><a href="#5-2-Simple-example" class="headerlink" title="5.2 Simple example"></a>5.2 Simple example</h3><p>​Consider a problem that we need to calculate the expectation $\mathbb{E}[X]$ of random variable $X$ which takes value in the finite set $\mathcal{X}$. There are two ways. </p><p>​First, <em>model-based</em> approach, we can use the definition of expectation that<br>$$<br>\mathbb{E}[X] &#x3D; \sum_{x\in\mathcal{X}} p(x)x<br>$$<br>​Second, <em>model-free</em> approach, if the probability of distribution is unknown we can use the <em>Law of Large Numbers</em> to estimate the expectation.<br>$$<br>\mathbb{E}[X]\approx \bar{x} &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^n x_i<br>$$<br>The example gives us an intutition that: When the system model is available, the expectation can be calculated based on the model.</p><p>When the model is unavailable, the expectation can be estimated approximately using stochastic samples.</p><h3 id="5-3-Monte-Carlo-Basic"><a href="#5-3-Monte-Carlo-Basic" class="headerlink" title="5.3 Monte Carlo Basic"></a>5.3 Monte Carlo Basic</h3><p><strong>Review policy iteration</strong>:</p><p>​The simplest Monte Carlo learning also called Monte Carlo Basic (MC Basic) just replaces the policy iteration model-based part by MC estimation. Recall the <font color=blue>matrix-vector form of policy iteration</font></p><p>$$<br>\begin{aligned}<br>    v_{\pi_k}^{(j+1)} &amp; &#x3D; r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}^{(j)}\quad \text{(policy evaluation)} \<br>    \pi_{k+1} &amp; &#x3D; \arg \max_{\pi_k}(r_{\pi_k} + \gamma P_{\pi_k}v_{\pi_k})\quad \text{(policy improvement)}<br>\end{aligned}<br>$$</p><p>The <font color=blue>elementwise form of policy iteration</font> is</p><p>$$<br>\begin{aligned}<br>    v_{\pi_k}^{(j+1)}(s) &amp; &#x3D; \sum_a \pi_k(a|s) \Big(\sum_r p(r|s,a)r + \sum_{s^\prime} p(s^\prime|s,a) v_{\pi_k}^{(j)} (s^\prime)\Big)\quad \text{(policy evaluation)}\<br>    \pi_{k+1}(s) &amp; &#x3D; \arg \max_{\pi_k}  \sum_a \pi(a|s)<br>    \underbrace{<br>    \Big(\sum_r p(r|s,a)r + \sum_{s^\prime} p(s^\prime|s,a) v_{\pi_k} (s^\prime)\Big)}<em>{q</em>{\pi_k}(s,a)}<br>    \quad \text{(policy improvement)}\<br>    \to \pi_{k+1}(s) &amp; &#x3D;  \sum_a \pi(a|s) \arg \max_a q_{\pi_k}(s,a)<br>\end{aligned}<br>$$</p><p>Its obvious that the core is to calculate the action values by usting<br>$$<br>q_{\pi_k}(s,a) &#x3D; \sum_r p(r|s,a)r + \sum_{s^\prime}p(s^\prime|s,a)v_{\pi_k}(s^\prime)<br>$$<br>​However, the above equation needs the model of the environment (that  $p(r|s,a)$ and $p(s^\prime|s,a)$ are required). We can replace this part by Monte Carlo estimation. Recall the definition of action value refer to section 2.5.<br>$$<br>q_\pi(s,a) \triangleq \mathbb{E}[G_t|S_t&#x3D;s, A_t&#x3D;a]<br>$$<br>Action value can be interpreted as <font color=blue>average return</font> along trajectory generated by <font color=blue>policy $\pi$ </font> after taking a <font color=blue>specific action $a$</font>.</p><p><strong>Convert to model free</strong>:</p><p>​Suppose there are $n$ episodes sampling starting at state $s$ and takeing action $a$,  and then denote the $i$ th episode return as $\textcolor{blue}{g^{(i)}(s,a)}$. Then using the idea of Monte Carlo estimation and Law of Large Numbers, the action values can be approxiamte as<br>$$<br>q_\pi(s,a) \triangleq \mathbb{E}[G_t|S_t&#x3D;s, A_t&#x3D;a] \textcolor{blue}{\approx \frac{1}{n} \sum_{j&#x3D;1}^n g^{(j)}(s,a)}<br>$$<br>Suppose the number of episodes $n$ is sufficiently large. So, the above equation is the unbiased estimation of action value $q_\pi(s,a)$.</p><p><strong>Pesudocode</strong>:</p><img src="https://pic.imgdb.cn/item/658bfa94c458853aef6a9405.png" class="" width="800" height="400" title="state-transform"><p>​Here are some questions.  <font color=red>Why does the MC Basic algorithm estimate action values instead of state values?</font> That is because state value cannot be used to improve policies directly. Even if we are given state values, we still need to calculate action values from these state value using $q_{\pi_k}(s,a) &#x3D; \sum_r p(r|s,a)r + \sum_{s^\prime}p(s^\prime|s,a)v_{\pi_k}(s^\prime)$. But this calculation requires the system model. Therefore, when models are not available, we should directly estimate action values.</p><h3 id="5-4-Monte-Carlo-Exploring-Starts"><a href="#5-4-Monte-Carlo-Exploring-Starts" class="headerlink" title="5.4 Monte Carlo Exploring Starts"></a>5.4 Monte Carlo Exploring Starts</h3><p><strong>Some concepts</strong>:</p><ul><li><p><font color=blue>Vist</font>: a state-action pair appears in the episode, it is called a <em>vist</em> of the state-action pair. Such as<br>$$<br>\textcolor{blue}{s_1 \xrightarrow{a_2}} s_2 \xrightarrow{a_4} \textcolor{blue}{s_1 \xrightarrow{a_2}} s_2 \xrightarrow{a_3} s_5 \xrightarrow{a_13}\cdots<br>$$<br>$(s_1,a_2)$ is called a visit.</p></li><li><p><font color=blue>First visit</font>: In the above trajectory $(s_1,a_2)$ is visited twice. If we only count the first-time visit, such kind of strategy is called <em>first-visit</em>.</p></li><li><p><font color=blue>Every visit</font>: If every time state-action pair is visited and the rest of the episode is used to estimate its action value, such a strategy is called <em>every-visit</em>.</p></li></ul><p>​For example,</p><p>$$<br>\begin{aligned}<br>    s_1 \xrightarrow{a_2} s_2 \xrightarrow{a_4} s_1 \xrightarrow{a_2} s_2 \xrightarrow{a_3} s_5 \xrightarrow{a_13}\cdots &amp; \quad [\textcolor{blue}{\text{origianl episode}}]\<br>    s_2 \xrightarrow{a_4} s_1 \xrightarrow{a_2} s_2 \xrightarrow{a_3} s_5 \xrightarrow{a_13}\cdots &amp; \quad [\text{episode starting from }(s_2,a_4)]\<br>    s_1 \xrightarrow{a_2} s_2 \xrightarrow{a_3} s_5 \xrightarrow{a_13}\cdots &amp; \quad [\textcolor{red}{\text{episode starting from }(s_1,a_2)}]\<br>    s_2 \xrightarrow{a_3} s_5 \xrightarrow{a_13}\cdots &amp; \quad [\text{episode starting from }(s_2,a_3)]\<br>    s_5 \xrightarrow{a_13}\cdots &amp; \quad [\text{episode starting from }(s_2,a_4)]<br>\end{aligned}<br>$$</p><p>Suppose we need to estimate the action value $q_\pi(s_1,a_2)$. The <em>first-visit</em> only counts the first visit of $(s_1,a_2)$. So we need a huge numbers of episodes starting from $(s_1,a_2)$. As the blue one episode in the above equations. The <em>every-visi</em>t counts every time the visit of $(s_1,a_2)$. Hence, we can use the blue one and the red one to estimate the action value $q_\pi(s_1,a_2)$. In this way, the samples in the episode are utilized more sufficiently.</p><p><strong>Using sample more efficiently</strong>:</p><p>​From the example, we are informed that <font color=blue>if an episode is sufficiently long so that it can visit all the state-action pairs many times, then this single episode is sufficient to estimate all the action values by using the every-visit strategy</font>. However, one single episode is pretty ideal result. Because the samples obtained by the <em>every-visit</em> is relevant due to the trajectory starting from the second visit is merely a subset of the trajectory starting from the first. Nevertheless, if the two visits are far away from each other, which means there is a siginificant non-overlap portion, the relevance would not be strong. Moreover, the relevance can be further suppressed due to the discount rate. <font color=blue>Therefore, when there are few episodes and each episode is very long, the every-visit strategy is a good option</font>.</p><p><strong>Using estimation more efficiently</strong>:</p><p>​The first startegy in MC Basic, in the policy evaluation step, to collect all the episodes starting from the same state-action pair and then approximate the action value using the average return of these episodes. The drawback of the strategy is that the agent must wait until all episodes have been collected.</p><p>​The second strategy, which can overcome the drawback, is to <font color=blue>use the return of a single episode to approximate the corresponding action value</font> (the idea of stochastic estimation). In this way, we can improve the policy in an <font color=blue>episode-by-episode</font> fasion.</p><p><strong>Pesudocode</strong>:</p><img src="https://pic.imgdb.cn/item/658bfac5c458853aef6b25fb.png" class="" width="800" height="500" title="state-transform"><p><font color=blue>MC Exploring Starts algorithm compared to MC Basic, the sample usage and estimation update are more efficient</font>.</p><h3 id="5-5-Monte-Carlo-epsilon-Greedy"><a href="#5-5-Monte-Carlo-epsilon-Greedy" class="headerlink" title="5.5 Monte Carlo $\epsilon$-Greedy"></a>5.5 Monte Carlo $\epsilon$-Greedy</h3><p>​Why is exploring starts important? In theory, exploring starts is necessary to find optimal policies. Only if every state-action pair is well explored, can we select optimal policies. However, in practice, exploring starts is difficult to achieve. Because its difficult to collect episodes starting from every state-action pair.</p><p>​We can use <em>soft policy</em> to remove the requirement of exploring starts. There are many soft policies. The most common one is $\textcolor{blue}{\epsilon}$<font color=blue>-greedy</font>. The $\epsilon$-greedy has the form of<br>$$<br>\pi(a|s) &#x3D;<br>\left {<br>    \begin{aligned}<br>    1 - \frac{\epsilon}{|\mathcal{A}(s)|}(|\mathcal{A(s)}|-1), &amp; \quad \text{for the geedy action}\<br>    \frac{\epsilon}{|\mathcal{A}(s)|}, &amp; \quad \text{for the other } |\mathcal{A}(s)|-1 \text{ actions}<br>    \end{aligned}<br>\right.<br>$$<br>where $|\mathcal{A}(s) |$ denotes the number of actions associated with $s$. It is worth noting taht the probability to take the greedy action is always greater than other action, because<br>$$<br>1 - \frac{\epsilon}{|\mathcal{A}(s)|}(|\mathcal{A(s)}|-1) &#x3D;<br>1 - \epsilon + \frac{\epsilon}{|\mathcal{A}(s)|} \ge<br>\frac{\epsilon}{|\mathcal{A}(s)|}<br>$$<br>for any $\epsilon\in[0,1]$, when $\epsilon&#x3D;0$ the $\epsilon$-greedy becomes greedy policy.</p><p><strong>Pesudocode</strong>:</p><img src="https://pic.imgdb.cn/item/658bfae1c458853aef6b7740.png" class="" width="800" height="500" title="state-transform"><hr><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>  <a href="https://www.bilibili.com/video/BV1sd4y167NS/?spm_id_from=333.999.0.0">赵世钰老师的课程</a></p>]]></content>
      
      
      <categories>
          
          <category> Reinforcement Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reinforcement Learning with Code [Chapter 4. Value Iteration and Policy Iteration]</title>
      <link href="/2023/07/23/Reinforcement-Learning-with-Code-Chapter-4-Value-Iteration-and-Policy-Iteration/"/>
      <url>/2023/07/23/Reinforcement-Learning-with-Code-Chapter-4-Value-Iteration-and-Policy-Iteration/</url>
      
        <content type="html"><![CDATA[<h1 id="Reinforcement-Learning-with-Code"><a href="#Reinforcement-Learning-with-Code" class="headerlink" title="Reinforcement Learning with Code"></a>Reinforcement Learning with Code</h1><p>This note records how the author begin to learn RL. Both theoretical understanding and code practice are presented. Many material are referenced such as ZhaoShiyu’s <em>Mathematical Foundation of Reinforcement Learning</em>, .</p><h2 id="Chapter-4-Value-Iteration-and-Policy-Iteration"><a href="#Chapter-4-Value-Iteration-and-Policy-Iteration" class="headerlink" title="Chapter 4. Value Iteration and Policy Iteration"></a>Chapter 4. Value Iteration and Policy Iteration</h2><p>​Value iteration and policy iteration have a common name called <font color=blue>dynamic programming</font>. Dynamic programming is model-based algorithm, which is the simplest RL algorithm. Its helpful to us to understand the model-free algorithm.</p><h3 id="4-1-Value-iteration"><a href="#4-1-Value-iteration" class="headerlink" title="4.1 Value iteration"></a>4.1 Value iteration</h3><p>​Value iteration is solving the Bellman optimal equation directly.</p><p><strong>Matrix-vector form</strong>:</p><p>​The <font color=red>value iteration</font> is exactly the algorithm suggested by the contraction mapping in chapter 3. Value iteration concludes two parts. First, in every iteration is called <em>policy update</em>. </p><p>$$<br>\pi_{k+1} &#x3D; \arg \textcolor{red}{\max_{\pi_k}(r_{\pi_k} + \gamma P_\pi v_k)}<br>$$<br>Second, the step is <em>value update</em>. Mathematically, it is to substitute $\pi_{k+1}$ and do the following operation:</p><p>$$<br>v_{k+1} &#x3D; r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}}v_k<br>$$</p><p>The above algorithm is matrix-vector form which is useful to understand the core idea. <font color=red>The above equation is iterative which means we calculate</font> $v_{k+1}$ <font color=red>is only one step calculation</font>.</p><p>There is a <font color=red>misunderstanding</font> that we doesn’t use Bellman equation to calculate state value $v_{k+1}$ directly. Instead, when we use greedy policy update strategy the state value $v_{k+1}$ is actually the maximum action value $\max_a q_k(s,a)$.</p><p>The value iteration includes solving the Bellman optimal equation part as show in red.</p><p><strong>Elementwise form</strong>:</p><p>​First, the elementwise form <em>policy update</em> is </p><p>$$<br>\begin{aligned}<br>\pi_{k+1} &amp; &#x3D; \arg \max_{\pi_k}(r_{\pi_k} + \gamma P_\pi v_k)\quad \text{(matrx-vector form)}\newline<br>\to \pi_{k+1}(s) &amp; &#x3D; \arg \max_{\pi_k} \sum_a \pi_k(a|s)<br>\Big(\sum_r p(r|s,a)r + \sum_{s^\prime} p(s^\prime|s,a) v_k(s^\prime)\Big) \quad \text{(elementwise  form)}\newline<br>\to \pi_{k+1}(s) &amp; &#x3D; \arg \max_{\pi_k} \sum_a \pi_k(a|s) q_{\pi_k}(s,a)\newline<br>\to \pi_{k+1}(s) &amp; &#x3D;  \sum_a \pi_k(a|s) \arg \max_{a_k}q_{\pi_k}(s,a)<br>\end{aligned}<br>$$</p><p>Then use the greedy policy update algorithm</p><p>$$<br>\pi_{k+1}(a|s) &#x3D;<br>\left {<br>    \begin{aligned}<br>        &amp; 1 &amp; a&#x3D;a_k^*(s)\newline<br>        &amp; 0 &amp; a\ne a_k^*(s)<br>    \end{aligned}<br>\right.<br>$$</p><p>where $a_k^*(s) &#x3D; \arg\max_a q_k(a,s)$.</p><p>​Second, the elementwise form <em>value update</em> is </p><p>$$<br>\begin{aligned}<br>    v_{k+1} &amp; &#x3D; r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}}v_k\quad \text{(matrx-vector form)}\newline<br>    \to v_{k+1}(s) &amp; &#x3D; \sum_a \pi_{k+1}(a|s)<br>    \underbrace{<br>    \Big(\sum_r p(r|s,a)r+ \gamma \sum_{s^\prime}p(s^\prime|s,a)v_k(s^\prime) \Big)}_{q_k(s,a)}\quad \text{(elementwise  form)}<br>\end{aligned}<br>$$</p><p><font color=blue>Since $\pi_{k+1}$ is a greedy policy, the above equation is simply</font></p><p>$$<br>v_{k+1}(s) &#x3D; \max_a q_k(s,a)<br>$$</p><p><strong>Pesudocode</strong>:</p><img src="https://pic.imgdb.cn/item/658be0b6c458853aef1b95ef.png" class="" width="800" height="400" title="state-transform"><h3 id="4-2-Policy-iteration"><a href="#4-2-Policy-iteration" class="headerlink" title="4.2 Policy iteration"></a>4.2 Policy iteration</h3><p>​The <font color=red>policy iteration</font> is not an algorithm directly solving the Bellman optimality equation. However, it has an intimate relationship to value iteration. The policy iteration includes two parts <font color=blue>policy evaluation</font> and <font color=blue>policy improvement</font>.</p><p><strong>Policy evaluation</strong>:</p><p>​The first step is policy evaluation. Mathematically, it is to sovle Bellman equation of $\pi_k$:</p><p>$$<br>v_{\pi_k} &#x3D; r_{\pi_k} + \gamma P_{\pi_k}v_{\pi_k}<br>$$</p><p>This is the matrix-vector form of the Bellman equation, where $r_{\pi_k}$ and $P_{\pi_k}$ are known. Here, $v_{\pi_k}$ is the state value to be solved.</p><p><font color=blue>How to calculate state value</font> $v_{\pi_k}$ is important. In section 2.4 we have already introduced how to solve Bellman equation to get state value $v_{\pi_k}$ as following</p><p>$$<br>\textcolor{blue}{v_{\pi_k}^{(j+1)} &#x3D; r_{\pi_k} + \gamma P_{\pi_k}v_{\pi_k}^{(j)}}, \quad j&#x3D;0,1,2,\dots<br>$$</p><p><font color=red>Its clear that policy iteration is directly calculate the state value using the above equation, which means the calculation is infity step calculation</font>.</p><p><strong>Policy improvement</strong>:</p><p>​The second step is to imporve the policy. How to do that? Once $v_{\pi_k}$ is calculated in the first step, a new and improved policy could be obtained as </p><p>$$<br>\pi_{k+1} &#x3D; \arg \max_\pi (r_\pi + \gamma P_\pi v_{\pi_k})<br>$$</p><p> <strong>Elementwise form</strong>:</p><p>​The policy evaluation step is to solve $v_{\pi_k}$ from the Bellman equation dirctly.</p><p>$$<br>\begin{aligned}<br>    v_{\pi_k}^{(j+1)} &amp; &#x3D; r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}^{(j)}\quad \text{(matrix-vector form)} \newline<br>    \to v_{\pi_k}^{(j+1)}(s) &amp; &#x3D; \sum_a \pi_k(a|s) \Big(\sum_r p(r|s,a)r + \sum_{s^\prime} p(s^\prime|s,a) v_{\pi_k}^{(j)} (s^\prime)\Big) \quad \text{(elementwise form)}<br>\end{aligned}<br>$$<br>where $j&#x3D;0,1,2,\dots$</p><p>The policy improvement step is to solve $\pi_{k+1}&#x3D;\arg \max_\pi (r_\pi + \gamma P_{\pi_k}v_{\pi_k})$.</p><p>$$<br>\begin{aligned}<br>    \pi_{k+1} &amp; &#x3D;\arg \max_{\pi_k} (r_{\pi_k} + \gamma P_{\pi_k}v_{\pi_k})\quad \text{(matrix-vector form)}\newline<br>    \to \pi_{k+1}(s) &amp; &#x3D; \arg \max_{\pi_k}  \sum_a \pi(a|s)<br>    \underbrace{<br>    \Big(\sum_r p(r|s,a)r + \sum_{s^\prime} p(s^\prime|s,a) v_{\pi_k} (s^\prime)\Big)}<em>{q</em>{\pi_k}(s,a)}<br>    \quad \text{(elementwise form)}\newline<br>    \to \pi_{k+1}(s) &amp; &#x3D;  \sum_a \pi(a|s) \arg \max_a q_{\pi_k}(s,a)<br>\end{aligned}<br>$$</p><p>Let $a^*<em>k(s) &#x3D; \arg \max_a q</em>{\pi_k}(s,a)$. The greedy optimal policy is</p><p>$$<br>\pi_{k+1}(a|s) &#x3D;<br>\left {<br>    \begin{aligned}<br>        &amp; 1 &amp; a&#x3D;a_k^*(s)\newline<br>        &amp; 0 &amp; a\ne a_k^*(s)<br>    \end{aligned}<br>\right.<br>$$</p><p><strong>Pesudocode:</strong></p><img src="https://pic.imgdb.cn/item/658be0dfc458853aef1c2798.png" class="" width="800" height="400" title="state-transform"><hr><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>  <a href="https://www.bilibili.com/video/BV1sd4y167NS/?spm_id_from=333.999.0.0">赵世钰老师的课程</a></p>]]></content>
      
      
      <categories>
          
          <category> Reinforcement Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reinforcement Learning with Code [Chapter 3. Optimal State Value and Bellman Optimal Equation]</title>
      <link href="/2023/07/23/Reinforcement-Learning-with-Code-Chapter-3-Optimal-State-Value-and-Bellman-Optimal-Equation/"/>
      <url>/2023/07/23/Reinforcement-Learning-with-Code-Chapter-3-Optimal-State-Value-and-Bellman-Optimal-Equation/</url>
      
        <content type="html"><![CDATA[<h1 id="Reinforcement-Learning-with-Code"><a href="#Reinforcement-Learning-with-Code" class="headerlink" title="Reinforcement Learning with Code."></a>Reinforcement Learning with Code.</h1><p>This note records how the author begin to learn RL. Both theoretical understanding and code practice are presented. Many material are referenced such as ZhaoShiyu’s <em>Mathematical Foundation of Reinforcement Learning</em>, .</p><h2 id="Chapter-3-Optimal-State-Value-and-Bellman-Optimality-Equation"><a href="#Chapter-3-Optimal-State-Value-and-Bellman-Optimality-Equation" class="headerlink" title="Chapter 3. Optimal State Value and Bellman Optimality Equation"></a>Chapter 3. Optimal State Value and Bellman Optimality Equation</h2><h3 id="3-1-How-to-define-optimal"><a href="#3-1-How-to-define-optimal" class="headerlink" title="3.1 How to define optimal"></a>3.1 How to define optimal</h3><p>​One core idea is that we use the action value to judge the optimality of the action. If we update the policy to select the action with the <em>greatest action value</em>, we could find a better policy.</p><p>​(<font color=blue>Optimal policy and optimal state value</font>). <em>A policy</em> $\pi^*$ <em>is optimal if</em> $v_{\pi^*}(s) \ge v_\pi(s)$ <em>for all</em> $s\in\mathcal{S}$ <em>and for any other policy</em> $\pi$. <em>The state values of</em>  $\pi^*$ <em>are the optimal state values</em>.</p><p><font color=blue>Why does the definition works? One intuitive explanation is that state value $v_\pi(s)$ denote the mean of return along the trajectory following policy $\pi$. If the policy $\pi^*$ has greatest expectation of return, hence we can believe that the policy $\pi^*$ is optimal.</font></p><h3 id="3-2-Bellman-optimal-equation-BOE"><a href="#3-2-Bellman-optimal-equation-BOE" class="headerlink" title="3.2 Bellman optimal equation (BOE)"></a>3.2 Bellman optimal equation (BOE)</h3><p>​The Bellamn optimal eqaution (BOE) is </p><p>$$<br>\begin{aligned}<br>v(s) &amp; &#x3D; \max_\pi \sum_a \pi(a|s) \Big[ \sum_r p(r|s,a)r + \gamma \sum_{s^\prime} p(s^{\prime}|s,a) v_\pi(s^{\prime}) \Big] \newline<br>\textcolor{red}{v(s)} &amp; \textcolor{red}{&#x3D; \max_\pi \sum_a \pi(a|s) q_\pi(s,a)}<br>\end{aligned}<br>$$</p><p>There is a problem that the BOE has two unknown variables $q_\pi(s,a)$ and $\pi(a|s)$. How can we solve the BOE?</p><p>​The idea is that we can <font color=blue>fix one variable</font> and solve the maximization problem. For Bellman optimal equation, we can <font color=blue>fix variable $\pi(a|s)$ for all $a\in\mathcal{A}(s)$</font> and then by maximize the state value to find the optimal policy $\pi$.</p><p>​Before analysis, we consider one example first. Suppose $x_1,x_2,x_3\in\mathbb{R}$ are given. Find $c_1^*,c_2^*,c_3^*$ solving<br>$$<br>\max_{c_1,c_2,c_3} c_1x_1+c_2x_2+c_3x_3\newline<br>\text{subject to } c_1+c_2+c_3&#x3D;1<br>$$<br>Without generality, suppose $x_3 \ge x_1, x_2$. Then, the optimal solution is $c_3^*&#x3D;1$ and $c_1^*&#x3D;c_2^*&#x3D;0$. This is because for any $c_1,c_2,c_3$<br>$$<br>x_3 &#x3D; (c_1+c_2+c_3)x_3 &#x3D; c_1x_3 + c_2x_3 + c_3x_3 \ge c_1x_1 +c_2x_2+c_3x_3<br>$$<br>​Hence, inspired by the above example, considering that $\sum_a \pi(a|s)&#x3D;1$, we have</p><p>$$<br>\begin{aligned}<br>v(s) &amp; &#x3D; \max_\pi \sum_a \pi(a|s) q_\pi(s,a) \newline<br>&amp; &#x3D; \sum_a \pi(a|s) \max_\pi q_\pi(s,a) \quad \text{by fix } \pi(a|s) \newline<br>&amp; &#x3D; \sum_a \pi(a|s) \max_{a\in\mathcal{A}} q_\pi(s,a) \newline<br>&amp; \le \max_{a\in\mathcal{A}} q_\pi(s,a)<br>\end{aligned}<br>$$</p><p>where the equality is achieved when $a&#x3D;a^*$, $\pi(a|s)&#x3D;1$ when $a\ne a^*$, $\pi(a|s)&#x3D;0$.</p><p>$$<br>a^* &#x3D; \text{arg} \max_a q(s,a)<br>$$</p><p>This policy is often called <font color=blue>greedy policy</font>.</p><h3 id="3-3-Matrix-vector-form-of-Bellman-optimal-equation"><a href="#3-3-Matrix-vector-form-of-Bellman-optimal-equation" class="headerlink" title="3.3 Matrix-vector form of Bellman optimal equation"></a>3.3 Matrix-vector form of Bellman optimal equation</h3><p>Refering to matrix-vector form of Bellman equation, it’s obvious to get matrix-vector form of Bellman optimal equation as follows<br>$$<br>\textcolor{blue} {v &#x3D; \max_\pi (r_\pi +\gamma P_\pi v)}<br>$$<br>where $v\in\mathbb{R^{|\mathcal{S}|}}$. The structure of $r_\pi$ and $P_\pi$ are the same as those in the matrix-vector form of Bellman equation:</p><p>$$<br>[r_\pi]_s \triangleq \sum_a \pi(a|s) \sum_r p(r|s,a)r<br>$$</p><p>$$<br>[P_\pi]<em>{s, s^\prime} &#x3D; \sum_a \pi(a|s) \sum</em>{s^\prime} p(s^\prime|s,a)<br>$$</p><p>Furthermore, denote the right hand side as </p><p>$$<br>f(v) \triangleq \max_\pi(r_\pi + \gamma P_\pi v)<br>$$</p><p>Hence, we have the Bellman optimal equation as</p><p>$$<br>v&#x3D;f(v) \newline<br>f(v) - v &#x3D; 0<br>$$</p><p><font color=blue>It turns solving Bellman optimal equation to solving find the root of function</font> $\textcolor{blue}{f(v) - v &#x3D;0}$.</p><h3 id="3-4-Contraction-mapping-theorem"><a href="#3-4-Contraction-mapping-theorem" class="headerlink" title="3.4 Contraction mapping theorem"></a>3.4 Contraction mapping theorem</h3><p>​<font color=red>Fixed point</font>: consider a function $f(x)$ where $x\in\mathbb{R}^b$ and $f:\mathbb{R}^b\to\mathbb{R}^b$. A point $x^*$ is called a fixed point if<br>$$<br>f(x^*) &#x3D; x^*<br>$$<br>The function $f$ is called a <font color=red>contracting mapping</font> if there <font color=blue>exists</font> $\gamma\in(0,1)$ such that<br>$$<br>||f(x_1)-f(x_2)|| \le \gamma ||x_1 - x_2||<br>$$<br>for any $x_1, x_2\in \mathbb{R}$.</p><p>​(<font color=red>Contraction mapping theorem</font>) For any equation that has the form of $x&#x3D;f(x)$ where $x$ and $f(x)$ are real vectors, if $f$ is a contraction mapping, then</p><ul><li><p>Existence: There exists a fixed point $x^*$ satisfying $f(x^*)&#x3D;x^*$.</p></li><li><p>Uniqueness: The fixed point $x^*$ is unique.</p></li><li><p>Algorithm: Consider the iterative process:<br>$$<br>x_{k+1} &#x3D; f(x_k)<br>$$<br>where $k&#x3D;0,1,2,\dots$. Then , $x_k\to x^*$ as $k\to \infty$ for any initial guess $x_0$. Moreover, the convergence rate is exponentially fast.</p></li></ul><h3 id="3-5-Solution-of-BOE"><a href="#3-5-Solution-of-BOE" class="headerlink" title="3.5 Solution of BOE"></a>3.5 Solution of BOE</h3><p>​(Contraction property of BOE). The function $f(v) \triangleq \max_\pi(r_\pi + \gamma P_\pi v)$ in the BOE is a contraction mapping statisfying<br>$$<br>||f(v_1)-f(v_2)|| \le \gamma ||v_1 - v_2||<br>$$<br>where $v_1,v_2\in\mathbb{R}^{|\mathcal{S}|}$ are any two vectors and $\gamma\in(0,1)$ is the discounted rate.</p><p>The proof is omitted.</p><p>​Using contraction mapping theorem to solve BOE, we have the following theorem:</p><p>​(<font color=red>Existence, Uniqueness, Algorithm</font>). <em>For the BOE</em> $v&#x3D;f(v) \triangleq \max_\pi(r_\pi + \gamma P_\pi v)$, <em>there always exists a unique solution</em> $v^*$, <em>which can be solved iteratively by</em><br>$$<br>v_{k+1} &#x3D; f(v_k) &#x3D; \max_\pi(r_\pi + \gamma P_\pi v_k), \quad k&#x3D;0,1,\dots<br>$$<br><em>The sequence</em> ${v(k)}$ <em>converges to optimal solution of BOE</em> $v^*$ <em>exponentially fast given any inital guess</em> $v_0$.</p><p>This algorithm is actually called <font  color=blue>value iteration</font>. </p><p>​<font color=blue>First</font>, following the algorithm above will give us the optimal state value $v^*$. <font color=blue>Second</font>, we solve the unknown policy $\pi$ in the BOE. Suppose $v^*$ has been obtained and<br>$$<br>\pi^* &#x3D; \text{arg}\max_\pi(r_\pi+\gamma P_\pi v^*)<br>$$<br>Then, $v^*$ and $\pi^*$ statisfy<br>$$<br>v^* &#x3D; r_{\pi^*} + \gamma P_{\pi^*} v^*<br>$$<br>​(<font color=blue>Greedy optimal policy</font>). <em>For any</em> $s\in\mathcal{S}$, <em>the deterministic greedy policy</em> </p><p>where the equality is achieved when $a&#x3D;a^*$, $\pi(a|s)&#x3D;1$ when $a\ne a^*$, $\pi(a|s)&#x3D;0$.</p><p><em>is an optimal policy solving the BOE. Here</em><br>$$<br>a^*(s) &#x3D; \text{arg} \max_a q^*(a,s)<br>$$<br><em>where</em><br>$$<br>q^*(s,a) \triangleq \sum_r p(r|s,a)r + \gamma \sum_{s^\prime} p(s^{\prime}|s,a) v^*(s)<br>$$<br>​The matrix-vecotr form of the optimal policy is $\pi^*&#x3D;\text{arg}\max_\pi(r_\pi+\gamma P_\pi v^*(s))$. Its <font color=blue>elementwise form</font> is</p><p>$$<br>\begin{aligned}<br> \pi^* &amp; &#x3D; \text{arg}\max_\pi(r_\pi+\gamma P_\pi v^*)\newline<br> \to \pi^*(s) &amp; &#x3D; \text{arg}\max_\pi \sum_a \pi(a|s) \Big( \sum_r p(r|s,a)r + \gamma \sum_{s^\prime} p(s^{\prime}|s,a) v^*(s)\Big)\newline<br> \to \pi^*(s) &amp; &#x3D; \text{arg}\max_\pi \sum_a \pi(a|s) q^*(s,a),\quad \text{for s}\in\mathcal{S}<br>\end{aligned}<br>$$</p><p>It is clear that $\sum_a \pi(a|s)q^*(s,a)$ is maximized if $\pi(s)$ selects the action with the greatest value of $q^*(s,a)$.</p><hr><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>  <a href="https://www.bilibili.com/video/BV1sd4y167NS/?spm_id_from=333.999.0.0">赵世钰老师的课程</a></p>]]></content>
      
      
      <categories>
          
          <category> Reinforcement Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reinforcement Learning with Code [Chapter 2. State Value and Bellman Equation]</title>
      <link href="/2023/07/21/Reinforcement-Learning-with-Code-Chapter-2-State-Value-and-Bellman-Equation/"/>
      <url>/2023/07/21/Reinforcement-Learning-with-Code-Chapter-2-State-Value-and-Bellman-Equation/</url>
      
        <content type="html"><![CDATA[<h1 id="Reinforcement-Learning-with-Code"><a href="#Reinforcement-Learning-with-Code" class="headerlink" title="Reinforcement Learning with Code."></a>Reinforcement Learning with Code.</h1><p>This note records how the author begin to learn RL. Both theoretical understanding and code practice are presented. Many material are referenced such as ZhaoShiyu’s <em>Mathematical Foundation of Reinforcement Learning</em>, .</p><h2 id="Chapter-2-State-Value-and-Bellman-Equation"><a href="#Chapter-2-State-Value-and-Bellman-Equation" class="headerlink" title="Chapter 2. State Value and Bellman Equation"></a>Chapter 2. State Value and Bellman Equation</h2><h3 id="2-1-State-value"><a href="#2-1-State-value" class="headerlink" title="2.1 State value"></a>2.1 State value</h3><ul><li><p><font color=red>State value</font> is defined as the mean of <font color=blue>all possible returns</font> starting from a state, which is actually the <font color=blue>expectation of return</font> from a specific state.</p></li><li><p>The mathematical definition is as follows:</p><p>Note that the capital letters denote <em>random variables</em>, such as $S_t, S_{t+1}, A_t, R_{t+1}$. In particular, $S_t,S_{t+1}\in\mathcal{S},A_t\in\mathcal{A}(S_t)$ and $R_{t+1}\in\mathcal{R}(S_t,A_t)$. $G_t$ denote the random variable of return.</p><p>Starting from $t$, we can obtain a state-action-reward trajectory:</p><p>$$<br>S_t \xrightarrow{A_t} S_{t+1},R_{t+1} \xrightarrow{A_{t+1}} S_{t+2},R_{t+2} \xrightarrow{A_{t+2}} S_{t+3},R_{t+3} \cdots<br>$$</p><p>The discounted return along the trajectory is </p><p>$$<br>G_t &#x3D; R_{t+1} + \gamma R_{t+2} + \gamma R_{t+3} + \cdots<br>$$</p><p>The state value is defined as:</p><p>$$<br>\textcolor{blue}{v_\pi(s)\triangleq \mathbb{E}[G_t|S_t&#x3D;s]}<br>$$</p><p>which means <font color=blue>start from state</font> $s$ can get the <font color=blue>expectation return</font> along the trajecotry generated by <font color=blue>policy $\pi$ </font>. </p><p>$v_\pi(s)$ is also called <font color=blue>state-value funtion</font>.</p></li></ul><h3 id="2-2-Bellman-Equation"><a href="#2-2-Bellman-Equation" class="headerlink" title="2.2 Bellman Equation"></a>2.2 Bellman Equation</h3><ul><li><p>Bellman equation is a set of linear equations <font color=blue>describing the relationship among the values of all the states</font>.</p><p>For example,</p></li></ul><img src="https://pic.imgdb.cn/item/658bd598c458853aeff51217.png" class="" width="400" height="400" title="state-transform"><p>  Let $v_i$ denote the return obtained starting from $s_i$. The return starting from the four states in figure can be respectively calculated as </p><p>$$<br>v_1 &#x3D; r_1 + \gamma r_2 + \gamma^2 r_3 + \cdots \newline<br>v_2 &#x3D; r_2 + \gamma r_3 + \gamma^2 r_4 + \cdots \newline<br>v_3 &#x3D; r_3 + \gamma r_4 + \gamma^2 r_1 + \cdots \newline<br>v_4 &#x3D; r_4 + \gamma r_1 + \gamma^2 r_2 + \cdots<br>$$</p><p> Using the idea of <em>bootstrapping</em>, we can rewrite it to</p><p>$$<br>v_1 &#x3D; r_1 + \gamma (r_2 + \gamma r_3) + \cdots &#x3D; r_1 + \gamma v_2 \newline<br>v_2 &#x3D; r_2 + \gamma (r_3 + \gamma r_4) + \cdots &#x3D; r_2 + \gamma v_3 \newline<br>v_3 &#x3D; r_3 + \gamma (r_4 + \gamma r_1) + \cdots &#x3D; r_3 + \gamma v_4 \newline<br>v_4 &#x3D; r_4 + \gamma (r_1 + \gamma r_2) + \cdots &#x3D; r_4 + \gamma v_1<br>$$</p><p>  Then rewrite it into matrix form</p><p>$$<br>\textbf{v} &#x3D;  \begin{bmatrix} v_1 \newline v_2 \newline v_3 \newline v_4 \end{bmatrix} , \textbf{r} &#x3D; \begin{bmatrix} r_1 \newline r_2 \newline r_3 \newline r_4 \end{bmatrix}, \textbf{v} &#x3D;  \begin{bmatrix} v_1 \newline v_2 \newline v_3 \newline v_4 \end{bmatrix}<br>$$</p><p>$$<br>\textbf{P} &#x3D;  \begin{bmatrix} 0 &amp; 1 &amp; 0 &amp; 0 \newline 0 &amp; 0 &amp; 1 &amp; 0 \newline 0 &amp; 0 &amp; 0 &amp; 1 \newline 1 &amp; 0 &amp; 0 &amp; 0 \end{bmatrix}<br>$$</p><p> The equation $\textbf{v}&#x3D;\textbf{r}+\gamma\textbf{P}\textbf{v}$ is called Bellman equation.</p><ul><li>Then we can derive Bellman equation from scratch as follows:</li></ul><p>Note that the discounted return random variable $G_t$ can be rewritten as </p><p>$$<br>\begin{aligned}<br>G_t &amp; &#x3D;  R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots\newline<br>&amp; &#x3D;  R_{t+1} + \gamma(R_{t+2}+\gamma R_{t+3} + \cdots) \newline<br>&amp; &#x3D;  R_{t+1} + \gamma G_{t+1}<br>\end{aligned}<br>$$</p><p> where $G_{t+1} &#x3D; R_{t+2} + \gamma R_{t+3} + \cdots$ . This equation establishes the relationship between $G_t$ and $G_{t+1}$. Then the state value can be rewritten as </p><p>$$<br>\begin{aligned}<br>v_\pi(s) &amp; &#x3D;  \mathbb{E}[G_t|S_t&#x3D;s] \newline<br>&amp; &#x3D;  \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t&#x3D;s] \newline<br>&amp; &#x3D;  \mathbb{E}[R_{t+1}|S_t&#x3D;s] + \gamma \mathbb{E}[G_{t+1}|S_t&#x3D;s]<br>\quad \text{(linear property of expectation)}<br>\end{aligned}<br>$$</p><p> The <font color = blue> mean of immediate reward</font> can be written as (use 2.3 conditional expectation)</p><p>$$<br>\begin{aligned}<br>\mathbb{E}[R_{t+1}|S_t&#x3D;s] &amp; &#x3D;  \sum_a \pi(a|s) \mathbb{E}[R_{t+1} | S_t&#x3D;s,A_t&#x3D;a] \quad \text{(conditional expectation)}\newline<br>&amp; &#x3D;  \sum_a \pi(a|s) \sum_r p(r|s,a)r<br>\end{aligned}<br>$$</p><p>  The <font color = blue>mean of future reward</font> can be written as </p><p>$$<br>\begin{aligned}<br>\gamma \mathbb{E}[G_{t+1}|S_t&#x3D;s] &amp; &#x3D;  \gamma \sum_a \pi(a|s) \mathbb{E}[G_{t+1}|S_t&#x3D;s, A_t&#x3D;a]\quad \text{(conditional expectation)}\newline<br>&amp; &#x3D;  \gamma \sum_a \pi(a|s) \sum_{s^\prime} p(s^\prime|s,a) v_\pi(s^\prime)\newline<br>\end{aligned}<br>$$</p><p>Then the state value can be rewritten as <font color=red>Bellman equation (BE)</font> form </p><p>$$<br>\begin{aligned}<br>v_\pi(s) &amp; &#x3D;  \mathbb{E}[G_t|S_t&#x3D;s] \newline<br>&amp; &#x3D;  \mathbb{E}[R_{t+1}|S_t&#x3D;s] + \gamma \mathbb{E}[G_{t+1}|S_t&#x3D;s] \newline<br>&amp; &#x3D;<br>\sum_a \pi(a|s) \sum_r p(r|s,a)r </p><p>+<br>\gamma \sum_a \pi(a|s) \sum_{s^\prime} p(s^\prime|s,a) v_\pi(s^\prime)\newline<br>&amp; &#x3D;  \sum_a \pi(a|s) \Big[\sum_r p(r|s,a)r+ \gamma\sum_{s^\prime} p(s^\prime|s,a) v_\pi(s^\prime)\Big], \quad \text{for all } s\in\mathcal{S}<br>\end{aligned}<br>$$</p><p>Next, we will introduce the matrix vector form of Bellman equation in terms of state value. Let $v_\pi&#x3D;[v_\pi(s_1),v_\pi(s_2),\cdots]^T\in\mathbb{R}^n$, $r_\pi&#x3D;[r_\pi(s_1),r_\pi(s_2),\cdots]^T\in\mathbb{R}^n$, and $P_\pi\in\mathbb{R}^{n\times n}$. Then we have the matrix-vector form of Bellman equation in terms of state value.</p><p>$$<br>\textcolor{red}{v_\pi &#x3D; r + \gamma P_\pi v_\pi} \quad \text{(matrix-vector form)}<br>$$</p><p>where</p><p>$$<br>[r_\pi]<em>s \triangleq \sum_a \pi(a|s) \sum_r p(r|s,a)r,<br>[P_\pi]</em>{s,s^\prime} &#x3D; \sum_a \pi(a|s) \sum_{s^\prime} p(s^\prime|s,a)<br>$$</p><h3 id="2-3-Review-conditional-expectaion"><a href="#2-3-Review-conditional-expectaion" class="headerlink" title="2.3 Review conditional expectaion"></a>2.3 Review conditional expectaion</h3><ul><li>The definition of conditional expectation is</li></ul><p>$$<br>\mathbb{E} &#x3D; [X|A&#x3D;a] &#x3D; \sum_{x}xp(x|a)<br>$$</p><p>  Similar to the law of total probability, we have the law of total expectation:</p><p>$$<br>\mathbb{E}[X] &#x3D; \sum_a p(a) \mathbb{E}[X|A&#x3D;a]<br>$$</p><p>  <em>Proof</em></p><p>  By definition of expectation the right hand side can be written as </p><p>$$<br>\begin{aligned}<br>\mathbb{E}[X] &amp; &#x3D;  \sum_a p(a) \sum_x x p(x|a)\newline<br>&amp; &#x3D;  \sum_x \sum_a p(x|a) p(a)  \quad \text{(law of total probability)}\newline<br>&amp; &#x3D;  \sum_x p(x)\newline<br>&amp; &#x3D;  \mathbb{E}[X]<br>\end{aligned}<br>$$</p><h3 id="2-4-Solve-Bellman-Equation"><a href="#2-4-Solve-Bellman-Equation" class="headerlink" title="2.4 Solve Bellman Equation"></a>2.4 Solve Bellman Equation</h3><p>​From the analysis above we are informed that Bellman equation is</p><p>$$<br>\textcolor{blue}{v_\pi(s)&#x3D;\sum_a \pi(a|s) \Big[\sum_r p(r|s,a)r+ \gamma\sum_{s^\prime} p(s^\prime|s,a) v_\pi(s^\prime)\Big]}<br>$$</p><p>We can rewrite it into the follow form</p><p>$$<br>v_\pi(s) &#x3D; r_\pi(s) + \gamma \sum_{s^\prime} p_\pi(s^\prime|s) v_\pi(s^\prime)<br>$$</p><p>where</p><p>$$<br>\begin{aligned}<br>r_\pi(s) \triangleq \sum_a \pi(a|s) \sum_r p(r|s,a)r\newline<br>\newline<br>\newline<br>\newline<br>\end{aligned} \qquad<br>\begin{aligned}<br>p_\pi(s^\prime|s) &amp; \triangleq \sum_a \pi(a|s) \sum_{s^\prime} p(s^\prime|s,a)\newline<br>&amp; &#x3D; \sum_{s^\prime} \sum_a \pi(a|s) p(s^\prime|s,a)\newline<br>&amp; &#x3D; p_\pi(s^\prime|s)<br>\end{aligned}<br>$$</p><p>​Suppose the states are indexed as $s_i$. For state $s_i$, the Bellman equation is </p><p>$$<br>v_\pi(s_i) &#x3D; r_\pi(s_i) + \gamma \sum_{s_j} p_\pi(s_j|s_i) v_\pi(s_j)<br>$$</p><p>Let $v_\pi&#x3D;[v_\pi(s_1),\dots,v_\pi(s_n)]^T \in \mathbb{R}^n$, $r_\pi&#x3D;[r_\pi(s_1),\dots,r_\pi(s_n)]^T \in \mathbb{R}^n$, and $P_\pi \in \mathbb{R}^{n\times n}$, where $[P_\pi]_{ij} &#x3D; p_\pi(s_j|s_i)$. Hence we have the matrix form as </p><p>$$<br>v_\pi &#x3D; r_\pi + \gamma P_\pi v_\pi<br>$$</p><p>For example, consider four states $s_i$ and four actions $a_i$ the matrix form can be</p><p>$$<br>\underbrace{<br>\begin{bmatrix}<br>v_\pi(s_1) \newline<br>v_\pi(s_2) \newline<br>v_\pi(s_3) \newline<br>v_\pi(s_4)<br>\end{bmatrix}<br>}<em>{v_\pi} &#x3D;<br>\underbrace{<br>\begin{bmatrix}<br>r_\pi(s_1) \newline<br>r_\pi(s_1) \newline<br>r_\pi(s_1) \newline<br>r_\pi(s_1)<br>\end{bmatrix}<br>}</em>{r_\pi} + \gamma<br>\underbrace{<br>\begin{bmatrix}<br>p_\pi(s_1|s_1) &amp; p_\pi(s_2|s_1) &amp; p_\pi(s_3|s_1) &amp; p_\pi(s_4|s_1) \newline<br>p_\pi(s_1|s_2) &amp; p_\pi(s_2|s_2) &amp; p_\pi(s_3|s_2) &amp; p_\pi(s_4|s_2) \newline<br>p_\pi(s_1|s_3) &amp; p_\pi(s_2|s_3) &amp; p_\pi(s_3|s_3) &amp; p_\pi(s_4|s_3) \newline<br>p_\pi(s_1|s_4) &amp; p_\pi(s_2|s_4) &amp; p_\pi(s_3|s_4) &amp; p_\pi(s_4|s_4) \newline<br>\end{bmatrix}<br>}<em>{P_\pi}<br>\underbrace{<br>\begin{bmatrix}<br>v_\pi(s_1) \newline<br>v_\pi(s_2) \newline<br>v_\pi(s_3) \newline<br>v_\pi(s_4)<br>\end{bmatrix}<br>}</em>{v_\pi}<br>$$</p><p><strong>Conclusions</strong>:</p><ul><li>So we have the <font color=blue>closed form</font> of the solution as $v_\pi &#x3D; (I-\gamma P_\pi)^{-1}r_\pi$. However the inverse operation is hard to implement.</li><li>We seek for other iterative solving methods.</li></ul><p><strong>Iterative solution</strong></p><p>​We can directly sovle the Bellman equation using the following iterative algorithm:</p><p>$$<br>\textcolor{blue}{v_{k+1} &#x3D; r_\pi + \gamma P_\pi v_k}<br>$$</p><p>The proof is omitted.</p><h3 id="2-5-Action-value"><a href="#2-5-Action-value" class="headerlink" title="2.5 Action value"></a>2.5 Action value</h3><p>​<font color=red>Action value</font> is denoted by $q_\pi(s,a)$ which is defined as</p><p>$$<br>q_\pi(s,a) \triangleq \mathbb{E}[G_t|S_t&#x3D;s, A_t&#x3D;a]<br>$$</p><p>for all $s \in \mathcal{S}, a\in \mathcal{A}(s)$. </p><p>Action value can be interpreted as <font color=blue>average return</font> along trajectory generated by <font color=blue>policy $\pi$ </font> after taking a <font color=blue>specific action $a$</font>.</p><p>​From the conditional expectation property that</p><p>$$<br>\underbrace{\mathbb{E}[G_t|S_t&#x3D;s]}<em>{v_\pi(s)} &#x3D; \sum_a \pi(a|s) \underbrace{\mathbb{E}[G_t|S_t&#x3D;s,A_t&#x3D;a]}</em>{q_\pi(s)}<br>$$</p><p>Hence, </p><p>$$<br>v_\pi(s) &#x3D; \sum_a \pi(a|s) q_\pi(s)<br>$$</p><p>So we can obtain the mathematical definition of action value as</p><p>$$<br>\textcolor{blue}{q_\pi(s)&#x3D;\sum_r p(r|s,a)r+ \gamma\sum_{s^\prime} p(s^\prime|s,a) v_\pi(s^\prime)}<br>$$</p><p>Substituting $(1)$ into $(2)$ we have</p><p>$$<br>q_\pi(s,a)&#x3D;\sum_r p(r|s,a)r+ \gamma\sum_{s^\prime} p(s^\prime|s,a) \sum_{a^\prime \in \mathcal{A}(s^\prime)}\pi(a^\prime|s^\prime) q_\pi(s^\prime,a^\prime)<br>$$</p><p>​Suppose each state has the same number of actions. The matrix-vecotr form of Bellman eqaution in terms of action value is</p><p>$$<br>\textcolor{red}{q_\pi &#x3D; \tilde{r} + \gamma P \Pi q_\pi}\quad \text{(matrix-vector form)}<br>$$</p><p>where $q_\pi \in \mathbb{R}^{|\mathcal{S}||\mathcal{A}|}$ is the action value vector indexed by state-action pairs. In particular, the $(s,a)$th element is </p><p>$$<br>[q_\pi]_{(s,a)} &#x3D; q_\pi(s,a)<br>$$</p><p>Here, $\tilde{r}\in\mathbb{R}^{|\mathcal{S} ||\mathcal{A} |}$ is the immediate reward vector indexed by state-action pairs. In paricular, the $(s,a)$th reward is</p><p>$$<br>[\tilde{r}]_{(s,a)} &#x3D; \sum_r p(r|s,a)r<br>$$</p><p>Here, $P\in \mathbb{R}^{|\mathcal{S}||\mathcal{A}|\times|\mathcal{S}| }$ is the probability transition matrix, whose row is indexed by state-action pairs and column indexed by states. In particular</p><p>$$<br>[P]_{(s,a),s^\prime} &#x3D; p(s^\prime|s,a)<br>$$</p><p>And $\Pi \in\mathbb{R}^{|\mathcal{S}|\times|\mathcal{S}||\mathcal{A}|}$ describes the policy $\pi$. In particular</p><p>$$<br>\Pi_{s^\prime, (s^\prime,a^\prime)} &#x3D; \pi(a^\prime|s^\prime)<br>$$</p><p>and the other entries of $\Pi$ is zero. $\Pi$ is block diagonal matrix with each block as a $1\times|\mathcal{A}|$ vector.</p><hr><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>  <a href="https://www.bilibili.com/video/BV1sd4y167NS/?spm_id_from=333.999.0.0">赵世钰老师的课程</a></p>]]></content>
      
      
      <categories>
          
          <category> Reinforcement Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reinforcement Learning with Code [Chapter 1. Basic Concepts]</title>
      <link href="/2023/07/02/Reinforcement-Learning-with-Code-Chapter-1-Basic-Concepts/"/>
      <url>/2023/07/02/Reinforcement-Learning-with-Code-Chapter-1-Basic-Concepts/</url>
      
        <content type="html"><![CDATA[<h1 id="Reinforcement-Learning-with-Code"><a href="#Reinforcement-Learning-with-Code" class="headerlink" title="Reinforcement Learning with Code."></a>Reinforcement Learning with Code.</h1><p>This note record how the author begin to learn RL. Both theoretical understanding and code practice are presented. Many material are referenced such as ZhaoShiyu’s <em>Mathematical Foundation of Reinforcement Learning</em> .</p><h2 id="Chapter-1-Basic-Concepts"><a href="#Chapter-1-Basic-Concepts" class="headerlink" title="Chapter 1. Basic Concepts"></a>Chapter 1. Basic Concepts</h2><h3 id="1-1-State-and-action"><a href="#1-1-State-and-action" class="headerlink" title="1.1 State and action"></a>1.1 State and action</h3><ul><li><font color=red><em>State</em> </font>describe the status of the agent with respect to the environment, denoted by $s$.</li><li><font color=red><em>State space</em> </font> is the set of all states, denoted by $\mathcal{S}&#x3D;{s_1, s_2,\dots,s_n}$.</li><li><font color = red><em>Action</em></font> describe the action that the agent may take with respect to the environment, denoted by $a$.</li><li><font color = red><em>Action space</em></font> is the set of all actions, denoted by $\mathcal{A}&#x3D;{a_1, a_2,\dots,a_n}.$</li></ul><h3 id="1-2-State-transition"><a href="#1-2-State-transition" class="headerlink" title="1.2 State transition"></a>1.2 State transition</h3><p>When taking an action, the agent may move from one state to another. Such a process is called <font color=red><em>state transition</em></font>. State trasition can be denoted by<br>$$<br>s_1 \stackrel{a_2}\longrightarrow s_2<br>$$<br>described by $p(s^\prime|s,a)$.</p><p>State trainsition can be both <em>deterministic</em> and <em>stochastic</em>. For example the deterministic state transition is<br>$$<br>p(s_1|s_1,a_2) &#x3D; 0 \<br>p(s_2|s_1,a_2) &#x3D; 1 \<br>p(s_3|s_1,a_2) &#x3D; 0<br>$$<br>For example the stochastic state transition is<br>$$<br>p(s_1|s_1,a_2) &#x3D; 0.5 \<br>p(s_2|s_1,a_2) &#x3D; 0.3 \<br>p(s_3|s_1,a_2) &#x3D; 0.2<br>$$</p><h3 id="1-3-Policy"><a href="#1-3-Policy" class="headerlink" title="1.3 Policy"></a>1.3 Policy</h3><ul><li><font color = red><em>Policy</em></font> tells the agents which actions to take <font color=blue>at each state</font>, denoted by $\pi$.</li><li>Policy is described by conditional probability.</li><li>Policy can be <em>deterministic</em> or <em>stochastic</em>, which means one state has a deterministic action or one state has probability to select other actions.</li></ul><p>Suppose the actions space is $\mathcal{A}&#x3D;{a_1, a_2,a_3}$, such  deterministic policy can be dentoed by<br>$$<br>\pi(a_1|s_1) &#x3D; 0 \<br>\pi(a_2|s_1) &#x3D; 1 \<br>\pi(a_3|s_1) &#x3D; 0<br>$$<br>which indicated the probability of taking action $a_2$ is $1$ and others are zero. </p><p>Such stochastic policy can be denoted by<br>$$<br>\pi(a_1|s_1) &#x3D; 0.5 \<br>\pi(a_2|s_1) &#x3D; 0.3 \<br>\pi(a_3|s_1) &#x3D; 0.2<br>$$</p><h3 id="1-4-Reward"><a href="#1-4-Reward" class="headerlink" title="1.4 Reward"></a>1.4 Reward</h3><ul><li><font color = red><em>Reward</em></font> is one of the most unique concept in RL.</li><li><font color = red><em>Immediate reward</em></font> can be obtained after taking an action.</li><li><font color = red><em>Reward transition</em></font> is the process of getting a reward after taking an action, reward transition can be <em>deterministic</em> or <em>stochastic</em>. Reward transition is described by $p(r|s,a)$</li></ul><p>For example deterministic reward transition can be denoted by<br>$$<br>p(r&#x3D;-1|s_1,a_2) &#x3D; 1, p(r\ne -1|s_1,a_2)&#x3D;0<br>$$<br>which means at state $s_1$ take action $a_2$ the probability to get immediate reward $-1$ is $1$.</p><p>Stochastic reward transition can be denoted by<br>$$<br>p(r&#x3D;1|s_1,a_2) &#x3D; 0.5, p(r&#x3D; 0|s_1,a_2)&#x3D;0.5<br>$$<br>which means at state $s_1$ take action $a_2$ the probability to get immediate reward $1$ is $0.5$, the probability to get immediate reward $0$ is $0.5$.</p><h3 id="1-5-Trajectory-return-episode"><a href="#1-5-Trajectory-return-episode" class="headerlink" title="1.5 Trajectory, return, episode"></a>1.5 Trajectory, return, episode</h3><ul><li><p><font color = red><em>Trajectory</em></font> is a state-action-reward chain, such as $s_1 \underset{r&#x3D;0}{\xrightarrow{a_2}} s_2 \underset{r&#x3D;0}{\xrightarrow{a_3}} s_3 \underset{r&#x3D;0}{\xrightarrow{a_4}} \cdots\underset{r&#x3D;1}{\xrightarrow{a_n}} s_{n}$.</p></li><li><p><font color = red><em>Return</em></font> of this trajecotry is the sum of all the rewards collected along the trajectory, such as $\text{return} &#x3D; 0+0+0+\cdots+1&#x3D;1$. Return is also called <font color=blue><em>total rewards</em></font> or <font color=blue><em>cumulative rewards</em></font>.</p></li><li><p><font color = red><em>Discounted return</em></font> is defined by the <font color=blue><em>discounted rate</em></font>, denoted by $\gamma\in(0,1)$. Such discounted retrun is<br>$$<br>\text{discounted return} &#x3D; 0+\gamma0+\gamma^2 0 + \gamma^3 0 + \cdots + \gamma^n 1<br>$$</p></li><li><p><font color = red><em>Episode</em></font> refers the trajectory that interacting with the enviornment following a policy <font color = blue>until the agent reach the terminal state</font>. An episode is usually assumed to be a finite trajectory,  that task with episodes are called <em>episodic tasks</em>. Some task may have no terminal state, such task is called <em>continuing tasks</em>.</p></li></ul><h3 id="1-6-Markov-decision-process-MDP"><a href="#1-6-Markov-decision-process-MDP" class="headerlink" title="1.6 Markov decision process (MDP)"></a>1.6 Markov decision process (MDP)</h3><p>Markov decision process is a general framework to <font color=blue>describe stochastic dynamical systems</font>. The key ingredients of an MDP are listed:</p><ul><li><p>Sets:</p><ul><li>State set: the set of all states, denoted as $\mathcal{S}$.</li><li>Actions set: a set of actions, denoted as $\mathcal{A}(s)$, is associated for each state $s\in\mathcal{S}$.</li><li>Reward set: a set of rewards, denoted as $\mathcal{R}(s,a)$, is associated for each state action pari $(s,a)$.</li></ul></li><li><p>Model:</p><ul><li>State transition probability: at state $s$, taking actions $a$, the probability to transit to state $s^\prime$ is $p(s^\prime|s,a)$.</li><li>Reward transition probability: at state $s$, taking action $a$, the probability to get reward $r$ is $p(r|s,a)$.</li></ul></li><li><p>Policy: as state $s$, the probability to choose action $a$ is $\pi(a|s)$.</p></li><li><p>Markov property: one key property of MDPs is the <em>Markov property</em>, which refers to the <font color=blue>memoryless property</font> of a stochastic process, which means<br>$$<br>p(s_{t+1}|s_t,a_t,s_{t-1},a_{t-1},\dots,s_0,a_0)&#x3D;p(s_{t+1}|s_t,a_t)\<br>p(r_{t+1}|s_t,a_t,s_{t-1},a_{t-1},\dots,s_0,a_0)&#x3D;p(r_{t+1}|s_t,a_t)\<br>$$</p><hr><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://www.bilibili.com/video/BV1sd4y167NS/?spm_id_from=333.999.0.0">赵世钰老师的课程</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Reinforcement Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
