<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/12/24/hello-world/"/>
      <url>/2023/12/24/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Reinforcement Learning with Code [Chapter 1. Basic Concepts]</title>
      <link href="/2023/07/02/Reinforcement-Learning-with-Code-Chapter-1-Basic-Concepts/"/>
      <url>/2023/07/02/Reinforcement-Learning-with-Code-Chapter-1-Basic-Concepts/</url>
      
        <content type="html"><![CDATA[<h1 id="Reinforcement-Learning-with-Code"><a href="#Reinforcement-Learning-with-Code" class="headerlink" title="Reinforcement Learning with Code"></a>Reinforcement Learning with Code</h1><p>This note record how the author begin to learn RL. Both theoretical understanding and code practice are presented. Many material are referenced such as ZhaoShiyu’s <em>Mathematical Foundation of Reinforcement Learning</em> .</p><h2 id="Chapter-1-Basic-Concepts"><a href="#Chapter-1-Basic-Concepts" class="headerlink" title="Chapter 1. Basic Concepts"></a>Chapter 1. Basic Concepts</h2><h3 id="1-1-State-and-action"><a href="#1-1-State-and-action" class="headerlink" title="1.1 State and action"></a>1.1 State and action</h3><ul><li><font color=red><em>State</em> </font>describe the status of the agent with respect to the environment, denoted by $s$.</li><li><font color=red><em>State space</em> </font> is the set of all states, denoted by $\mathcal{S}&#x3D;{s_1, s_2,\dots,s_n}$.</li><li><font color = red><em>Action</em></font> describe the action that the agent may take with respect to the environment, denoted by $a$.</li><li><font color = red><em>Action space</em></font> is the set of all actions, denoted by $\mathcal{A}&#x3D;{a_1, a_2,\dots,a_n}.$</li></ul><h3 id="1-2-State-transition"><a href="#1-2-State-transition" class="headerlink" title="1.2 State transition"></a>1.2 State transition</h3><p>When taking an action, the agent may move from one state to another. Such a process is called <font color=red><em>state transition</em></font>. State trasition can be denoted by<br>$$<br>s_1 \stackrel{a_2}\longrightarrow s_2<br>$$<br>described by $p(s^\prime|s,a)$.</p><p>State trainsition can be both <em>deterministic</em> and <em>stochastic</em>. For example the deterministic state transition is<br>$$<br>p(s_1|s_1,a_2) &#x3D; 0 \<br>p(s_2|s_1,a_2) &#x3D; 1 \<br>p(s_3|s_1,a_2) &#x3D; 0<br>$$<br>For example the stochastic state transition is<br>$$<br>p(s_1|s_1,a_2) &#x3D; 0.5 \<br>p(s_2|s_1,a_2) &#x3D; 0.3 \<br>p(s_3|s_1,a_2) &#x3D; 0.2<br>$$</p><h3 id="1-3-Policy"><a href="#1-3-Policy" class="headerlink" title="1.3 Policy"></a>1.3 Policy</h3><ul><li><font color = red><em>Policy</em></font> tells the agents which actions to take <font color=blue>at each state</font>, denoted by $\pi$.</li><li>Policy is described by conditional probability.</li><li>Policy can be <em>deterministic</em> or <em>stochastic</em>, which means one state has a deterministic action or one state has probability to select other actions.</li></ul><p>Suppose the actions space is $\mathcal{A}&#x3D;{a_1, a_2,a_3}$, such  deterministic policy can be dentoed by<br>$$<br>\pi(a_1|s_1) &#x3D; 0 \<br>\pi(a_2|s_1) &#x3D; 1 \<br>\pi(a_3|s_1) &#x3D; 0<br>$$<br>which indicated the probability of taking action $a_2$ is $1$ and others are zero. </p><p>Such stochastic policy can be denoted by<br>$$<br>\pi(a_1|s_1) &#x3D; 0.5 \<br>\pi(a_2|s_1) &#x3D; 0.3 \<br>\pi(a_3|s_1) &#x3D; 0.2<br>$$</p><h3 id="1-4-Reward"><a href="#1-4-Reward" class="headerlink" title="1.4 Reward"></a>1.4 Reward</h3><ul><li><font color = red><em>Reward</em></font> is one of the most unique concept in RL.</li><li><font color = red><em>Immediate reward</em></font> can be obtained after taking an action.</li><li><font color = red><em>Reward transition</em></font> is the process of getting a reward after taking an action, reward transition can be <em>deterministic</em> or <em>stochastic</em>. Reward transition is described by $p(r|s,a)$</li></ul><p>For example deterministic reward transition can be denoted by<br>$$<br>p(r&#x3D;-1|s_1,a_2) &#x3D; 1, p(r\ne -1|s_1,a_2)&#x3D;0<br>$$<br>which means at state $s_1$ take action $a_2$ the probability to get immediate reward $-1$ is $1$.</p><p>Stochastic reward transition can be denoted by<br>$$<br>p(r&#x3D;1|s_1,a_2) &#x3D; 0.5, p(r&#x3D; 0|s_1,a_2)&#x3D;0.5<br>$$<br>which means at state $s_1$ take action $a_2$ the probability to get immediate reward $1$ is $0.5$, the probability to get immediate reward $0$ is $0.5$.</p><h3 id="1-5-Trajectory-return-episode"><a href="#1-5-Trajectory-return-episode" class="headerlink" title="1.5 Trajectory, return, episode"></a>1.5 Trajectory, return, episode</h3><ul><li><p><font color = red><em>Trajectory</em></font> is a state-action-reward chain, such as $s_1 \underset{r&#x3D;0}{\xrightarrow{a_2}} s_2 \underset{r&#x3D;0}{\xrightarrow{a_3}} s_3 \underset{r&#x3D;0}{\xrightarrow{a_4}} \cdots\underset{r&#x3D;1}{\xrightarrow{a_n}} s_{n}$.</p></li><li><p><font color = red><em>Return</em></font> of this trajecotry is the sum of all the rewards collected along the trajectory, such as $\text{return} &#x3D; 0+0+0+\cdots+1&#x3D;1$. Return is also called <font color=blue><em>total rewards</em></font> or <font color=blue><em>cumulative rewards</em></font>.</p></li><li><p><font color = red><em>Discounted return</em></font> is defined by the <font color=blue><em>discounted rate</em></font>, denoted by $\gamma\in(0,1)$. Such discounted retrun is<br>$$<br>\text{discounted return} &#x3D; 0+\gamma0+\gamma^2 0 + \gamma^3 0 + \cdots + \gamma^n 1<br>$$</p></li><li><p><font color = red><em>Episode</em></font> refers the trajectory that interacting with the enviornment following a policy <font color = blue>until the agent reach the terminal state</font>. An episode is usually assumed to be a finite trajectory,  that task with episodes are called <em>episodic tasks</em>. Some task may have no terminal state, such task is called <em>continuing tasks</em>.</p></li></ul><h3 id="1-6-Markov-decision-process-MDP"><a href="#1-6-Markov-decision-process-MDP" class="headerlink" title="1.6 Markov decision process (MDP)"></a>1.6 Markov decision process (MDP)</h3><p>Markov decision process is a general framework to <font color=blue>describe stochastic dynamical systems</font>. The key ingredients of an MDP are listed:</p><ul><li><p>Sets:</p><ul><li>State set: the set of all states, denoted as $\mathcal{S}$.</li><li>Actions set: a set of actions, denoted as $\mathcal{A}(s)$, is associated for each state $s\in\mathcal{S}$.</li><li>Reward set: a set of rewards, denoted as $\mathcal{R}(s,a)$, is associated for each state action pari $(s,a)$.</li></ul></li><li><p>Model:</p><ul><li>State transition probability: at state $s$, taking actions $a$, the probability to transit to state $s^\prime$ is $p(s^\prime|s,a)$.</li><li>Reward transition probability: at state $s$, taking action $a$, the probability to get reward $r$ is $p(r|s,a)$.</li></ul></li><li><p>Policy: as state $s$, the probability to choose action $a$ is $\pi(a|s)$.</p></li><li><p>Markov property: one key property of MDPs is the <em>Markov property</em>, which refers to the <font color=blue>memoryless property</font> of a stochastic process, which means<br>$$<br>p(s_{t+1}|s_t,a_t,s_{t-1},a_{t-1},\dots,s_0,a_0)&#x3D;p(s_{t+1}|s_t,a_t)\<br>p(r_{t+1}|s_t,a_t,s_{t-1},a_{t-1},\dots,s_0,a_0)&#x3D;p(r_{t+1}|s_t,a_t)\<br>$$</p><hr><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://www.bilibili.com/video/BV1sd4y167NS/?spm_id_from=333.999.0.0">赵世钰老师的课程</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Reinforcement Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
