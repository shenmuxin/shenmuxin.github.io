<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>hexo+github搭建个人博客 教程(三)客制化butterfly主题</title>
      <link href="/2023/12/28/hexo-github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2-%E6%95%99%E7%A8%8B-%E4%B8%89-%E5%AE%A2%E5%88%B6%E5%8C%96butterfly%E4%B8%BB%E9%A2%98/"/>
      <url>/2023/12/28/hexo-github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2-%E6%95%99%E7%A8%8B-%E4%B8%89-%E5%AE%A2%E5%88%B6%E5%8C%96butterfly%E4%B8%BB%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h1 id="Use-Hexo-Github-Build-Your-Blog-教程-三-客制化butterfly主题"><a href="#Use-Hexo-Github-Build-Your-Blog-教程-三-客制化butterfly主题" class="headerlink" title="Use Hexo + Github Build Your Blog 教程(三)客制化butterfly主题"></a>Use Hexo + Github Build Your Blog 教程(三)客制化butterfly主题</h1><h2 id="参考官方的配置教程"><a href="#参考官方的配置教程" class="headerlink" title="参考官方的配置教程"></a>参考官方的配置教程</h2><p>每个主题都有自己的配置方法，官方都有详细的说明，可以参考官方的说明文档。这里以博主使用的<code>butterfly</code>主题为例。<code>butterfly</code>的官方配置教程在这里<a href="https://butterfly.js.org/">https://butterfly.js.org/</a></p><p>我们的大部分修改要在主题文件的<code>_config.yml</code>和hexo目录中的<code>_config.yml</code>文件中进行。</p><h2 id="修改Hexo配置文件"><a href="#修改Hexo配置文件" class="headerlink" title="修改Hexo配置文件"></a>修改Hexo配置文件</h2><p>在<code>_config.yml</code>中找到字段<code>Site</code>，修改基本信息</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Site</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">网站的标题</span></span><br><span class="line"><span class="attr">subtitle:</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line"><span class="attr">keywords:</span></span><br><span class="line"><span class="attr">author:</span> <span class="string">网站作者名</span></span><br><span class="line"><span class="attr">language:</span> <span class="string">zh-CN</span> <span class="comment"># en # zh-TW</span></span><br><span class="line"><span class="attr">timezone:</span> <span class="string">&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="修改主题配置文件"><a href="#修改主题配置文件" class="headerlink" title="修改主题配置文件"></a>修改主题配置文件</h2><p>在<code>themes/butterfly/_config.yml</code>进行修改，具体的修改方式可以参考官方的说明文档<a href="https://butterfly.js.org/">https://butterfly.js.org/</a>。这里不再赘述了。</p><h2 id="为butterfly主题添加Gitalk评论功能"><a href="#为butterfly主题添加Gitalk评论功能" class="headerlink" title="为butterfly主题添加Gitalk评论功能"></a>为butterfly主题添加Gitalk评论功能</h2><p>使用gitalk评论系统，原因是因为它可以直接在github上管理评论，不需要在别的平台注册，并且有github的背书，所以安全性更高。</p><h3 id="注册OAuth-Application"><a href="#注册OAuth-Application" class="headerlink" title="注册OAuth Application"></a>注册OAuth Application</h3><p>点开Github头像，点击<code>Settings</code>，然后点击<code>Developer settings</code>，再点<code>OAuth Apps</code>，然后点击<code>New OAuth App</code>。</p><p>填写相关信息，注意<code>Homepage URL</code>和<code>Authorization callback URL</code>要填写博客的地址。</p><img src="https://pic.imgdb.cn/item/658ce904c458853aefb3205f.jpg" class="" title="注册OAuth Application"><img src="https://pic.imgdb.cn/item/658ce99dc458853aefb4ba76.jpg" class="" title="Client ID"><h3 id="配置Gitalk"><a href="#配置Gitalk" class="headerlink" title="配置Gitalk"></a>配置Gitalk</h3><p>在<code>themes/butterfly/_config.yml</code>中找到字段<code>comments</code>，修改为如下内容</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">comments:</span></span><br><span class="line">  <span class="comment"># Up to two comments system, the first will be shown as default</span></span><br><span class="line">  <span class="comment"># Choose: Disqus/Disqusjs/Livere/Gitalk/Valine/Waline/Utterances/Facebook Comments/Twikoo/Giscus/Remark42/Artalk</span></span><br><span class="line">  <span class="attr">use:</span>  <span class="string">Gitalk</span></span><br><span class="line">  <span class="attr">text:</span> <span class="literal">true</span> <span class="comment"># Display the comment name next to the button</span></span><br><span class="line">  <span class="comment"># lazyload: The comment system will be load when comment element enters the browser&#x27;s viewport.</span></span><br><span class="line">  <span class="comment"># If you set it to true, the comment count will be invalid</span></span><br><span class="line">  <span class="attr">lazyload:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">count:</span> <span class="literal">false</span> <span class="comment"># Display comment count in post&#x27;s top_img</span></span><br><span class="line">  <span class="attr">card_post_count:</span> <span class="literal">true</span> <span class="comment"># Display comment count in Home Page</span></span><br></pre></td></tr></table></figure><p>然后找到<code>gitalk</code>字段，修改如下</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># gitalk</span></span><br><span class="line"><span class="comment"># https://github.com/gitalk/gitalk</span></span><br><span class="line"><span class="attr">gitalk:</span></span><br><span class="line">  <span class="attr">client_id:</span> <span class="string">xxxxxxxxxxxx</span> <span class="string">your</span> <span class="string">client</span> <span class="string">id</span></span><br><span class="line">  <span class="attr">client_secret:</span> <span class="string">xxxxxxxxxxxxxxxxxxx</span> <span class="string">your</span> <span class="string">client</span> <span class="string">secret</span></span><br><span class="line">  <span class="attr">repo:</span> <span class="string">shenmuxin.github.io</span> <span class="comment"># 博客的仓库名称(注意不是地址)</span></span><br><span class="line">  <span class="attr">owner:</span> <span class="string">shenmuxin</span> <span class="comment"># github用户名</span></span><br><span class="line">  <span class="attr">admin:</span> <span class="string">shenmuxin</span> <span class="comment"># github用户名</span></span><br><span class="line">  <span class="attr">option:</span></span><br></pre></td></tr></table></figure><p>这样就成功配置了<code>Gitalk</code>，效果如下</p><img src="https://pic.imgdb.cn/item/658ceac7c458853aefb81024.jpg" class="" title="Gitalk效果"><h2 id="为butterfly配置加载动画preloader"><a href="#为butterfly配置加载动画preloader" class="headerlink" title="为butterfly配置加载动画preloader"></a>为butterfly配置加载动画preloader</h2><p>在<code>themes/butterfly/_config.yml</code>中找到字段<code>preloader</code>，修改为如下内容</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loading Animation (加載動畫)</span></span><br><span class="line"><span class="attr">preloader:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment"># source</span></span><br><span class="line">  <span class="comment"># 1. fullpage-loading</span></span><br><span class="line">  <span class="comment"># 2. pace (progress bar)</span></span><br><span class="line">  <span class="attr">source:</span> <span class="number">2</span></span><br><span class="line">  <span class="comment"># pace theme (see https://codebyzach.github.io/pace/)</span></span><br><span class="line">  <span class="attr">pace_css_url:</span></span><br></pre></td></tr></table></figure><h2 id="为butterfly配置打字副标题"><a href="#为butterfly配置打字副标题" class="headerlink" title="为butterfly配置打字副标题"></a>为butterfly配置打字副标题</h2><p>在<code>themes/butterfly/_config.yml</code>中找到字段<code>subtitle</code>，修改为如下内容</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># the subtitle on homepage (主頁subtitle)</span></span><br><span class="line"><span class="attr">subtitle:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment"># Typewriter Effect (打字效果)</span></span><br><span class="line">  <span class="attr">effect:</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment"># Customize typed.js (配置typed.js)</span></span><br><span class="line">  <span class="comment"># https://github.com/mattboldt/typed.js/#customization</span></span><br><span class="line">  <span class="attr">typed_option:</span></span><br><span class="line">  <span class="comment"># source 調用第三方服務</span></span><br><span class="line">  <span class="comment"># source: false 關閉調用</span></span><br><span class="line">  <span class="comment"># source: 1  調用一言網的一句話（簡體） https://hitokoto.cn/</span></span><br><span class="line">  <span class="comment"># source: 2  調用一句網（簡體） https://yijuzhan.com/</span></span><br><span class="line">  <span class="comment"># source: 3  調用今日詩詞（簡體） https://www.jinrishici.com/</span></span><br><span class="line">  <span class="comment"># subtitle 會先顯示 source , 再顯示 sub 的內容</span></span><br><span class="line">  <span class="attr">source:</span> <span class="literal">false</span></span><br><span class="line">  <span class="comment"># 如果關閉打字效果，subtitle 只會顯示 sub 的第一行文字</span></span><br><span class="line">  <span class="attr">sub:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">Your</span> <span class="string">slogan.</span></span><br></pre></td></tr></table></figure><h2 id="为butterfly配置local-search"><a href="#为butterfly配置local-search" class="headerlink" title="为butterfly配置local search"></a>为butterfly配置local search</h2><p>在<code>themes/butterfly/_config.yml</code>中找到字段<code>local_search</code>，修改为如下内容</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Local search</span></span><br><span class="line"><span class="attr">local_search:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment"># Preload the search data when the page loads.</span></span><br><span class="line">  <span class="attr">preload:</span> <span class="literal">false</span></span><br><span class="line">  <span class="comment"># Show top n results per article, show all results by setting to -1</span></span><br><span class="line">  <span class="attr">top_n_per_article:</span> <span class="number">-1</span></span><br><span class="line">  <span class="comment"># Unescape html strings to the readable one.</span></span><br><span class="line">  <span class="attr">unescape:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">CDN:</span></span><br></pre></td></tr></table></figure><p>然后前往博客的根目录，安装依赖</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-search --save</span><br></pre></td></tr></table></figure><h2 id="使用更好的渲染器"><a href="#使用更好的渲染器" class="headerlink" title="使用更好的渲染器"></a>使用更好的渲染器</h2><p>首先安装<code>Mathjax</code>插件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-math --save</span><br></pre></td></tr></table></figure><p>然后更改Hexo的Markdown渲染器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure><p>修改转义配置进入<code>node_modules\kramed\lig\rules\inline.js</code><br>找到<code>escape</code>字段，修改如下</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">escape</span>: <span class="regexp">/^\\([`*\[\]()#$+\-.!_&gt;])/</span>,</span><br></pre></td></tr></table></figure><p>找到<code>em</code>字段，修改如下</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">em</span>: <span class="regexp">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://www.cnblogs.com/qisi007/p/13731562.html">https://www.cnblogs.com/qisi007/p/13731562.html</a><br><a href="https://blog.misaka.rest/2023/02/01/hexo-gitalk/">https://blog.misaka.rest/2023/02/01/hexo-gitalk/</a><br><a href="https://butterfly.js.org/">https://butterfly.js.org/</a><br><a href="https://shanhainanhua.github.io/2019/09/18/hexo-next%E4%B8%BB%E9%A2%98-markdown%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E4%B8%8D%E6%AD%A3%E5%B8%B8%E6%98%BE%E7%A4%BA%E9%97%AE%E9%A2%98/">https://shanhainanhua.github.io/2019/09/18/hexo-next%E4%B8%BB%E9%A2%98-markdown%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E4%B8%8D%E6%AD%A3%E5%B8%B8%E6%98%BE%E7%A4%BA%E9%97%AE%E9%A2%98/</a></p>]]></content>
      
      
      <categories>
          
          <category> Blog </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Blog </tag>
            
            <tag> Hexo </tag>
            
            <tag> Github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo+github搭建个人博客 教程(二)配置github与自选主题</title>
      <link href="/2023/12/28/hexo-github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2-%E6%95%99%E7%A8%8B-%E4%BA%8C-%E9%85%8D%E7%BD%AEgithub%E4%B8%8E%E8%87%AA%E9%80%89%E4%B8%BB%E9%A2%98/"/>
      <url>/2023/12/28/hexo-github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2-%E6%95%99%E7%A8%8B-%E4%BA%8C-%E9%85%8D%E7%BD%AEgithub%E4%B8%8E%E8%87%AA%E9%80%89%E4%B8%BB%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h1 id="Use-Hexo-Github-Build-Your-Blog-教程-二-配置github与自选主题"><a href="#Use-Hexo-Github-Build-Your-Blog-教程-二-配置github与自选主题" class="headerlink" title="Use Hexo + Github Build Your Blog 教程(二)配置github与自选主题"></a>Use Hexo + Github Build Your Blog 教程(二)配置github与自选主题</h1><h2 id="配置github远端连接"><a href="#配置github远端连接" class="headerlink" title="配置github远端连接"></a>配置github远端连接</h2><p>我们需要在<code>Github</code>上新建一个仓库来托管我们的网页，需要注意<code>Github</code>只能使用一个同名的仓库名来托管一个静态的站点，所以我新建的仓库名字应该和我们的用户名一致，比如我的用户名是<code>shenmuxin</code>，那么我的仓库名应该为<code>shenmuxin.github.io</code>，具体可以参考下图</p><img src="https://pic.imgdb.cn/item/658cde5bc458853aef90518d.jpg" class="" title="创建github仓库"><p>然后我们需要配置SSH key，这样我们就可以通过SSH来连接<code>Github</code>了，具体操作如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name <span class="string">&quot;your name&quot;</span></span><br><span class="line">git config --global user.email <span class="string">&quot;your email&quot;</span></span><br><span class="line">ssh-keygen -t rsa -C <span class="string">&quot;your email(和上面的邮箱一致)&quot;</span></span><br></pre></td></tr></table></figure><p>按照提示完成三次回车，便可以生成SSH key，可以采用以下的指令来查看自己生成的SSH key</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> ~/.ssh/id_rsa.pub</span><br></pre></td></tr></table></figure><p>也可以手动打开<code>~/.ssh/id_rsa.pub</code>文件来查看，复制SSH key的全部内容</p><img src="https://pic.imgdb.cn/item/658cdf64c458853aef941ad2.jpg" class="" title="ssh key的路径"><p>然后将SSH key添加到<code>Github</code>上，在<code>Github</code>上找到<code>Settings</code>，然后找到<code>SSH and GPG keys</code>，点击<code>New SSH key</code>，将SSH key的内容粘贴到<code>Key</code>中，然后点击<code>Add SSH key</code></p><img src="https://pic.imgdb.cn/item/658cdfdcc458853aef956abe.jpg" class="" title="添加ssh key"><p>然后在<code>_config.yml</code>文件中进行配置，</p><img src="https://pic.imgdb.cn/item/658ce091c458853aef976745.jpg" class="" title="配置_config.yml"><p>将<code>deploy</code>的配置修改为如下内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  <span class="built_in">type</span>: git</span><br><span class="line">  repo: https://github.com/your name/your name.github.io.git</span><br><span class="line">  brach: master</span><br></pre></td></tr></table></figure><p>然后再安装一个部署从插件<code>hexo-deployer-git</code>,在博客文件中使用<code>Git Bash Here</code>打开终端，输入以下指令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><p>最后执行如下的两条命令就能够部署上传了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure><p>此时用浏览器打开<code>your name.github.io</code>就能访问你的网页了。</p><h2 id="配置好看的主题"><a href="#配置好看的主题" class="headerlink" title="配置好看的主题"></a>配置好看的主题</h2><p>Hexo官方收录了许多好看主题，可以在这里找到<a href="https://hexo.io/themes/index.html">https://hexo.io/themes/index.html</a>，笔者使用的主题是<code>butterfly</code>，<a href="https://github.com/jerryc127/hexo-theme-butterfly">https://github.com/jerryc127/hexo-theme-butterfly</a>。<br>直接下载主题文件夹，然后放置在<code>themes</code>文件夹下，在<code>themes/butterfly/_config.yml</code>文件中进行配置，选择当前使用的主题名字</p><img src="https://pic.imgdb.cn/item/658ce2fec458853aef9ebfa0.jpg" class="" title="配置主题"><p>然后我们可以进行渲染</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo s</span><br></pre></td></tr></table></figure><p>查看当前的主题效果。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://butterfly.js.org/">https://butterfly.js.org/</a><br><a href="https://www.bilibili.com/video/BV1Eg41157tL/?spm_id_from=333.337.search-card.all.click&vd_source=d02fb26eb2345ac42c054db0bb8d8864">https://www.bilibili.com/video/BV1Eg41157tL/?spm_id_from&#x3D;333.337.search-card.all.click&amp;vd_source&#x3D;d02fb26eb2345ac42c054db0bb8d8864</a></p>]]></content>
      
      
      <categories>
          
          <category> Blog </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Blog </tag>
            
            <tag> Hexo </tag>
            
            <tag> Github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo+github搭建个人博客 教程(一)搭建基本环境与基本操作</title>
      <link href="/2023/12/28/hexo-github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2-%E6%95%99%E7%A8%8B-%E4%B8%80-%E6%90%AD%E5%BB%BA%E5%9F%BA%E6%9C%AC%E7%8E%AF%E5%A2%83%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
      <url>/2023/12/28/hexo-github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2-%E6%95%99%E7%A8%8B-%E4%B8%80-%E6%90%AD%E5%BB%BA%E5%9F%BA%E6%9C%AC%E7%8E%AF%E5%A2%83%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="Use-Hexo-Github-Build-Your-Blog-教程-一-搭建基本环境与基本操作"><a href="#Use-Hexo-Github-Build-Your-Blog-教程-一-搭建基本环境与基本操作" class="headerlink" title="Use Hexo + Github Build Your Blog 教程(一)搭建基本环境与基本操作"></a>Use Hexo + Github Build Your Blog 教程(一)搭建基本环境与基本操作</h1><h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><p>首先笔者使用的操作系统环境为<code>windows11</code>,以下默认的安装环境为<code>windows11</code>。</p><h3 id="安装Git"><a href="#安装Git" class="headerlink" title="安装Git"></a>安装Git</h3><p>首先下载Git，<a href="https://git-scm.com/download/win">https://git-scm.com/download/win</a><br>下载完成后直接安装即可。然后在git中进行基本配置，首先绑定自己的<code>Github</code>账号，鼠标右键点击<code>Git Bash Here</code>打开Git的终端，输入以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name <span class="string">&quot;your name&quot;</span></span><br><span class="line">git config --global user.email <span class="string">&quot;your email&quot;</span></span><br></pre></td></tr></table></figure><p>将相应位置替换为你自己的<code>Github</code>账号信息。配置完成之后可以使用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --list</span><br></pre></td></tr></table></figure><p>查看一下配置是否正确。</p><h3 id="安装Node-js"><a href="#安装Node-js" class="headerlink" title="安装Node.js"></a>安装Node.js</h3><p>然后，下载Nodejs，<a href="https://nodejs.org/en">https://nodejs.org/en</a><br>下载完成后直接安装即可。安装完成之后，查看安装是否正确，同样用<code>Git Bash Here</code>打开终端，然后在终端中输入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git version</span><br><span class="line">node -v</span><br><span class="line">npm -v</span><br></pre></td></tr></table></figure><p>如果出现了相应的版本号，则说明安装成功。</p><h3 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h3><p>安装Hexo，在终端中输入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-cli -g</span><br></pre></td></tr></table></figure><p>等待安装完成后，查看版本号</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo -v</span><br></pre></td></tr></table></figure><p>如果出现了相应的版本号，则说明安装成功。</p><h2 id="初始化Blog文件"><a href="#初始化Blog文件" class="headerlink" title="初始化Blog文件"></a>初始化Blog文件</h2><ul><li>使用Hexo初始化一个博客文件夹</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo init YourBlogFileName</span><br><span class="line"><span class="built_in">cd</span> YourBlogFileName</span><br><span class="line">npm install</span><br></pre></td></tr></table></figure><p>等待安装完成，我们可以直接进行渲染，看安装是否正确</p><ul><li>开启Hexo服务</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo server <span class="comment">#或者缩写成 hexo s</span></span><br></pre></td></tr></table></figure><p>如果过程无误，可以打开本地的端口<code>http://localhost:4000/</code>查看网页的效果，那么显示的结果应该如下所示</p><img src="https://pic.imgdb.cn/item/658cdac6c458853aef83433f.png" class="" width="1000" height="600" title="渲染结果"><h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><ul><li>使用Hexo初始化blog</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo init YourBlogFileName</span><br><span class="line"><span class="built_in">cd</span> YourBlogFileName</span><br><span class="line">npm install</span><br></pre></td></tr></table></figure><ul><li>新建一个page</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page <span class="string">&quot;YourPageName&quot;</span></span><br></pre></td></tr></table></figure><ul><li>新建一个post</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new post <span class="string">&quot;YourPostName&quot;</span></span><br></pre></td></tr></table></figure><ul><li>生成静态文件（可以理解为编译）</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo generate <span class="comment"># 缩写为 hexo g</span></span><br></pre></td></tr></table></figure><ul><li>渲染（开启服务）</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo server <span class="comment"># 缩写为 hexo s</span></span><br></pre></td></tr></table></figure><ul><li>推送到远端</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo deploy <span class="comment"># 缩写为 hexo d</span></span><br></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://www.bilibili.com/video/BV1Eg41157tL/?spm_id_from=333.337.search-card.all.click&vd_source=d02fb26eb2345ac42c054db0bb8d8864">https://www.bilibili.com/video/BV1Eg41157tL/?spm_id_from&#x3D;333.337.search-card.all.click&amp;vd_source&#x3D;d02fb26eb2345ac42c054db0bb8d8864</a></p>]]></content>
      
      
      <categories>
          
          <category> Blog </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Blog </tag>
            
            <tag> Hexo </tag>
            
            <tag> Github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/12/24/hello-world/"/>
      <url>/2023/12/24/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Reinforcement Learning with Code [Chapter 5. Monte Carlo Learning]</title>
      <link href="/2023/07/24/Reinforcement-Learning-with-Code-Chapter-5-Monte-Carlo-Learning/"/>
      <url>/2023/07/24/Reinforcement-Learning-with-Code-Chapter-5-Monte-Carlo-Learning/</url>
      
        <content type="html"><![CDATA[<h1 id="Reinforcement-Learning-with-Code"><a href="#Reinforcement-Learning-with-Code" class="headerlink" title="Reinforcement Learning with Code"></a>Reinforcement Learning with Code</h1><p>This note records how the author begin to learn RL. Both theoretical understanding and code practice are presented. Many material are referenced such as ZhaoShiyu’s <em>Mathematical Foundation of Reinforcement Learning</em>, .</p><h2 id="Chapter-5-Monte-Carlo-Learning"><a href="#Chapter-5-Monte-Carlo-Learning" class="headerlink" title="Chapter 5. Monte Carlo Learning"></a>Chapter 5. Monte Carlo Learning</h2><p>​What is Monte Carlo estimation? Monte Carlo estimation refers to a broad class of techniques that use stochastic samples to solve approximation problems using the <font color=blue>Law of Large Numbers</font>. </p><p>​ChatGpt tells us that “Monte Carlo estimation is a statistical method used to estimate unknown quantities or solve problems by generating random samples and using the law of large numbers to approximate the true value.”</p><h3 id="5-1-Law-of-large-numbers"><a href="#5-1-Law-of-large-numbers" class="headerlink" title="5.1 Law of large numbers"></a>5.1 Law of large numbers</h3><p>​(Law of Large Numbers) <em>For a random variable</em> $X$. <em>Suppose</em> ${x_i}<em>{i&#x3D;1}^n$ <em>are some independent and indentically distribution (iid) samples</em>. <em>Let</em> $\bar{x}&#x3D;\frac{1}{n}\sum</em>{i&#x3D;1}^n x_i$ <em>be the average of the samples. Then</em>,<br>$$<br>\mathbb{E}[\bar{x}] &#x3D; \mathbb{E}[X]\<br>\mathrm{var}[\bar{x}] &#x3D; \frac{1}{n} \mathrm{var}[X]<br>$$<br>The above two equations indicate that $\bar{x}$ is an <em>unbiased estimate</em> of $\mathbb{E}[X]$ and its variance decreases to zero as $n$ increases to infinity.</p><p>​<strong>Proof</strong>, First, $\mathbb{E}[\bar{x}] &#x3D; \mathbb{E}[\frac{1}{n}\sum_{i&#x3D;1}^n x_i]&#x3D;\frac{1}{n}\sum_{i&#x3D;1}^n\mathbb{E}[x_i]&#x3D;\frac{1}{n}n\mathbb{E}[X]&#x3D;\mathbb{E}[X]$, where the last equability is because the samples are <em>independent and indentically distribution (iid)</em> (that is, $\mathbb{E}[x_i]&#x3D;\mathbb{E}[X]$).</p><p>​Second, $\mathrm{var}[\bar{x}]&#x3D;\mathrm{var}[\frac{1}{n}\sum_{i&#x3D;1}^n x_i]&#x3D;\frac{1}{n^2}\sum_{i&#x3D;1}^n\mathrm{var}[x_i]&#x3D;\frac{1}{n^2}n*\mathrm{var}[X]&#x3D;\frac{1}{n}\mathrm{var}[X]$, where the second equality is because the samples are <em>independent and indentically distribution (iid)</em> (that is, $\mathrm{var}[x_i]&#x3D;\mathrm{var}[X]$)</p><h3 id="5-2-Simple-example"><a href="#5-2-Simple-example" class="headerlink" title="5.2 Simple example"></a>5.2 Simple example</h3><p>​Consider a problem that we need to calculate the expectation $\mathbb{E}[X]$ of random variable $X$ which takes value in the finite set $\mathcal{X}$. There are two ways. </p><p>​First, <em>model-based</em> approach, we can use the definition of expectation that<br>$$<br>\mathbb{E}[X] &#x3D; \sum_{x\in\mathcal{X}} p(x)x<br>$$<br>​Second, <em>model-free</em> approach, if the probability of distribution is unknown we can use the <em>Law of Large Numbers</em> to estimate the expectation.<br>$$<br>\mathbb{E}[X]\approx \bar{x} &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^n x_i<br>$$<br>The example gives us an intutition that: When the system model is available, the expectation can be calculated based on the model.</p><p>When the model is unavailable, the expectation can be estimated approximately using stochastic samples.</p><h3 id="5-3-Monte-Carlo-Basic"><a href="#5-3-Monte-Carlo-Basic" class="headerlink" title="5.3 Monte Carlo Basic"></a>5.3 Monte Carlo Basic</h3><p><strong>Review policy iteration</strong>:</p><p>​The simplest Monte Carlo learning also called Monte Carlo Basic (MC Basic) just replaces the policy iteration model-based part by MC estimation. Recall the <font color=blue>matrix-vector form of policy iteration</font></p><p>$$<br>\begin{aligned}<br>    v_{\pi_k}^{(j+1)} &amp; &#x3D; r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}^{(j)}\quad \text{(policy evaluation)} \<br>    \pi_{k+1} &amp; &#x3D; \arg \max_{\pi_k}(r_{\pi_k} + \gamma P_{\pi_k}v_{\pi_k})\quad \text{(policy improvement)}<br>\end{aligned}<br>$$</p><p>The <font color=blue>elementwise form of policy iteration</font> is</p><p>$$<br>\begin{aligned}<br>    v_{\pi_k}^{(j+1)}(s) &amp; &#x3D; \sum_a \pi_k(a|s) \Big(\sum_r p(r|s,a)r + \sum_{s^\prime} p(s^\prime|s,a) v_{\pi_k}^{(j)} (s^\prime)\Big)\quad \text{(policy evaluation)}\<br>    \pi_{k+1}(s) &amp; &#x3D; \arg \max_{\pi_k}  \sum_a \pi(a|s)<br>    \underbrace{<br>    \Big(\sum_r p(r|s,a)r + \sum_{s^\prime} p(s^\prime|s,a) v_{\pi_k} (s^\prime)\Big)}<em>{q</em>{\pi_k}(s,a)}<br>    \quad \text{(policy improvement)}\<br>    \to \pi_{k+1}(s) &amp; &#x3D;  \sum_a \pi(a|s) \arg \max_a q_{\pi_k}(s,a)<br>\end{aligned}<br>$$</p><p>Its obvious that the core is to calculate the action values by usting<br>$$<br>q_{\pi_k}(s,a) &#x3D; \sum_r p(r|s,a)r + \sum_{s^\prime}p(s^\prime|s,a)v_{\pi_k}(s^\prime)<br>$$<br>​However, the above equation needs the model of the environment (that  $p(r|s,a)$ and $p(s^\prime|s,a)$ are required). We can replace this part by Monte Carlo estimation. Recall the definition of action value refer to section 2.5.<br>$$<br>q_\pi(s,a) \triangleq \mathbb{E}[G_t|S_t&#x3D;s, A_t&#x3D;a]<br>$$<br>Action value can be interpreted as <font color=blue>average return</font> along trajectory generated by <font color=blue>policy $\pi$ </font> after taking a <font color=blue>specific action $a$</font>.</p><p><strong>Convert to model free</strong>:</p><p>​Suppose there are $n$ episodes sampling starting at state $s$ and takeing action $a$,  and then denote the $i$ th episode return as $\textcolor{blue}{g^{(i)}(s,a)}$. Then using the idea of Monte Carlo estimation and Law of Large Numbers, the action values can be approxiamte as<br>$$<br>q_\pi(s,a) \triangleq \mathbb{E}[G_t|S_t&#x3D;s, A_t&#x3D;a] \textcolor{blue}{\approx \frac{1}{n} \sum_{j&#x3D;1}^n g^{(j)}(s,a)}<br>$$<br>Suppose the number of episodes $n$ is sufficiently large. So, the above equation is the unbiased estimation of action value $q_\pi(s,a)$.</p><p><strong>Pesudocode</strong>:</p><img src="https://pic.imgdb.cn/item/658bfa94c458853aef6a9405.png" class="" width="800" height="400" title="state-transform"><p>​Here are some questions.  <font color=red>Why does the MC Basic algorithm estimate action values instead of state values?</font> That is because state value cannot be used to improve policies directly. Even if we are given state values, we still need to calculate action values from these state value using $q_{\pi_k}(s,a) &#x3D; \sum_r p(r|s,a)r + \sum_{s^\prime}p(s^\prime|s,a)v_{\pi_k}(s^\prime)$. But this calculation requires the system model. Therefore, when models are not available, we should directly estimate action values.</p><h3 id="5-4-Monte-Carlo-Exploring-Starts"><a href="#5-4-Monte-Carlo-Exploring-Starts" class="headerlink" title="5.4 Monte Carlo Exploring Starts"></a>5.4 Monte Carlo Exploring Starts</h3><p><strong>Some concepts</strong>:</p><ul><li><p><font color=blue>Vist</font>: a state-action pair appears in the episode, it is called a <em>vist</em> of the state-action pair. Such as<br>$$<br>\textcolor{blue}{s_1 \xrightarrow{a_2}} s_2 \xrightarrow{a_4} \textcolor{blue}{s_1 \xrightarrow{a_2}} s_2 \xrightarrow{a_3} s_5 \xrightarrow{a_13}\cdots<br>$$<br>$(s_1,a_2)$ is called a visit.</p></li><li><p><font color=blue>First visit</font>: In the above trajectory $(s_1,a_2)$ is visited twice. If we only count the first-time visit, such kind of strategy is called <em>first-visit</em>.</p></li><li><p><font color=blue>Every visit</font>: If every time state-action pair is visited and the rest of the episode is used to estimate its action value, such a strategy is called <em>every-visit</em>.</p></li></ul><p>​For example,</p><p>$$<br>\begin{aligned}<br>    s_1 \xrightarrow{a_2} s_2 \xrightarrow{a_4} s_1 \xrightarrow{a_2} s_2 \xrightarrow{a_3} s_5 \xrightarrow{a_13}\cdots &amp; \quad [\textcolor{blue}{\text{origianl episode}}]\<br>    s_2 \xrightarrow{a_4} s_1 \xrightarrow{a_2} s_2 \xrightarrow{a_3} s_5 \xrightarrow{a_13}\cdots &amp; \quad [\text{episode starting from }(s_2,a_4)]\<br>    s_1 \xrightarrow{a_2} s_2 \xrightarrow{a_3} s_5 \xrightarrow{a_13}\cdots &amp; \quad [\textcolor{red}{\text{episode starting from }(s_1,a_2)}]\<br>    s_2 \xrightarrow{a_3} s_5 \xrightarrow{a_13}\cdots &amp; \quad [\text{episode starting from }(s_2,a_3)]\<br>    s_5 \xrightarrow{a_13}\cdots &amp; \quad [\text{episode starting from }(s_2,a_4)]<br>\end{aligned}<br>$$</p><p>Suppose we need to estimate the action value $q_\pi(s_1,a_2)$. The <em>first-visit</em> only counts the first visit of $(s_1,a_2)$. So we need a huge numbers of episodes starting from $(s_1,a_2)$. As the blue one episode in the above equations. The <em>every-visi</em>t counts every time the visit of $(s_1,a_2)$. Hence, we can use the blue one and the red one to estimate the action value $q_\pi(s_1,a_2)$. In this way, the samples in the episode are utilized more sufficiently.</p><p><strong>Using sample more efficiently</strong>:</p><p>​From the example, we are informed that <font color=blue>if an episode is sufficiently long so that it can visit all the state-action pairs many times, then this single episode is sufficient to estimate all the action values by using the every-visit strategy</font>. However, one single episode is pretty ideal result. Because the samples obtained by the <em>every-visit</em> is relevant due to the trajectory starting from the second visit is merely a subset of the trajectory starting from the first. Nevertheless, if the two visits are far away from each other, which means there is a siginificant non-overlap portion, the relevance would not be strong. Moreover, the relevance can be further suppressed due to the discount rate. <font color=blue>Therefore, when there are few episodes and each episode is very long, the every-visit strategy is a good option</font>.</p><p><strong>Using estimation more efficiently</strong>:</p><p>​The first startegy in MC Basic, in the policy evaluation step, to collect all the episodes starting from the same state-action pair and then approximate the action value using the average return of these episodes. The drawback of the strategy is that the agent must wait until all episodes have been collected.</p><p>​The second strategy, which can overcome the drawback, is to <font color=blue>use the return of a single episode to approximate the corresponding action value</font> (the idea of stochastic estimation). In this way, we can improve the policy in an <font color=blue>episode-by-episode</font> fasion.</p><p><strong>Pesudocode</strong>:</p><img src="https://pic.imgdb.cn/item/658bfac5c458853aef6b25fb.png" class="" width="800" height="500" title="state-transform"><p><font color=blue>MC Exploring Starts algorithm compared to MC Basic, the sample usage and estimation update are more efficient</font>.</p><h3 id="5-5-Monte-Carlo-epsilon-Greedy"><a href="#5-5-Monte-Carlo-epsilon-Greedy" class="headerlink" title="5.5 Monte Carlo $\epsilon$-Greedy"></a>5.5 Monte Carlo $\epsilon$-Greedy</h3><p>​Why is exploring starts important? In theory, exploring starts is necessary to find optimal policies. Only if every state-action pair is well explored, can we select optimal policies. However, in practice, exploring starts is difficult to achieve. Because its difficult to collect episodes starting from every state-action pair.</p><p>​We can use <em>soft policy</em> to remove the requirement of exploring starts. There are many soft policies. The most common one is $\textcolor{blue}{\epsilon}$<font color=blue>-greedy</font>. The $\epsilon$-greedy has the form of<br>$$<br>\pi(a|s) &#x3D;<br>\left {<br>    \begin{aligned}<br>    1 - \frac{\epsilon}{|\mathcal{A}(s)|}(|\mathcal{A(s)}|-1), &amp; \quad \text{for the geedy action}\<br>    \frac{\epsilon}{|\mathcal{A}(s)|}, &amp; \quad \text{for the other } |\mathcal{A}(s)|-1 \text{ actions}<br>    \end{aligned}<br>\right.<br>$$<br>where $|\mathcal{A}(s) |$ denotes the number of actions associated with $s$. It is worth noting taht the probability to take the greedy action is always greater than other action, because<br>$$<br>1 - \frac{\epsilon}{|\mathcal{A}(s)|}(|\mathcal{A(s)}|-1) &#x3D;<br>1 - \epsilon + \frac{\epsilon}{|\mathcal{A}(s)|} \ge<br>\frac{\epsilon}{|\mathcal{A}(s)|}<br>$$<br>for any $\epsilon\in[0,1]$, when $\epsilon&#x3D;0$ the $\epsilon$-greedy becomes greedy policy.</p><p><strong>Pesudocode</strong>:</p><img src="https://pic.imgdb.cn/item/658bfae1c458853aef6b7740.png" class="" width="800" height="500" title="state-transform"><hr><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>  <a href="https://www.bilibili.com/video/BV1sd4y167NS/?spm_id_from=333.999.0.0">赵世钰老师的课程</a></p>]]></content>
      
      
      <categories>
          
          <category> Reinforcement Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reinforcement Learning with Code [Chapter 4. Value Iteration and Policy Iteration]</title>
      <link href="/2023/07/23/Reinforcement-Learning-with-Code-Chapter-4-Value-Iteration-and-Policy-Iteration/"/>
      <url>/2023/07/23/Reinforcement-Learning-with-Code-Chapter-4-Value-Iteration-and-Policy-Iteration/</url>
      
        <content type="html"><![CDATA[<h1 id="Reinforcement-Learning-with-Code"><a href="#Reinforcement-Learning-with-Code" class="headerlink" title="Reinforcement Learning with Code"></a>Reinforcement Learning with Code</h1><p>This note records how the author begin to learn RL. Both theoretical understanding and code practice are presented. Many material are referenced such as ZhaoShiyu’s <em>Mathematical Foundation of Reinforcement Learning</em>, .</p><h2 id="Chapter-4-Value-Iteration-and-Policy-Iteration"><a href="#Chapter-4-Value-Iteration-and-Policy-Iteration" class="headerlink" title="Chapter 4. Value Iteration and Policy Iteration"></a>Chapter 4. Value Iteration and Policy Iteration</h2><p>​Value iteration and policy iteration have a common name called <font color=blue>dynamic programming</font>. Dynamic programming is model-based algorithm, which is the simplest RL algorithm. Its helpful to us to understand the model-free algorithm.</p><h3 id="4-1-Value-iteration"><a href="#4-1-Value-iteration" class="headerlink" title="4.1 Value iteration"></a>4.1 Value iteration</h3><p>​Value iteration is solving the Bellman optimal equation directly.</p><p><strong>Matrix-vector form</strong>:</p><p>​The <font color=red>value iteration</font> is exactly the algorithm suggested by the contraction mapping in chapter 3. Value iteration concludes two parts. First, in every iteration is called <em>policy update</em>. </p><p>$$<br>\pi_{k+1} &#x3D; \arg \textcolor{red}{\max_{\pi_k}(r_{\pi_k} + \gamma P_\pi v_k)}<br>$$<br>Second, the step is <em>value update</em>. Mathematically, it is to substitute $\pi_{k+1}$ and do the following operation:</p><p>$$<br>v_{k+1} &#x3D; r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}}v_k<br>$$</p><p>The above algorithm is matrix-vector form which is useful to understand the core idea. <font color=red>The above equation is iterative which means we calculate</font> $v_{k+1}$ <font color=red>is only one step calculation</font>.</p><p>There is a <font color=red>misunderstanding</font> that we doesn’t use Bellman equation to calculate state value $v_{k+1}$ directly. Instead, when we use greedy policy update strategy the state value $v_{k+1}$ is actually the maximum action value $\max_a q_k(s,a)$.</p><p>The value iteration includes solving the Bellman optimal equation part as show in red.</p><p><strong>Elementwise form</strong>:</p><p>​First, the elementwise form <em>policy update</em> is </p><p>$$<br>\begin{aligned}<br>\pi_{k+1} &amp; &#x3D; \arg \max_{\pi_k}(r_{\pi_k} + \gamma P_\pi v_k)\quad \text{(matrx-vector form)}\newline<br>\to \pi_{k+1}(s) &amp; &#x3D; \arg \max_{\pi_k} \sum_a \pi_k(a|s)<br>\Big(\sum_r p(r|s,a)r + \sum_{s^\prime} p(s^\prime|s,a) v_k(s^\prime)\Big) \quad \text{(elementwise  form)}\newline<br>\to \pi_{k+1}(s) &amp; &#x3D; \arg \max_{\pi_k} \sum_a \pi_k(a|s) q_{\pi_k}(s,a)\newline<br>\to \pi_{k+1}(s) &amp; &#x3D;  \sum_a \pi_k(a|s) \arg \max_{a_k}q_{\pi_k}(s,a)<br>\end{aligned}<br>$$</p><p>Then use the greedy policy update algorithm, when $a&#x3D;a_k^*(s)$, $\pi_{k+1}(a|s) &#x3D; 1$, when $a\ne a_k^*(s)$, $\pi_{k+1}(a|s) &#x3D; 0$, where $a_k^*(s) &#x3D; \arg\max_a q_k(a,s)$.</p><p>​Second, the elementwise form <em>value update</em> is </p><p>$$<br>\begin{aligned}<br>    v_{k+1} &amp; &#x3D; r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}}v_k\quad \text{(matrx-vector form)}\newline<br>    \to v_{k+1}(s) &amp; &#x3D; \sum_a \pi_{k+1}(a|s)<br>    \underbrace{<br>    \Big(\sum_r p(r|s,a)r+ \gamma \sum_{s^\prime}p(s^\prime|s,a)v_k(s^\prime) \Big)}_{q_k(s,a)}\quad \text{(elementwise  form)}<br>\end{aligned}<br>$$</p><p><font color=blue>Since $\pi_{k+1}$ is a greedy policy, the above equation is simply</font></p><p>$$<br>v_{k+1}(s) &#x3D; \max_a q_k(s,a)<br>$$</p><p><strong>Pesudocode</strong>:</p><img src="https://pic.imgdb.cn/item/658be0b6c458853aef1b95ef.png" class="" width="800" height="400" title="state-transform"><h3 id="4-2-Policy-iteration"><a href="#4-2-Policy-iteration" class="headerlink" title="4.2 Policy iteration"></a>4.2 Policy iteration</h3><p>​The <font color=red>policy iteration</font> is not an algorithm directly solving the Bellman optimality equation. However, it has an intimate relationship to value iteration. The policy iteration includes two parts <font color=blue>policy evaluation</font> and <font color=blue>policy improvement</font>.</p><p><strong>Policy evaluation</strong>:</p><p>​The first step is policy evaluation. Mathematically, it is to sovle Bellman equation of $\pi_k$:</p><p>$$<br>v_{\pi_k} &#x3D; r_{\pi_k} + \gamma P_{\pi_k}v_{\pi_k}<br>$$</p><p>This is the matrix-vector form of the Bellman equation, where $r_{\pi_k}$ and $P_{\pi_k}$ are known. Here, $v_{\pi_k}$ is the state value to be solved.</p><p><font color=blue>How to calculate state value</font> $v_{\pi_k}$ is important. In section 2.4 we have already introduced how to solve Bellman equation to get state value $v_{\pi_k}$ as following</p><p>$$<br>\textcolor{blue}{v_{\pi_k}^{(j+1)} &#x3D; r_{\pi_k} + \gamma P_{\pi_k}v_{\pi_k}^{(j)}}, \quad j&#x3D;0,1,2,\dots<br>$$</p><p><font color=red>Its clear that policy iteration is directly calculate the state value using the above equation, which means the calculation is infity step calculation</font>.</p><p><strong>Policy improvement</strong>:</p><p>​The second step is to imporve the policy. How to do that? Once $v_{\pi_k}$ is calculated in the first step, a new and improved policy could be obtained as </p><p>$$<br>\pi_{k+1} &#x3D; \arg \max_\pi (r_\pi + \gamma P_\pi v_{\pi_k})<br>$$</p><p> <strong>Elementwise form</strong>:</p><p>​The policy evaluation step is to solve $v_{\pi_k}$ from the Bellman equation dirctly.</p><p>$$<br>\begin{aligned}<br>    v_{\pi_k}^{(j+1)} &amp; &#x3D; r_{\pi_k} + \gamma P_{\pi_k} v_{\pi_k}^{(j)}\quad \text{(matrix-vector form)} \newline<br>    \to v_{\pi_k}^{(j+1)}(s) &amp; &#x3D; \sum_a \pi_k(a|s) \Big(\sum_r p(r|s,a)r + \sum_{s^\prime} p(s^\prime|s,a) v_{\pi_k}^{(j)} (s^\prime)\Big) \quad \text{(elementwise form)}<br>\end{aligned}<br>$$<br>where $j&#x3D;0,1,2,\dots$</p><p>The policy improvement step is to solve $\pi_{k+1}&#x3D;\arg \max_\pi (r_\pi + \gamma P_{\pi_k}v_{\pi_k})$.</p><p>$$<br>\begin{aligned}<br>\pi_{k+1} &amp; &#x3D;\arg \max_{\pi_k} (r_{\pi_k} + \gamma P_{\pi_k}v_{\pi_k})\quad \text{(matrix-vector form)}\newline<br>\to \pi_{k+1}(s) &amp; &#x3D; \arg \max_{\pi_k}  \sum_a \pi(a|s)<br>\Big(\sum_r p(r|s,a)r + \sum_{s^\prime} p(s^\prime|s,a) v_{\pi_k} (s^\prime)\Big)<br>\quad \text{(elementwise form)}\newline<br>\to \pi_{k+1}(s) &amp; &#x3D;  \sum_a \pi(a|s) \arg \max_a q_{\pi_k}(s,a)<br>\end{aligned}<br>$$</p><p>Let $a^*<em>k(s) &#x3D; \text{arg} \max_a q</em>{\pi_k}(s,a)$. The greedy optimal policy is when $a&#x3D;a_k^*(s)$, $\pi_{k+1}(a|s) &#x3D; 1$, when $a\ne a_k^*(s)$, $\pi_{k+1}(a|s) &#x3D; 0$, where $a_k^*(s) &#x3D; \arg\max_a q_k(a,s)$.</p><p><strong>Pesudocode:</strong></p><img src="https://pic.imgdb.cn/item/658be0dfc458853aef1c2798.png" class="" width="800" height="400" title="state-transform"><hr><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>  <a href="https://www.bilibili.com/video/BV1sd4y167NS/?spm_id_from=333.999.0.0">赵世钰老师的课程</a></p>]]></content>
      
      
      <categories>
          
          <category> Reinforcement Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reinforcement Learning with Code [Chapter 3. Optimal State Value and Bellman Optimal Equation]</title>
      <link href="/2023/07/23/Reinforcement-Learning-with-Code-Chapter-3-Optimal-State-Value-and-Bellman-Optimal-Equation/"/>
      <url>/2023/07/23/Reinforcement-Learning-with-Code-Chapter-3-Optimal-State-Value-and-Bellman-Optimal-Equation/</url>
      
        <content type="html"><![CDATA[<h1 id="Reinforcement-Learning-with-Code"><a href="#Reinforcement-Learning-with-Code" class="headerlink" title="Reinforcement Learning with Code."></a>Reinforcement Learning with Code.</h1><p>This note records how the author begin to learn RL. Both theoretical understanding and code practice are presented. Many material are referenced such as ZhaoShiyu’s <em>Mathematical Foundation of Reinforcement Learning</em>, .</p><h2 id="Chapter-3-Optimal-State-Value-and-Bellman-Optimality-Equation"><a href="#Chapter-3-Optimal-State-Value-and-Bellman-Optimality-Equation" class="headerlink" title="Chapter 3. Optimal State Value and Bellman Optimality Equation"></a>Chapter 3. Optimal State Value and Bellman Optimality Equation</h2><h3 id="3-1-How-to-define-optimal"><a href="#3-1-How-to-define-optimal" class="headerlink" title="3.1 How to define optimal"></a>3.1 How to define optimal</h3><p>​One core idea is that we use the action value to judge the optimality of the action. If we update the policy to select the action with the <em>greatest action value</em>, we could find a better policy.</p><p>​(<font color=blue>Optimal policy and optimal state value</font>). <em>A policy</em> $\pi^*$ <em>is optimal if</em> $v_{\pi^*}(s) \ge v_\pi(s)$ <em>for all</em> $s\in\mathcal{S}$ <em>and for any other policy</em> $\pi$. <em>The state values of</em>  $\pi^*$ <em>are the optimal state values</em>.</p><p><font color=blue>Why does the definition works? One intuitive explanation is that state value $v_\pi(s)$ denote the mean of return along the trajectory following policy $\pi$. If the policy $\pi^*$ has greatest expectation of return, hence we can believe that the policy $\pi^*$ is optimal.</font></p><h3 id="3-2-Bellman-optimal-equation-BOE"><a href="#3-2-Bellman-optimal-equation-BOE" class="headerlink" title="3.2 Bellman optimal equation (BOE)"></a>3.2 Bellman optimal equation (BOE)</h3><p>​The Bellamn optimal eqaution (BOE) is </p><p>$$<br>\begin{aligned}<br>v(s) &amp; &#x3D; \max_\pi \sum_a \pi(a|s) \Big[ \sum_r p(r|s,a)r + \gamma \sum_{s^\prime} p(s^{\prime}|s,a) v_\pi(s^{\prime}) \Big] \newline<br>\textcolor{red}{v(s)} &amp; \textcolor{red}{&#x3D; \max_\pi \sum_a \pi(a|s) q_\pi(s,a)}<br>\end{aligned}<br>$$</p><p>There is a problem that the BOE has two unknown variables $q_\pi(s,a)$ and $\pi(a|s)$. How can we solve the BOE?</p><p>​The idea is that we can <font color=blue>fix one variable</font> and solve the maximization problem. For Bellman optimal equation, we can <font color=blue>fix variable $\pi(a|s)$ for all $a\in\mathcal{A}(s)$</font> and then by maximize the state value to find the optimal policy $\pi$.</p><p>​Before analysis, we consider one example first. Suppose $x_1,x_2,x_3\in\mathbb{R}$ are given. Find $c_1^*,c_2^*,c_3^*$ solving<br>$$<br>\max_{c_1,c_2,c_3} c_1x_1+c_2x_2+c_3x_3\newline<br>\text{subject to } c_1+c_2+c_3&#x3D;1<br>$$<br>Without generality, suppose $x_3 \ge x_1, x_2$. Then, the optimal solution is $c_3^*&#x3D;1$ and $c_1^*&#x3D;c_2^*&#x3D;0$. This is because for any $c_1,c_2,c_3$<br>$$<br>x_3 &#x3D; (c_1+c_2+c_3)x_3 &#x3D; c_1x_3 + c_2x_3 + c_3x_3 \ge c_1x_1 +c_2x_2+c_3x_3<br>$$<br>​Hence, inspired by the above example, considering that $\sum_a \pi(a|s)&#x3D;1$, we have</p><p>$$<br>\begin{aligned}<br>v(s) &amp; &#x3D; \max_\pi \sum_a \pi(a|s) q_\pi(s,a) \newline<br>&amp; &#x3D; \sum_a \pi(a|s) \max_\pi q_\pi(s,a) \quad \text{by fix } \pi(a|s) \newline<br>&amp; &#x3D; \sum_a \pi(a|s) \max_{a\in\mathcal{A}} q_\pi(s,a) \newline<br>&amp; \le \max_{a\in\mathcal{A}} q_\pi(s,a)<br>\end{aligned}<br>$$</p><p>where the equality is achieved when $a&#x3D;a^*$, $\pi(a|s)&#x3D;1$ when $a\ne a^*$, $\pi(a|s)&#x3D;0$.</p><p>$$<br>a^* &#x3D; \text{arg} \max_a q(s,a)<br>$$</p><p>This policy is often called <font color=blue>greedy policy</font>.</p><h3 id="3-3-Matrix-vector-form-of-Bellman-optimal-equation"><a href="#3-3-Matrix-vector-form-of-Bellman-optimal-equation" class="headerlink" title="3.3 Matrix-vector form of Bellman optimal equation"></a>3.3 Matrix-vector form of Bellman optimal equation</h3><p>Refering to matrix-vector form of Bellman equation, it’s obvious to get matrix-vector form of Bellman optimal equation as follows<br>$$<br>\textcolor{blue} {v &#x3D; \max_\pi (r_\pi +\gamma P_\pi v)}<br>$$<br>where $v\in\mathbb{R^{|\mathcal{S}|}}$. The structure of $r_\pi$ and $P_\pi$ are the same as those in the matrix-vector form of Bellman equation:</p><p>$$<br>[r_\pi]_s \triangleq \sum_a \pi(a|s) \sum_r p(r|s,a)r<br>$$</p><p>$$<br>[P_\pi]<em>{s, s^\prime} &#x3D; \sum_a \pi(a|s) \sum</em>{s^\prime} p(s^\prime|s,a)<br>$$</p><p>Furthermore, denote the right hand side as </p><p>$$<br>f(v) \triangleq \max_\pi(r_\pi + \gamma P_\pi v)<br>$$</p><p>Hence, we have the Bellman optimal equation as</p><p>$$<br>v&#x3D;f(v) \newline<br>f(v) - v &#x3D; 0<br>$$</p><p><font color=blue>It turns solving Bellman optimal equation to solving find the root of function</font> $\textcolor{blue}{f(v) - v &#x3D;0}$.</p><h3 id="3-4-Contraction-mapping-theorem"><a href="#3-4-Contraction-mapping-theorem" class="headerlink" title="3.4 Contraction mapping theorem"></a>3.4 Contraction mapping theorem</h3><p>​<font color=red>Fixed point</font>: consider a function $f(x)$ where $x\in\mathbb{R}^b$ and $f:\mathbb{R}^b\to\mathbb{R}^b$. A point $x^*$ is called a fixed point if<br>$$<br>f(x^*) &#x3D; x^*<br>$$<br>The function $f$ is called a <font color=red>contracting mapping</font> if there <font color=blue>exists</font> $\gamma\in(0,1)$ such that<br>$$<br>||f(x_1)-f(x_2)|| \le \gamma ||x_1 - x_2||<br>$$<br>for any $x_1, x_2\in \mathbb{R}$.</p><p>​(<font color=red>Contraction mapping theorem</font>) For any equation that has the form of $x&#x3D;f(x)$ where $x$ and $f(x)$ are real vectors, if $f$ is a contraction mapping, then</p><ul><li><p>Existence: There exists a fixed point $x^*$ satisfying $f(x^*)&#x3D;x^*$.</p></li><li><p>Uniqueness: The fixed point $x^*$ is unique.</p></li><li><p>Algorithm: Consider the iterative process:<br>$$<br>x_{k+1} &#x3D; f(x_k)<br>$$<br>where $k&#x3D;0,1,2,\dots$. Then , $x_k\to x^*$ as $k\to \infty$ for any initial guess $x_0$. Moreover, the convergence rate is exponentially fast.</p></li></ul><h3 id="3-5-Solution-of-BOE"><a href="#3-5-Solution-of-BOE" class="headerlink" title="3.5 Solution of BOE"></a>3.5 Solution of BOE</h3><p>​(Contraction property of BOE). The function $f(v) \triangleq \max_\pi(r_\pi + \gamma P_\pi v)$ in the BOE is a contraction mapping statisfying<br>$$<br>||f(v_1)-f(v_2)|| \le \gamma ||v_1 - v_2||<br>$$<br>where $v_1,v_2\in\mathbb{R}^{|\mathcal{S}|}$ are any two vectors and $\gamma\in(0,1)$ is the discounted rate.</p><p>The proof is omitted.</p><p>​Using contraction mapping theorem to solve BOE, we have the following theorem:</p><p>​(<font color=red>Existence, Uniqueness, Algorithm</font>). <em>For the BOE</em> $v&#x3D;f(v) \triangleq \max_\pi(r_\pi + \gamma P_\pi v)$, <em>there always exists a unique solution</em> $v^*$, <em>which can be solved iteratively by</em><br>$$<br>v_{k+1} &#x3D; f(v_k) &#x3D; \max_\pi(r_\pi + \gamma P_\pi v_k), \quad k&#x3D;0,1,\dots<br>$$<br><em>The sequence</em> ${v(k)}$ <em>converges to optimal solution of BOE</em> $v^*$ <em>exponentially fast given any inital guess</em> $v_0$.</p><p>This algorithm is actually called <font  color=blue>value iteration</font>. </p><p>​<font color=blue>First</font>, following the algorithm above will give us the optimal state value $v^*$. <font color=blue>Second</font>, we solve the unknown policy $\pi$ in the BOE. Suppose $v^*$ has been obtained and<br>$$<br>\pi^* &#x3D; \text{arg}\max_\pi(r_\pi+\gamma P_\pi v^*)<br>$$<br>Then, $v^*$ and $\pi^*$ statisfy<br>$$<br>v^* &#x3D; r_{\pi^*} + \gamma P_{\pi^*} v^*<br>$$<br>​(<font color=blue>Greedy optimal policy</font>). <em>For any</em> $s\in\mathcal{S}$, <em>the deterministic greedy policy</em> </p><p>where the equality is achieved when $a&#x3D;a^*$, $\pi(a|s)&#x3D;1$ when $a\ne a^*$, $\pi(a|s)&#x3D;0$.</p><p><em>is an optimal policy solving the BOE. Here</em><br>$$<br>a^*(s) &#x3D; \text{arg} \max_a q^*(a,s)<br>$$<br><em>where</em><br>$$<br>q^*(s,a) \triangleq \sum_r p(r|s,a)r + \gamma \sum_{s^\prime} p(s^{\prime}|s,a) v^*(s)<br>$$<br>​The matrix-vecotr form of the optimal policy is $\pi^*&#x3D;\text{arg}\max_\pi(r_\pi+\gamma P_\pi v^*(s))$. Its <font color=blue>elementwise form</font> is</p><p>$$<br>\begin{aligned}<br> \pi^* &amp; &#x3D; \text{arg}\max_\pi(r_\pi+\gamma P_\pi v^*)\newline<br> \to \pi^*(s) &amp; &#x3D; \text{arg}\max_\pi \sum_a \pi(a|s) \Big( \sum_r p(r|s,a)r + \gamma \sum_{s^\prime} p(s^{\prime}|s,a) v^*(s)\Big)\newline<br> \to \pi^*(s) &amp; &#x3D; \text{arg}\max_\pi \sum_a \pi(a|s) q^*(s,a),\quad \text{for s}\in\mathcal{S}<br>\end{aligned}<br>$$</p><p>It is clear that $\sum_a \pi(a|s)q^*(s,a)$ is maximized if $\pi(s)$ selects the action with the greatest value of $q^*(s,a)$.</p><hr><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>  <a href="https://www.bilibili.com/video/BV1sd4y167NS/?spm_id_from=333.999.0.0">赵世钰老师的课程</a></p>]]></content>
      
      
      <categories>
          
          <category> Reinforcement Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reinforcement Learning with Code [Chapter 2. State Value and Bellman Equation]</title>
      <link href="/2023/07/21/Reinforcement-Learning-with-Code-Chapter-2-State-Value-and-Bellman-Equation/"/>
      <url>/2023/07/21/Reinforcement-Learning-with-Code-Chapter-2-State-Value-and-Bellman-Equation/</url>
      
        <content type="html"><![CDATA[<h1 id="Reinforcement-Learning-with-Code"><a href="#Reinforcement-Learning-with-Code" class="headerlink" title="Reinforcement Learning with Code."></a>Reinforcement Learning with Code.</h1><p>This note records how the author begin to learn RL. Both theoretical understanding and code practice are presented. Many material are referenced such as ZhaoShiyu’s <em>Mathematical Foundation of Reinforcement Learning</em>, .</p><h2 id="Chapter-2-State-Value-and-Bellman-Equation"><a href="#Chapter-2-State-Value-and-Bellman-Equation" class="headerlink" title="Chapter 2. State Value and Bellman Equation"></a>Chapter 2. State Value and Bellman Equation</h2><h3 id="2-1-State-value"><a href="#2-1-State-value" class="headerlink" title="2.1 State value"></a>2.1 State value</h3><ul><li><p><font color=red>State value</font> is defined as the mean of <font color=blue>all possible returns</font> starting from a state, which is actually the <font color=blue>expectation of return</font> from a specific state.</p></li><li><p>The mathematical definition is as follows:</p><p>Note that the capital letters denote <em>random variables</em>, such as $S_t, S_{t+1}, A_t, R_{t+1}$. In particular, $S_t,S_{t+1}\in\mathcal{S},A_t\in\mathcal{A}(S_t)$ and $R_{t+1}\in\mathcal{R}(S_t,A_t)$. $G_t$ denote the random variable of return.</p><p>Starting from $t$, we can obtain a state-action-reward trajectory:</p><p>$$<br>S_t \xrightarrow{A_t} S_{t+1},R_{t+1} \xrightarrow{A_{t+1}} S_{t+2},R_{t+2} \xrightarrow{A_{t+2}} S_{t+3},R_{t+3} \cdots<br>$$</p><p>The discounted return along the trajectory is </p><p>$$<br>G_t &#x3D; R_{t+1} + \gamma R_{t+2} + \gamma R_{t+3} + \cdots<br>$$</p><p>The state value is defined as:</p><p>$$<br>\textcolor{blue}{v_\pi(s)\triangleq \mathbb{E}[G_t|S_t&#x3D;s]}<br>$$</p><p>which means <font color=blue>start from state</font> $s$ can get the <font color=blue>expectation return</font> along the trajecotry generated by <font color=blue>policy $\pi$ </font>. </p><p>$v_\pi(s)$ is also called <font color=blue>state-value funtion</font>.</p></li></ul><h3 id="2-2-Bellman-Equation"><a href="#2-2-Bellman-Equation" class="headerlink" title="2.2 Bellman Equation"></a>2.2 Bellman Equation</h3><ul><li><p>Bellman equation is a set of linear equations <font color=blue>describing the relationship among the values of all the states</font>.</p><p>For example,</p></li></ul><img src="https://pic.imgdb.cn/item/658bd598c458853aeff51217.png" class="" width="400" height="400" title="state-transform"><p>  Let $v_i$ denote the return obtained starting from $s_i$. The return starting from the four states in figure can be respectively calculated as </p><p>$$<br>v_1 &#x3D; r_1 + \gamma r_2 + \gamma^2 r_3 + \cdots \newline<br>v_2 &#x3D; r_2 + \gamma r_3 + \gamma^2 r_4 + \cdots \newline<br>v_3 &#x3D; r_3 + \gamma r_4 + \gamma^2 r_1 + \cdots \newline<br>v_4 &#x3D; r_4 + \gamma r_1 + \gamma^2 r_2 + \cdots<br>$$</p><p> Using the idea of <em>bootstrapping</em>, we can rewrite it to</p><p>$$<br>v_1 &#x3D; r_1 + \gamma (r_2 + \gamma r_3) + \cdots &#x3D; r_1 + \gamma v_2 \newline<br>v_2 &#x3D; r_2 + \gamma (r_3 + \gamma r_4) + \cdots &#x3D; r_2 + \gamma v_3 \newline<br>v_3 &#x3D; r_3 + \gamma (r_4 + \gamma r_1) + \cdots &#x3D; r_3 + \gamma v_4 \newline<br>v_4 &#x3D; r_4 + \gamma (r_1 + \gamma r_2) + \cdots &#x3D; r_4 + \gamma v_1<br>$$</p><p>  Then rewrite it into matrix form</p><p>$$<br>\textbf{v} &#x3D;  \begin{bmatrix} v_1 \newline v_2 \newline v_3 \newline v_4 \end{bmatrix} , \textbf{r} &#x3D; \begin{bmatrix} r_1 \newline r_2 \newline r_3 \newline r_4 \end{bmatrix}, \textbf{v} &#x3D;  \begin{bmatrix} v_1 \newline v_2 \newline v_3 \newline v_4 \end{bmatrix}<br>$$</p><p>$$<br>\textbf{P} &#x3D;  \begin{bmatrix} 0 &amp; 1 &amp; 0 &amp; 0 \newline 0 &amp; 0 &amp; 1 &amp; 0 \newline 0 &amp; 0 &amp; 0 &amp; 1 \newline 1 &amp; 0 &amp; 0 &amp; 0 \end{bmatrix}<br>$$</p><p> The equation $\textbf{v}&#x3D;\textbf{r}+\gamma\textbf{P}\textbf{v}$ is called Bellman equation.</p><ul><li>Then we can derive Bellman equation from scratch as follows:</li></ul><p>Note that the discounted return random variable $G_t$ can be rewritten as </p><p>$$<br>\begin{aligned}<br>G_t &amp; &#x3D;  R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots\newline<br>&amp; &#x3D;  R_{t+1} + \gamma(R_{t+2}+\gamma R_{t+3} + \cdots) \newline<br>&amp; &#x3D;  R_{t+1} + \gamma G_{t+1}<br>\end{aligned}<br>$$</p><p> where $G_{t+1} &#x3D; R_{t+2} + \gamma R_{t+3} + \cdots$ . This equation establishes the relationship between $G_t$ and $G_{t+1}$. Then the state value can be rewritten as </p><p>$$<br>\begin{aligned}<br>v_\pi(s) &amp; &#x3D;  \mathbb{E}[G_t|S_t&#x3D;s] \newline<br>&amp; &#x3D;  \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t&#x3D;s] \newline<br>&amp; &#x3D;  \mathbb{E}[R_{t+1}|S_t&#x3D;s] + \gamma \mathbb{E}[G_{t+1}|S_t&#x3D;s]<br>\quad \text{(linear property of expectation)}<br>\end{aligned}<br>$$</p><p> The <font color = blue> mean of immediate reward</font> can be written as (use 2.3 conditional expectation)</p><p>$$<br>\begin{aligned}<br>\mathbb{E}[R_{t+1}|S_t&#x3D;s] &amp; &#x3D;  \sum_a \pi(a|s) \mathbb{E}[R_{t+1} | S_t&#x3D;s,A_t&#x3D;a] \quad \text{(conditional expectation)}\newline<br>&amp; &#x3D;  \sum_a \pi(a|s) \sum_r p(r|s,a)r<br>\end{aligned}<br>$$</p><p>  The <font color = blue>mean of future reward</font> can be written as </p><p>$$<br>\begin{aligned}<br>\gamma \mathbb{E}[G_{t+1}|S_t&#x3D;s] &amp; &#x3D;  \gamma \sum_a \pi(a|s) \mathbb{E}[G_{t+1}|S_t&#x3D;s, A_t&#x3D;a]\quad \text{(conditional expectation)}\newline<br>&amp; &#x3D;  \gamma \sum_a \pi(a|s) \sum_{s^\prime} p(s^\prime|s,a) v_\pi(s^\prime)\newline<br>\end{aligned}<br>$$</p><p>Then the state value can be rewritten as <font color=red>Bellman equation (BE)</font> form </p><p>$$<br>\begin{aligned}<br>v_\pi(s) &amp; &#x3D;  \mathbb{E}[G_t|S_t&#x3D;s] \newline<br>&amp; &#x3D;  \mathbb{E}[R_{t+1}|S_t&#x3D;s] + \gamma \mathbb{E}[G_{t+1}|S_t&#x3D;s] \newline<br>&amp; &#x3D;<br>\sum_a \pi(a|s) \sum_r p(r|s,a)r </p><p>+<br>\gamma \sum_a \pi(a|s) \sum_{s^\prime} p(s^\prime|s,a) v_\pi(s^\prime)\newline<br>&amp; &#x3D;  \sum_a \pi(a|s) \Big[\sum_r p(r|s,a)r+ \gamma\sum_{s^\prime} p(s^\prime|s,a) v_\pi(s^\prime)\Big], \quad \text{for all } s\in\mathcal{S}<br>\end{aligned}<br>$$</p><p>Next, we will introduce the matrix vector form of Bellman equation in terms of state value. Let $v_\pi&#x3D;[v_\pi(s_1),v_\pi(s_2),\cdots]^T\in\mathbb{R}^n$, $r_\pi&#x3D;[r_\pi(s_1),r_\pi(s_2),\cdots]^T\in\mathbb{R}^n$, and $P_\pi\in\mathbb{R}^{n\times n}$. Then we have the matrix-vector form of Bellman equation in terms of state value.</p><p>$$<br>\textcolor{red}{v_\pi &#x3D; r + \gamma P_\pi v_\pi} \quad \text{(matrix-vector form)}<br>$$</p><p>where</p><p>$$<br>[r_\pi]<em>s \triangleq \sum_a \pi(a|s) \sum_r p(r|s,a)r,<br>[P_\pi]</em>{s,s^\prime} &#x3D; \sum_a \pi(a|s) \sum_{s^\prime} p(s^\prime|s,a)<br>$$</p><h3 id="2-3-Review-conditional-expectaion"><a href="#2-3-Review-conditional-expectaion" class="headerlink" title="2.3 Review conditional expectaion"></a>2.3 Review conditional expectaion</h3><ul><li>The definition of conditional expectation is</li></ul><p>$$<br>\mathbb{E} &#x3D; [X|A&#x3D;a] &#x3D; \sum_{x}xp(x|a)<br>$$</p><p>  Similar to the law of total probability, we have the law of total expectation:</p><p>$$<br>\mathbb{E}[X] &#x3D; \sum_a p(a) \mathbb{E}[X|A&#x3D;a]<br>$$</p><p>  <em>Proof</em></p><p>  By definition of expectation the right hand side can be written as </p><p>$$<br>\begin{aligned}<br>\mathbb{E}[X] &amp; &#x3D;  \sum_a p(a) \sum_x x p(x|a)\newline<br>&amp; &#x3D;  \sum_x \sum_a p(x|a) p(a)  \quad \text{(law of total probability)}\newline<br>&amp; &#x3D;  \sum_x p(x)\newline<br>&amp; &#x3D;  \mathbb{E}[X]<br>\end{aligned}<br>$$</p><h3 id="2-4-Solve-Bellman-Equation"><a href="#2-4-Solve-Bellman-Equation" class="headerlink" title="2.4 Solve Bellman Equation"></a>2.4 Solve Bellman Equation</h3><p>​From the analysis above we are informed that Bellman equation is</p><p>$$<br>\textcolor{blue}{v_\pi(s)&#x3D;\sum_a \pi(a|s) \Big[\sum_r p(r|s,a)r+ \gamma\sum_{s^\prime} p(s^\prime|s,a) v_\pi(s^\prime)\Big]}<br>$$</p><p>We can rewrite it into the follow form</p><p>$$<br>v_\pi(s) &#x3D; r_\pi(s) + \gamma \sum_{s^\prime} p_\pi(s^\prime|s) v_\pi(s^\prime)<br>$$</p><p>where</p><p>$$<br>\begin{aligned}<br>r_\pi(s) \triangleq \sum_a \pi(a|s) \sum_r p(r|s,a)r\newline<br>\newline<br>\newline<br>\newline<br>\end{aligned} \qquad<br>\begin{aligned}<br>p_\pi(s^\prime|s) &amp; \triangleq \sum_a \pi(a|s) \sum_{s^\prime} p(s^\prime|s,a)\newline<br>&amp; &#x3D; \sum_{s^\prime} \sum_a \pi(a|s) p(s^\prime|s,a)\newline<br>&amp; &#x3D; p_\pi(s^\prime|s)<br>\end{aligned}<br>$$</p><p>​Suppose the states are indexed as $s_i$. For state $s_i$, the Bellman equation is </p><p>$$<br>v_\pi(s_i) &#x3D; r_\pi(s_i) + \gamma \sum_{s_j} p_\pi(s_j|s_i) v_\pi(s_j)<br>$$</p><p>Let $v_\pi&#x3D;[v_\pi(s_1),\dots,v_\pi(s_n)]^T \in \mathbb{R}^n$, $r_\pi&#x3D;[r_\pi(s_1),\dots,r_\pi(s_n)]^T \in \mathbb{R}^n$, and $P_\pi \in \mathbb{R}^{n\times n}$, where $[P_\pi]_{ij} &#x3D; p_\pi(s_j|s_i)$. Hence we have the matrix form as </p><p>$$<br>v_\pi &#x3D; r_\pi + \gamma P_\pi v_\pi<br>$$</p><p>For example, consider four states $s_i$ and four actions $a_i$ the matrix form can be</p><p>$$<br>\underbrace{<br>\begin{bmatrix}<br>v_\pi(s_1) \newline<br>v_\pi(s_2) \newline<br>v_\pi(s_3) \newline<br>v_\pi(s_4)<br>\end{bmatrix}<br>}<em>{v_\pi} &#x3D;<br>\underbrace{<br>\begin{bmatrix}<br>r_\pi(s_1) \newline<br>r_\pi(s_1) \newline<br>r_\pi(s_1) \newline<br>r_\pi(s_1)<br>\end{bmatrix}<br>}</em>{r_\pi} + \gamma<br>\underbrace{<br>\begin{bmatrix}<br>p_\pi(s_1|s_1) &amp; p_\pi(s_2|s_1) &amp; p_\pi(s_3|s_1) &amp; p_\pi(s_4|s_1) \newline<br>p_\pi(s_1|s_2) &amp; p_\pi(s_2|s_2) &amp; p_\pi(s_3|s_2) &amp; p_\pi(s_4|s_2) \newline<br>p_\pi(s_1|s_3) &amp; p_\pi(s_2|s_3) &amp; p_\pi(s_3|s_3) &amp; p_\pi(s_4|s_3) \newline<br>p_\pi(s_1|s_4) &amp; p_\pi(s_2|s_4) &amp; p_\pi(s_3|s_4) &amp; p_\pi(s_4|s_4) \newline<br>\end{bmatrix}<br>}<em>{P_\pi}<br>\underbrace{<br>\begin{bmatrix}<br>v_\pi(s_1) \newline<br>v_\pi(s_2) \newline<br>v_\pi(s_3) \newline<br>v_\pi(s_4)<br>\end{bmatrix}<br>}</em>{v_\pi}<br>$$</p><p><strong>Conclusions</strong>:</p><ul><li>So we have the <font color=blue>closed form</font> of the solution as $v_\pi &#x3D; (I-\gamma P_\pi)^{-1}r_\pi$. However the inverse operation is hard to implement.</li><li>We seek for other iterative solving methods.</li></ul><p><strong>Iterative solution</strong></p><p>​We can directly sovle the Bellman equation using the following iterative algorithm:</p><p>$$<br>\textcolor{blue}{v_{k+1} &#x3D; r_\pi + \gamma P_\pi v_k}<br>$$</p><p>The proof is omitted.</p><h3 id="2-5-Action-value"><a href="#2-5-Action-value" class="headerlink" title="2.5 Action value"></a>2.5 Action value</h3><p>​<font color=red>Action value</font> is denoted by $q_\pi(s,a)$ which is defined as</p><p>$$<br>q_\pi(s,a) \triangleq \mathbb{E}[G_t|S_t&#x3D;s, A_t&#x3D;a]<br>$$</p><p>for all $s \in \mathcal{S}, a\in \mathcal{A}(s)$. </p><p>Action value can be interpreted as <font color=blue>average return</font> along trajectory generated by <font color=blue>policy $\pi$ </font> after taking a <font color=blue>specific action $a$</font>.</p><p>​From the conditional expectation property that</p><p>$$<br>\underbrace{\mathbb{E}[G_t|S_t&#x3D;s]}<em>{v_\pi(s)} &#x3D; \sum_a \pi(a|s) \underbrace{\mathbb{E}[G_t|S_t&#x3D;s,A_t&#x3D;a]}</em>{q_\pi(s)}<br>$$</p><p>Hence, </p><p>$$<br>v_\pi(s) &#x3D; \sum_a \pi(a|s) q_\pi(s)<br>$$</p><p>So we can obtain the mathematical definition of action value as</p><p>$$<br>\textcolor{blue}{q_\pi(s)&#x3D;\sum_r p(r|s,a)r+ \gamma\sum_{s^\prime} p(s^\prime|s,a) v_\pi(s^\prime)}<br>$$</p><p>Substituting $(1)$ into $(2)$ we have</p><p>$$<br>q_\pi(s,a)&#x3D;\sum_r p(r|s,a)r+ \gamma\sum_{s^\prime} p(s^\prime|s,a) \sum_{a^\prime \in \mathcal{A}(s^\prime)}\pi(a^\prime|s^\prime) q_\pi(s^\prime,a^\prime)<br>$$</p><p>​Suppose each state has the same number of actions. The matrix-vecotr form of Bellman eqaution in terms of action value is</p><p>$$<br>\textcolor{red}{q_\pi &#x3D; \tilde{r} + \gamma P \Pi q_\pi}\quad \text{(matrix-vector form)}<br>$$</p><p>where $q_\pi \in \mathbb{R}^{|\mathcal{S}||\mathcal{A}|}$ is the action value vector indexed by state-action pairs. In particular, the $(s,a)$th element is </p><p>$$<br>[q_\pi]_{(s,a)} &#x3D; q_\pi(s,a)<br>$$</p><p>Here, $\tilde{r}\in\mathbb{R}^{|\mathcal{S} ||\mathcal{A} |}$ is the immediate reward vector indexed by state-action pairs. In paricular, the $(s,a)$th reward is</p><p>$$<br>[\tilde{r}]_{(s,a)} &#x3D; \sum_r p(r|s,a)r<br>$$</p><p>Here, $P\in \mathbb{R}^{|\mathcal{S}||\mathcal{A}|\times|\mathcal{S}| }$ is the probability transition matrix, whose row is indexed by state-action pairs and column indexed by states. In particular</p><p>$$<br>[P]_{(s,a),s^\prime} &#x3D; p(s^\prime|s,a)<br>$$</p><p>And $\Pi \in\mathbb{R}^{|\mathcal{S}|\times|\mathcal{S}||\mathcal{A}|}$ describes the policy $\pi$. In particular</p><p>$$<br>\Pi_{s^\prime, (s^\prime,a^\prime)} &#x3D; \pi(a^\prime|s^\prime)<br>$$</p><p>and the other entries of $\Pi$ is zero. $\Pi$ is block diagonal matrix with each block as a $1\times|\mathcal{A}|$ vector.</p><hr><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>  <a href="https://www.bilibili.com/video/BV1sd4y167NS/?spm_id_from=333.999.0.0">赵世钰老师的课程</a></p>]]></content>
      
      
      <categories>
          
          <category> Reinforcement Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reinforcement Learning with Code [Chapter 1. Basic Concepts]</title>
      <link href="/2023/07/02/Reinforcement-Learning-with-Code-Chapter-1-Basic-Concepts/"/>
      <url>/2023/07/02/Reinforcement-Learning-with-Code-Chapter-1-Basic-Concepts/</url>
      
        <content type="html"><![CDATA[<h1 id="Reinforcement-Learning-with-Code"><a href="#Reinforcement-Learning-with-Code" class="headerlink" title="Reinforcement Learning with Code."></a>Reinforcement Learning with Code.</h1><p>This note record how the author begin to learn RL. Both theoretical understanding and code practice are presented. Many material are referenced such as ZhaoShiyu’s <em>Mathematical Foundation of Reinforcement Learning</em> .</p><h2 id="Chapter-1-Basic-Concepts"><a href="#Chapter-1-Basic-Concepts" class="headerlink" title="Chapter 1. Basic Concepts"></a>Chapter 1. Basic Concepts</h2><h3 id="1-1-State-and-action"><a href="#1-1-State-and-action" class="headerlink" title="1.1 State and action"></a>1.1 State and action</h3><ul><li><font color=red><em>State</em> </font>describe the status of the agent with respect to the environment, denoted by $s$.</li><li><font color=red><em>State space</em> </font> is the set of all states, denoted by $\mathcal{S}&#x3D;{s_1, s_2,\dots,s_n}$.</li><li><font color = red><em>Action</em></font> describe the action that the agent may take with respect to the environment, denoted by $a$.</li><li><font color = red><em>Action space</em></font> is the set of all actions, denoted by $\mathcal{A}&#x3D;{a_1, a_2,\dots,a_n}.$</li></ul><h3 id="1-2-State-transition"><a href="#1-2-State-transition" class="headerlink" title="1.2 State transition"></a>1.2 State transition</h3><p>When taking an action, the agent may move from one state to another. Such a process is called <font color=red><em>state transition</em></font>. State trasition can be denoted by<br>$$<br>s_1 \stackrel{a_2}\longrightarrow s_2<br>$$<br>described by $p(s^\prime|s,a)$.</p><p>State trainsition can be both <em>deterministic</em> and <em>stochastic</em>. For example the deterministic state transition is<br>$$<br>p(s_1|s_1,a_2) &#x3D; 0 \<br>p(s_2|s_1,a_2) &#x3D; 1 \<br>p(s_3|s_1,a_2) &#x3D; 0<br>$$<br>For example the stochastic state transition is<br>$$<br>p(s_1|s_1,a_2) &#x3D; 0.5 \<br>p(s_2|s_1,a_2) &#x3D; 0.3 \<br>p(s_3|s_1,a_2) &#x3D; 0.2<br>$$</p><h3 id="1-3-Policy"><a href="#1-3-Policy" class="headerlink" title="1.3 Policy"></a>1.3 Policy</h3><ul><li><font color = red><em>Policy</em></font> tells the agents which actions to take <font color=blue>at each state</font>, denoted by $\pi$.</li><li>Policy is described by conditional probability.</li><li>Policy can be <em>deterministic</em> or <em>stochastic</em>, which means one state has a deterministic action or one state has probability to select other actions.</li></ul><p>Suppose the actions space is $\mathcal{A}&#x3D;{a_1, a_2,a_3}$, such  deterministic policy can be dentoed by<br>$$<br>\pi(a_1|s_1) &#x3D; 0 \<br>\pi(a_2|s_1) &#x3D; 1 \<br>\pi(a_3|s_1) &#x3D; 0<br>$$<br>which indicated the probability of taking action $a_2$ is $1$ and others are zero. </p><p>Such stochastic policy can be denoted by<br>$$<br>\pi(a_1|s_1) &#x3D; 0.5 \<br>\pi(a_2|s_1) &#x3D; 0.3 \<br>\pi(a_3|s_1) &#x3D; 0.2<br>$$</p><h3 id="1-4-Reward"><a href="#1-4-Reward" class="headerlink" title="1.4 Reward"></a>1.4 Reward</h3><ul><li><font color = red><em>Reward</em></font> is one of the most unique concept in RL.</li><li><font color = red><em>Immediate reward</em></font> can be obtained after taking an action.</li><li><font color = red><em>Reward transition</em></font> is the process of getting a reward after taking an action, reward transition can be <em>deterministic</em> or <em>stochastic</em>. Reward transition is described by $p(r|s,a)$</li></ul><p>For example deterministic reward transition can be denoted by<br>$$<br>p(r&#x3D;-1|s_1,a_2) &#x3D; 1, p(r\ne -1|s_1,a_2)&#x3D;0<br>$$<br>which means at state $s_1$ take action $a_2$ the probability to get immediate reward $-1$ is $1$.</p><p>Stochastic reward transition can be denoted by<br>$$<br>p(r&#x3D;1|s_1,a_2) &#x3D; 0.5, p(r&#x3D; 0|s_1,a_2)&#x3D;0.5<br>$$<br>which means at state $s_1$ take action $a_2$ the probability to get immediate reward $1$ is $0.5$, the probability to get immediate reward $0$ is $0.5$.</p><h3 id="1-5-Trajectory-return-episode"><a href="#1-5-Trajectory-return-episode" class="headerlink" title="1.5 Trajectory, return, episode"></a>1.5 Trajectory, return, episode</h3><ul><li><p><font color = red><em>Trajectory</em></font> is a state-action-reward chain, such as $s_1 \underset{r&#x3D;0}{\xrightarrow{a_2}} s_2 \underset{r&#x3D;0}{\xrightarrow{a_3}} s_3 \underset{r&#x3D;0}{\xrightarrow{a_4}} \cdots\underset{r&#x3D;1}{\xrightarrow{a_n}} s_{n}$.</p></li><li><p><font color = red><em>Return</em></font> of this trajecotry is the sum of all the rewards collected along the trajectory, such as $\text{return} &#x3D; 0+0+0+\cdots+1&#x3D;1$. Return is also called <font color=blue><em>total rewards</em></font> or <font color=blue><em>cumulative rewards</em></font>.</p></li><li><p><font color = red><em>Discounted return</em></font> is defined by the <font color=blue><em>discounted rate</em></font>, denoted by $\gamma\in(0,1)$. Such discounted retrun is<br>$$<br>\text{discounted return} &#x3D; 0+\gamma0+\gamma^2 0 + \gamma^3 0 + \cdots + \gamma^n 1<br>$$</p></li><li><p><font color = red><em>Episode</em></font> refers the trajectory that interacting with the enviornment following a policy <font color = blue>until the agent reach the terminal state</font>. An episode is usually assumed to be a finite trajectory,  that task with episodes are called <em>episodic tasks</em>. Some task may have no terminal state, such task is called <em>continuing tasks</em>.</p></li></ul><h3 id="1-6-Markov-decision-process-MDP"><a href="#1-6-Markov-decision-process-MDP" class="headerlink" title="1.6 Markov decision process (MDP)"></a>1.6 Markov decision process (MDP)</h3><p>Markov decision process is a general framework to <font color=blue>describe stochastic dynamical systems</font>. The key ingredients of an MDP are listed:</p><ul><li><p>Sets:</p><ul><li>State set: the set of all states, denoted as $\mathcal{S}$.</li><li>Actions set: a set of actions, denoted as $\mathcal{A}(s)$, is associated for each state $s\in\mathcal{S}$.</li><li>Reward set: a set of rewards, denoted as $\mathcal{R}(s,a)$, is associated for each state action pari $(s,a)$.</li></ul></li><li><p>Model:</p><ul><li>State transition probability: at state $s$, taking actions $a$, the probability to transit to state $s^\prime$ is $p(s^\prime|s,a)$.</li><li>Reward transition probability: at state $s$, taking action $a$, the probability to get reward $r$ is $p(r|s,a)$.</li></ul></li><li><p>Policy: as state $s$, the probability to choose action $a$ is $\pi(a|s)$.</p></li><li><p>Markov property: one key property of MDPs is the <em>Markov property</em>, which refers to the <font color=blue>memoryless property</font> of a stochastic process, which means<br>$$<br>p(s_{t+1}|s_t,a_t,s_{t-1},a_{t-1},\dots,s_0,a_0)&#x3D;p(s_{t+1}|s_t,a_t)\<br>p(r_{t+1}|s_t,a_t,s_{t-1},a_{t-1},\dots,s_0,a_0)&#x3D;p(r_{t+1}|s_t,a_t)\<br>$$</p><hr><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://www.bilibili.com/video/BV1sd4y167NS/?spm_id_from=333.999.0.0">赵世钰老师的课程</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Reinforcement Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
