<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/12/24/hello-world/"/>
      <url>/2023/12/24/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Reinforcement Learning with Code [Chapter 2. State Value and Bellman Equation]</title>
      <link href="/2023/07/27/Reinforcement-Learning-with-Code-Chapter-2-State-Value-and-Bellman-Equation/"/>
      <url>/2023/07/27/Reinforcement-Learning-with-Code-Chapter-2-State-Value-and-Bellman-Equation/</url>
      
        <content type="html"><![CDATA[<h1 id="Reinforcement-Learning-with-Code"><a href="#Reinforcement-Learning-with-Code" class="headerlink" title="Reinforcement Learning with Code"></a>Reinforcement Learning with Code</h1><p>This note records how the author begin to learn RL. Both theoretical understanding and code practice are presented. Many material are referenced such as ZhaoShiyu’s <em>Mathematical Foundation of Reinforcement Learning</em>, .</p><h2 id="Chapter-2-State-Value-and-Bellman-Equation"><a href="#Chapter-2-State-Value-and-Bellman-Equation" class="headerlink" title="Chapter 2. State Value and Bellman Equation"></a>Chapter 2. State Value and Bellman Equation</h2><h3 id="2-1-State-value"><a href="#2-1-State-value" class="headerlink" title="2.1 State value"></a>2.1 State value</h3><ul><li><p><font color="red">State value</font> is defined as the mean of <font color="blue">all possible returns</font> starting from a state, which is actually the <font color="blue">expectation of return</font> from a specific state.</p></li><li><p>The mathematical definition is as follows:</p><p>Note that the capital letters denote <em>random variables</em>, such as $S_t, S_{t+1}, A_t, R_{t+1}$. In particular, $S_t,S_{t+1}\in\mathcal{S},A_t\in\mathcal{A}(S_t)$ and $R_{t+1}\in\mathcal{R}(S_t,A_t)$. $G_t$ denote the random variable of return.</p><p>Starting from $t$, we can obtain a state-action-reward trajectory:</p><script type="math/tex; mode=display">S_t \xrightarrow{A_t} S_{t+1},R_{t+1} \xrightarrow{A_{t+1}} S_{t+2},R_{t+2} \xrightarrow{A_{t+2}} S_{t+3},R_{t+3} \cdots</script><p>The discounted return along the trajectory is </p><script type="math/tex; mode=display">G_t = R_{t+1} + \gamma R_{t+2} + \gamma R_{t+3} + \cdots</script><p>The state value is defined as:</p><script type="math/tex; mode=display">\textcolor{blue}{v_\pi(s)\triangleq \mathbb{E}[G_t|S_t=s]}</script><p>which means <font color="blue">start from state</font> $s$ can get the <font color="blue">expectation return</font> along the trajecotry generated by <font color="blue">policy $\pi$ </font>. </p><p>$v_\pi(s)$ is also called <font color="blue">state-value funtion</font>.</p></li></ul><h3 id="2-2-Bellman-Equation"><a href="#2-2-Bellman-Equation" class="headerlink" title="2.2 Bellman Equation"></a>2.2 Bellman Equation</h3><ul><li><p>Bellman equation is a set of linear equations <font color="blue">describing the relationship among the values of all the states</font>.</p><p>For example,</p></li></ul><img src="https://pic.imgdb.cn/item/658bd598c458853aeff51217.png" class width="400" height="400" title="state-transform"><p>  Let $v_i$ denote the return obtained starting from $s_i$. The return starting from the four states in figure can be respectively calculated as </p><script type="math/tex; mode=display">v_1 = r_1 + \gamma r_2 + \gamma^2 r_3 + \cdots \newlinev_2 = r_2 + \gamma r_3 + \gamma^2 r_4 + \cdots \newlinev_3 = r_3 + \gamma r_4 + \gamma^2 r_1 + \cdots \newlinev_4 = r_4 + \gamma r_1 + \gamma^2 r_2 + \cdots</script><p> Using the idea of <em>bootstrapping</em>, we can rewrite it to</p><script type="math/tex; mode=display">v_1 = r_1 + \gamma (r_2 + \gamma r_3) + \cdots = r_1 + \gamma v_2 \newlinev_2 = r_2 + \gamma (r_3 + \gamma r_4) + \cdots = r_2 + \gamma v_3 \newlinev_3 = r_3 + \gamma (r_4 + \gamma r_1) + \cdots = r_3 + \gamma v_4 \newlinev_4 = r_4 + \gamma (r_1 + \gamma r_2) + \cdots = r_4 + \gamma v_1</script><p>  Then rewrite it into matrix form</p><script type="math/tex; mode=display">\underbrace{ \begin{bmatrix} v_1 \newline v_2 \newline v_3 \newline v_4 \end{bmatrix}}_{\text{v}} = \underbrace{ \begin{bmatrix} r_1 \newline r_2 \newline r_3 \newline r_4 \end{bmatrix}}_{\text{r}} + \gamma\underbrace{ \begin{bmatrix} 0 & 1 & 0 & 0 \newline 0 & 0 & 1 & 0 \newline 0 & 0 & 0 & 1 \newline 1 & 0 & 0 & 0 \end{bmatrix}}_{\text{P}}\underbrace{ \begin{bmatrix} v_1 \newline v_2 \newline v_3 \newline v_4 \end{bmatrix}}_{\text{v}}</script><p> The equation $\textbf{v}=\textbf{r}+\gamma\textbf{P}\textbf{v}$ is called Bellman equation.</p><ul><li>Then we can derive Bellman equation from scratch as follows:</li></ul><p>Note that the discounted return random variable $G_t$ can be rewritten as </p><script type="math/tex; mode=display">\begin{aligned}G_t & =  R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots    \newline& =  R_{t+1} + \gamma(R_{t+2}+\gamma R_{t+3} + \cdots) \newline& =  R_{t+1} + \gamma G_{t+1}\end{aligned}</script><p> where $G_{t+1} = R_{t+2} + \gamma R_{t+3} + \cdots$ . This equation establishes the relationship between $G_t$ and $G_{t+1}$. Then the state value can be rewritten as </p><script type="math/tex; mode=display">\begin{aligned}v_\pi(s) & =  \mathbb{E}[G_t|S_t=s] \newline& =  \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t=s] \newline& =  \underbrace{\mathbb{E}[R_{t+1}|S_t=s]}_{\text{mean of immediate reward}} + \underbrace{\gamma \mathbb{E}[G_{t+1}|S_t=s]}_{\text{mean of future reward}} \quad \text{(linear property of expectation)}\end{aligned}</script><p> The <font color="blue"> mean of immediate reward</font> can be written as (use 2.3 conditional expectation)</p><script type="math/tex; mode=display">\begin{aligned}\mathbb{E}[R_{t+1}|S_t=s] & =  \sum_a \pi(a|s) \mathbb{E}[R_{t+1} | S_t=s,A_t=a] \quad \text{(conditional expectation)}    \newline& =  \sum_a \pi(a|s) \sum_r p(r|s,a)r\end{aligned}</script><p>  The <font color="blue">mean of future reward</font> can be written as </p><script type="math/tex; mode=display">\begin{aligned}\gamma \mathbb{E}[G_{t+1}|S_t=s] & =  \gamma \sum_a \pi(a|s) \mathbb{E}[G_{t+1}|S_t=s, A_t=a]    \quad \text{(conditional expectation)}    \newline& =  \gamma \sum_a \pi(a|s) \sum_{s^\prime} p(s^\prime|s,a) v_\pi(s^\prime)    \newline\end{aligned}</script><p>Then the state value can be rewritten as <font color="red">Bellman equation (BE)</font> form </p><script type="math/tex; mode=display">\begin{aligned}v_\pi(s) & =  \mathbb{E}[G_t|S_t=s] \newline& =  \mathbb{E}[R_{t+1}|S_t=s] + \gamma \mathbb{E}[G_{t+1}|S_t=s]     \newline& =  \underbrace{\sum_a \pi(a|s) \sum_r p(r|s,a)r}_{\text{mean of immediate reward}} + \underbrace{\gamma \sum_a \pi(a|s) \sum_{s^\prime} p(s^\prime|s,a) v_\pi(s^\prime)}_{\text{mean of future reward}}    \newline& =  \sum_a \pi(a|s) \Big[\sum_r p(r|s,a)r    + \gamma\sum_{s^\prime} p(s^\prime|s,a) v_\pi(s^\prime)    \Big], \quad \text{for all } s\in\mathcal{S}\end{aligned}</script><p>Next, we will introduce the matrix vector form of Bellman equation in terms of state value. Let $v_\pi=[v_\pi(s_1),v_\pi(s_2),\cdots]^T\in\mathbb{R}^n$, $r_\pi=[r_\pi(s_1),r_\pi(s_2),\cdots]^T\in\mathbb{R}^n$, and $P_\pi\in\mathbb{R}^{n\times n}$. Then we have the matrix-vector form of Bellman equation in terms of state value.</p><script type="math/tex; mode=display">\textcolor{red}{v_\pi = r + \gamma P_\pi v_\pi} \quad \text{(matrix-vector form)}</script><p>where</p><script type="math/tex; mode=display">[r_\pi]_s \triangleq \sum_a \pi(a|s) \sum_r p(r|s,a)r \qquad [P_\pi]_{s,s^\prime} = \sum_a \pi(a|s) \sum_{s^\prime} p(s^\prime|s,a)</script><h3 id="2-3-Review-conditional-expectaion"><a href="#2-3-Review-conditional-expectaion" class="headerlink" title="2.3 Review conditional expectaion"></a>2.3 Review conditional expectaion</h3><ul><li>The definition of conditional expectation is </li></ul><script type="math/tex; mode=display">\mathbb{E} = [X|A=a] = \sum_{x}xp(x|a)</script><p>  Similar to the law of total probability, we have the law of total expectation:</p><script type="math/tex; mode=display">\mathbb{E}[X] = \sum_a p(a) \mathbb{E}[X|A=a]</script><p>  <em>Proof</em></p><p>  By definition of expectation the right hand side can be written as </p><script type="math/tex; mode=display">\begin{aligned}\mathbb{E}[X] & =  \sum_a p(a) \sum_x x p(x|a)    \newline& =  \sum_x \sum_a p(x|a) p(a)  \quad \text{(law of total probability)}    \newline& =  \sum_x p(x)    \newline& =  \mathbb{E}[X]\end{aligned}</script><h3 id="2-4-Solve-Bellman-Equation"><a href="#2-4-Solve-Bellman-Equation" class="headerlink" title="2.4 Solve Bellman Equation"></a>2.4 Solve Bellman Equation</h3><p>​    From the analysis above we are informed that Bellman equation is</p><script type="math/tex; mode=display">\textcolor{blue}{v_\pi(s)=\sum_a \pi(a|s) \Big[\sum_r p(r|s,a)r    + \gamma\sum_{s^\prime} p(s^\prime|s,a) v_\pi(s^\prime)    \Big]}</script><p>We can rewrite it into the follow form</p><script type="math/tex; mode=display">v_\pi(s) = r_\pi(s) + \gamma \sum_{s^\prime} p_\pi(s^\prime|s) v_\pi(s^\prime)</script><p>where</p><script type="math/tex; mode=display">\begin{aligned}    r_\pi(s) \triangleq \sum_a \pi(a|s) \sum_r p(r|s,a)r    \newline\newline\newline\newline\end{aligned} \qquad\begin{aligned}p_\pi(s^\prime|s) & \triangleq \sum_a \pi(a|s) \sum_{s^\prime} p(s^\prime|s,a)    \newline& = \sum_{s^\prime} \sum_a \pi(a|s) p(s^\prime|s,a)    \newline& = p_\pi(s^\prime|s)\end{aligned}</script><p>​    Suppose the states are indexed as $s_i$. For state $s_i$, the Bellman equation is </p><script type="math/tex; mode=display">v_\pi(s_i) = r_\pi(s_i) + \gamma \sum_{s_j} p_\pi(s_j|s_i) v_\pi(s_j)</script><p>Let $v_\pi=[v_\pi(s_1),\dots,v_\pi(s_n)]^T \in \mathbb{R}^n$, $r_\pi=[r_\pi(s_1),\dots,r_\pi(s_n)]^T \in \mathbb{R}^n$, and $P_\pi \in \mathbb{R}^{n\times n}$, where $[P_\pi]_{ij} = p_\pi(s_j|s_i)$. Hence we have the matrix form as </p><script type="math/tex; mode=display">v_\pi = r_\pi + \gamma P_\pi v_\pi</script><p>For example, consider four states $s_i$ and four actions $a_i$ the matrix form can be</p><script type="math/tex; mode=display">\underbrace{\begin{bmatrix}v_\pi(s_1) \newlinev_\pi(s_2) \newlinev_\pi(s_3) \newlinev_\pi(s_4) \end{bmatrix}}_{v_\pi} = \underbrace{\begin{bmatrix}r_\pi(s_1) \newliner_\pi(s_1) \newliner_\pi(s_1) \newliner_\pi(s_1) \end{bmatrix}}_{r_\pi} + \gamma\underbrace{\begin{bmatrix}p_\pi(s_1|s_1) & p_\pi(s_2|s_1) & p_\pi(s_3|s_1) & p_\pi(s_4|s_1) \newlinep_\pi(s_1|s_2) & p_\pi(s_2|s_2) & p_\pi(s_3|s_2) & p_\pi(s_4|s_2) \newlinep_\pi(s_1|s_3) & p_\pi(s_2|s_3) & p_\pi(s_3|s_3) & p_\pi(s_4|s_3) \newlinep_\pi(s_1|s_4) & p_\pi(s_2|s_4) & p_\pi(s_3|s_4) & p_\pi(s_4|s_4) \newline \end{bmatrix}}_{P_\pi}\underbrace{\begin{bmatrix}v_\pi(s_1) \newlinev_\pi(s_2) \newlinev_\pi(s_3) \newlinev_\pi(s_4) \end{bmatrix}}_{v_\pi}</script><p><strong>Conclusions</strong>:</p><ul><li>So we have the <font color="blue">closed form</font> of the solution as $v_\pi = (I-\gamma P_\pi)^{-1}r_\pi$. However the inverse operation is hard to implement.</li><li>We seek for other iterative solving methods.</li></ul><p><strong>Iterative solution</strong></p><p>​    We can directly sovle the Bellman equation using the following iterative algorithm:</p><script type="math/tex; mode=display">\textcolor{blue}{v_{k+1} = r_\pi + \gamma P_\pi v_k}</script><p>The proof is omitted.</p><h3 id="2-5-Action-value"><a href="#2-5-Action-value" class="headerlink" title="2.5 Action value"></a>2.5 Action value</h3><p>​    <font color="red">Action value</font> is denoted by $q_\pi(s,a)$ which is defined as</p><script type="math/tex; mode=display">q_\pi(s,a) \triangleq \mathbb{E}[G_t|S_t=s, A_t=a]</script><p>for all $s \in \mathcal{S}, a\in \mathcal{A}(s)$. </p><p>Action value can be interpreted as <font color="blue">average return</font> along trajectory generated by <font color="blue">policy $\pi$ </font> after taking a <font color="blue">specific action $a$</font>.</p><p>​    From the conditional expectation property that</p><script type="math/tex; mode=display">\underbrace{\mathbb{E}[G_t|S_t=s]}_{v_\pi(s)} = \sum_a \pi(a|s) \underbrace{\mathbb{E}[G_t|S_t=s,A_t=a]}_{q_\pi(s)}</script><p>Hence, </p><script type="math/tex; mode=display">v_\pi(s) = \sum_a \pi(a|s) q_\pi(s)</script><p>So we can obtain the mathematical definition of action value as</p><script type="math/tex; mode=display">\textcolor{blue}{q_\pi(s)=\sum_r p(r|s,a)r    + \gamma\sum_{s^\prime} p(s^\prime|s,a) v_\pi(s^\prime)    }</script><p>Substituting $(1)$ into $(2)$ we have</p><script type="math/tex; mode=display">q_\pi(s,a)=\sum_r p(r|s,a)r    + \gamma\sum_{s^\prime} p(s^\prime|s,a) \sum_{a^\prime \in \mathcal{A}(s^\prime)}\pi(a^\prime|s^\prime) q_\pi(s^\prime,a^\prime)</script><p>​    Suppose each state has the same number of actions. The matrix-vecotr form of Bellman eqaution in terms of action value is</p><script type="math/tex; mode=display">\textcolor{red}{q_\pi = \tilde{r} + \gamma P \Pi q_\pi}    \quad \text{(matrix-vector form)}</script><p>where $q_\pi \in \mathbb{R}^{|\mathcal{S}||\mathcal{A}|}$ is the action value vector indexed by state-action pairs. In particular, the $(s,a)$th element is </p><script type="math/tex; mode=display">[q_\pi]_{(s,a)} = q_\pi(s,a)</script><p>Here, $\tilde{r}\in\mathbb{R}^{|\mathcal{S} ||\mathcal{A} |}$ is the immediate reward vector indexed by state-action pairs. In paricular, the $(s,a)$th reward is</p><script type="math/tex; mode=display">[\tilde{r}]_{(s,a)} = \sum_r p(r|s,a)r</script><p>Here, $P\in \mathbb{R}^{|\mathcal{S}||\mathcal{A}|\times|\mathcal{S}| }$ is the probability transition matrix, whose row is indexed by state-action pairs and column indexed by states. In particular</p><script type="math/tex; mode=display">[P]_{(s,a),s^\prime} = p(s^\prime|s,a)</script><p>And $\Pi \in\mathbb{R}^{|\mathcal{S}|\times|\mathcal{S}||\mathcal{A}|}$ describes the policy $\pi$. In particular</p><script type="math/tex; mode=display">\Pi_{s^\prime, (s^\prime,a^\prime)} = \pi(a^\prime|s^\prime)</script><p>and the other entries of $\Pi$ is zero. $\Pi$ is block diagonal matrix with each block as a $1\times|\mathcal{A}|$ vector.</p><hr><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>  <a href="https://www.bilibili.com/video/BV1sd4y167NS/?spm_id_from=333.999.0.0">赵世钰老师的课程</a></p>]]></content>
      
      
      <categories>
          
          <category> Reinforcement Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reinforcement Learning with Code [Chapter 1. Basic Concepts]</title>
      <link href="/2023/07/02/Reinforcement-Learning-with-Code-Chapter-1-Basic-Concepts/"/>
      <url>/2023/07/02/Reinforcement-Learning-with-Code-Chapter-1-Basic-Concepts/</url>
      
        <content type="html"><![CDATA[<h1 id="Reinforcement-Learning-with-Code"><a href="#Reinforcement-Learning-with-Code" class="headerlink" title="Reinforcement Learning with Code"></a>Reinforcement Learning with Code</h1><p>This note record how the author begin to learn RL. Both theoretical understanding and code practice are presented. Many material are referenced such as ZhaoShiyu’s <em>Mathematical Foundation of Reinforcement Learning</em> .</p><h2 id="Chapter-1-Basic-Concepts"><a href="#Chapter-1-Basic-Concepts" class="headerlink" title="Chapter 1. Basic Concepts"></a>Chapter 1. Basic Concepts</h2><h3 id="1-1-State-and-action"><a href="#1-1-State-and-action" class="headerlink" title="1.1 State and action"></a>1.1 State and action</h3><ul><li><font color="red"><em>State</em> </font>describe the status of the agent with respect to the environment, denoted by $s$.</li><li><font color="red"><em>State space</em> </font> is the set of all states, denoted by $\mathcal{S}=\{s_1, s_2,\dots,s_n\}$.</li><li><font color="red"><em>Action</em></font> describe the action that the agent may take with respect to the environment, denoted by $a$.</li><li><font color="red"><em>Action space</em></font> is the set of all actions, denoted by $\mathcal{A}=\{a_1, a_2,\dots,a_n\}.$</li></ul><h3 id="1-2-State-transition"><a href="#1-2-State-transition" class="headerlink" title="1.2 State transition"></a>1.2 State transition</h3><p>When taking an action, the agent may move from one state to another. Such a process is called <font color="red"><em>state transition</em></font>. State trasition can be denoted by</p><script type="math/tex; mode=display">s_1 \stackrel{a_2}\longrightarrow s_2</script><p>described by $p(s^\prime|s,a)$.</p><p>State trainsition can be both <em>deterministic</em> and <em>stochastic</em>. For example the deterministic state transition is</p><script type="math/tex; mode=display">p(s_1|s_1,a_2) = 0 \\p(s_2|s_1,a_2) = 1 \\p(s_3|s_1,a_2) = 0</script><p>For example the stochastic state transition is </p><script type="math/tex; mode=display">p(s_1|s_1,a_2) = 0.5 \\p(s_2|s_1,a_2) = 0.3 \\p(s_3|s_1,a_2) = 0.2</script><h3 id="1-3-Policy"><a href="#1-3-Policy" class="headerlink" title="1.3 Policy"></a>1.3 Policy</h3><ul><li><font color="red"><em>Policy</em></font> tells the agents which actions to take <font color="blue">at each state</font>, denoted by $\pi$.</li><li>Policy is described by conditional probability.</li><li>Policy can be <em>deterministic</em> or <em>stochastic</em>, which means one state has a deterministic action or one state has probability to select other actions.</li></ul><p>Suppose the actions space is $\mathcal{A}=\{a_1, a_2,a_3\}$, such  deterministic policy can be dentoed by</p><script type="math/tex; mode=display">\pi(a_1|s_1) = 0 \\\pi(a_2|s_1) = 1 \\\pi(a_3|s_1) = 0</script><p>which indicated the probability of taking action $a_2$ is $1$ and others are zero. </p><p>Such stochastic policy can be denoted by</p><script type="math/tex; mode=display">\pi(a_1|s_1) = 0.5 \\\pi(a_2|s_1) = 0.3 \\\pi(a_3|s_1) = 0.2</script><h3 id="1-4-Reward"><a href="#1-4-Reward" class="headerlink" title="1.4 Reward"></a>1.4 Reward</h3><ul><li><font color="red"><em>Reward</em></font> is one of the most unique concept in RL.</li><li><font color="red"><em>Immediate reward</em></font> can be obtained after taking an action.</li><li><font color="red"><em>Reward transition</em></font> is the process of getting a reward after taking an action, reward transition can be <em>deterministic</em> or <em>stochastic</em>. Reward transition is described by $p(r|s,a)$</li></ul><p>For example deterministic reward transition can be denoted by</p><script type="math/tex; mode=display">p(r=-1|s_1,a_2) = 1, p(r\ne -1|s_1,a_2)=0</script><p>which means at state $s_1$ take action $a_2$ the probability to get immediate reward $-1$ is $1$.</p><p>Stochastic reward transition can be denoted by </p><script type="math/tex; mode=display">p(r=1|s_1,a_2) = 0.5, p(r= 0|s_1,a_2)=0.5</script><p>which means at state $s_1$ take action $a_2$ the probability to get immediate reward $1$ is $0.5$, the probability to get immediate reward $0$ is $0.5$.</p><h3 id="1-5-Trajectory-return-episode"><a href="#1-5-Trajectory-return-episode" class="headerlink" title="1.5 Trajectory, return, episode"></a>1.5 Trajectory, return, episode</h3><ul><li><p><font color="red"><em>Trajectory</em></font> is a state-action-reward chain, such as $s_1 \underset{r=0}{\xrightarrow{a_2}} s_2 \underset{r=0}{\xrightarrow{a_3}} s_3 \underset{r=0}{\xrightarrow{a_4}} \cdots\underset{r=1}{\xrightarrow{a_n}} s_{n}$.</p></li><li><p><font color="red"><em>Return</em></font> of this trajecotry is the sum of all the rewards collected along the trajectory, such as $\text{return} = 0+0+0+\cdots+1=1$. Return is also called <font color="blue"><em>total rewards</em></font> or <font color="blue"><em>cumulative rewards</em></font>.</p></li><li><p><font color="red"><em>Discounted return</em></font> is defined by the <font color="blue"><em>discounted rate</em></font>, denoted by $\gamma\in(0,1)$. Such discounted retrun is</p><script type="math/tex; mode=display">\text{discounted return} = 0+\gamma0+\gamma^2 0 + \gamma^3 0 + \cdots + \gamma^n 1</script></li><li><p><font color="red"><em>Episode</em></font> refers the trajectory that interacting with the enviornment following a policy <font color="blue">until the agent reach the terminal state</font>. An episode is usually assumed to be a finite trajectory,  that task with episodes are called <em>episodic tasks</em>. Some task may have no terminal state, such task is called <em>continuing tasks</em>.</p></li></ul><h3 id="1-6-Markov-decision-process-MDP"><a href="#1-6-Markov-decision-process-MDP" class="headerlink" title="1.6 Markov decision process (MDP)"></a>1.6 Markov decision process (MDP)</h3><p>Markov decision process is a general framework to <font color="blue">describe stochastic dynamical systems</font>. The key ingredients of an MDP are listed:</p><ul><li><p>Sets:</p><ul><li>State set: the set of all states, denoted as $\mathcal{S}$.</li><li>Actions set: a set of actions, denoted as $\mathcal{A}(s)$, is associated for each state $s\in\mathcal{S}$.</li><li>Reward set: a set of rewards, denoted as $\mathcal{R}(s,a)$, is associated for each state action pari $(s,a)$.</li></ul></li><li><p>Model:</p><ul><li>State transition probability: at state $s$, taking actions $a$, the probability to transit to state $s^\prime$ is $p(s^\prime|s,a)$.</li><li>Reward transition probability: at state $s$, taking action $a$, the probability to get reward $r$ is $p(r|s,a)$.</li></ul></li><li><p>Policy: as state $s$, the probability to choose action $a$ is $\pi(a|s)$.</p></li><li><p>Markov property: one key property of MDPs is the <em>Markov property</em>, which refers to the <font color="blue">memoryless property</font> of a stochastic process, which means</p><script type="math/tex; mode=display">p(s_{t+1}|s_t,a_t,s_{t-1},a_{t-1},\dots,s_0,a_0)=p(s_{t+1}|s_t,a_t)    \\p(r_{t+1}|s_t,a_t,s_{t-1},a_{t-1},\dots,s_0,a_0)=p(r_{t+1}|s_t,a_t)    \\</script><hr><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://www.bilibili.com/video/BV1sd4y167NS/?spm_id_from=333.999.0.0">赵世钰老师的课程</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Reinforcement Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RL </tag>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
